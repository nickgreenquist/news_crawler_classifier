CU Boulder Associate Professor Karen Chin excavates dinosaur coprolites at Grand Staircase-Escalante National Monument in Utah, in this May 10, 2013 photo.
Courtesy University of Colorado/Handout via REUTERS

WASHINGTON (Reuters) - Some plant-eating dinosaurs apparently liked a side order of crabs to go with their usual salad.
Scientists said on Thursday fossilized dung thought to have come from herbivorous duck-billed dinosaurs that inhabited southern Utah 75 million years ago contained pieces of crustacean shells along with vestiges of vegetation.
The discovery provides the strongest evidence to date that some large herbivorous dinosaurs sometimes strayed from a purely vegetarian diet, said University of Colorado paleontologist Karen Chin, who led the research published in the journal Scientific Reports.
This was a very exciting discovery, precisely because it was so unexpected, Chin said.
Fossilized dung, called coprolites, offers insight into the diet of extinct creatures that cannot be gleaned by merely studying teeth, jaws and skeletons.
Ten coprolites from Utahs Grand Staircase-Escalante National Monument contained crustacean shells mixed with rotted coniferous wood.
For at least part of the year these duck-billed dinosaurs may have munched on rotting logs because they contained stores of crustaceans and other invertebrates, Chin said.
Field Museum scientists Pete Makovicky (L), Associate Curator of Dinosaurs, and Bill Simpson, Head of Geological Collections, use a cast of one of the T. rex SUEs gastralia -- a set of bones that look like an additional set of ribs -- to show where they will be positioned on her skeleton in Chicago, Illinois, U.S. in this undated handout photo obtained by Reuters August 29, 2017.
Zachary James Johnston/The Field Museum/Handout via REUTERS

Chin said the researchers do not know precisely what types of crustaceans were eaten by the dinosaurs, but it was possible they were crabs.
A variety of crustaceans also including crayfish and pillbugs are known to shelter in the moist environments of rotting logs.
The crustaceans were at least two inches (5 cm) long.
Rotting wood and fungal tissues would have offered useful nutritional compounds such as cellulose and fiber, and the crustaceans would have provided good sources of protein and calcium.
Protein is an important component of animal diets, and is particularly important when animals are breeding, Chin said.
The size of two dinosaurs, the Argentine titanosaur Patagotitan mayorum and the North American predator Tyrannosaurus rex are compared with a human figure for scale, in this handout illustration obtained by Reuters August 29, 2017.
The Field Museum/Handout via REUTERS

The researchers think eating crustaceans may have been a seasonal dietary change linked to breeding and egg-laying.
Some birds, the evolutionary descendants of dinosaurs, consume more protein and calcium during breeding season.
Duck-billed dinosaurs, also called hadrosaurs, earned their name because the front of their skull resembles a ducks bill.
They also possessed beaks and specialized teeth for grinding plant material, and are believed to have roamed the landscape in herds.
Two hadrosaurs that lived in the area at the time were: Parasaurolophus, roughly 33 feet (10 meters) long with a long tubular head crest; and Gryposaurus, about 39 feet (12 meters) long with an arched nasal crest.
Hadrosaurs were common in western North America and other parts of the world during the Cretaceous Period, representing an important plant-eating group alongside armored dinosaurs and horned dinosaurs.
A primitive type of jellyfish called Cassiopea, which goes to sleep nightly, is seen on the floor of their tank at Caltech in Pasadena, California, U.S. in this image released on September 20, 2017.
Courtesy Caltech/Handout via REUTERS

WASHINGTON (Reuters) - Even a jellyfish - one of Earths first and most ancient animals - needs its sleep.
Scientists said on Thursday they have demonstrated that a primitive type of jellyfish called Cassiopea goes to sleep nightly.
While sleep has been confirmed in other invertebrates such as worms and fruit flies, the jellyfish is the most evolutionarily ancient animal that has been shown to slumber.
These results suggest that even those animals that lack a centralized nervous system require sleep, which means that sleep is one of the most ancient behavioral states, deeply rooted within the animal lineage, California Institute of Technology biologist Ravi Nath said.
Jellyfish have thrived in the seas for at least 600 million years, longer than nearly any other animal.
By comparison, dinosaurs appeared roughly 230 million years ago and humans appeared roughly 300,000 years ago.
The findings involving such a primordial creature raise fresh questions about sleeps origin and purpose.
We do not know if sleep is limited to just animals, said Nath, who helped lead the study published in the journal Current Biology.
Sleep is a genetically encoded behavioral state.
Genes and neural circuits interact to generate the sleep state, Nath added.
I think it would be hard to demonstrate a sleep state in an organism that is not an animal, but I think the sleep state that we know may have been co-opted from periods of quiescence in organisms as diverse as plants, bacteria and fungi.
Jellyfish are among the first animals to have developed neurons - nerve cells - though they lack a brain, spine or central nervous system.
Cassiopea jellyfish live in clear, shallow, tropical waters of the Pacific and western Atlantic oceans, eating plankton.
Measuring about 1-2 inches (2.5-5 cm) in diameter, they are dubbed the upside-down jellyfish because they lie on the seafloor inverted in the water with their tentacles upward.
Through lab experiments, the researchers determined Cassiopea met three important sleep criteria: periods of decreased activity known as behavioral quiescence; a decreased response to stimuli; and an increased sleep drive after being sleep deprived.
The jellyfish were found to display periods of inactivity at night, pulsing their bodies 30 percent less often than during daytime.
When a platform underneath them was removed, they took up to 5 seconds to wake up and reorient themselves.
And when deprived of nighttime sleep by being nudged with a squirt of water, they became more likely to sleep during the day.
The researchers did not examine whether jellyfish dream.
SOURCE: bit.ly/2fElMrT Current Biology, online September 21, 2017.
(Reuters) - The U.S. space agency NASA received a final signal from its Cassini spacecraft on Friday as it ended a groundbreaking, 13-year Saturn mission with a meteor-like plunge into the ringed planets atmosphere.
Cassini, the first spacecraft to orbit Saturn, entered the gaseous giants crushing atmosphere at 7:55 a.m. EDT (1155 GMT) at about 70,000 miles per hour (113,000 km per hour), the National Aeronautics and Space Administration said.
This morning a lone explorer - a machine made by human-kind - finished its mission 900 million miles away, Cassini project manager Earl Maize said at a news conference on Friday at NASAs Jet Propulsion Laboratory in Pasadena, California.
We believe we got every last second of data.
The end of Cassinis voyage, which began with its launch in 1997 and a seven-year journey to Saturn, was met with applause, hugs and tears from NASA officials after its final transmission was received, according to video footage on the space agencys website.
Officials at the news conference displayed the last set of images Cassini captured of Saturn as it crashed into the planet.
The planets lakes and seas near its north pole were visible, along with detailed views of gaps in its massive rings.
Maize said Cassinis data, sent until the final fiery moment, was already being studied by NASA analysts in Arizona.
The transmissions are expected to include unprecedented data from the atmospheres upper fringe, about 1,190 miles (1,915 km) above Saturns cloudtops.
The data took 84 minutes to reach NASA antennas in Canberra, Australia, Maize said.
FILE PHOTO: The spacecraft Cassini is pictured above Saturn's northern hemisphere prior to making one of its Grand Finale dives in this NASA handout illustration obtained by Reuters August 29, 2017.
NASA/Handout via REUTERS

The final dive ended a mission that gave scientists a ringside seat to the sixth planet from the sun.
The spacecrafts discoveries included seasonal changes on Saturn, a hexagon-shaped pattern on its north pole and the moon Titans resemblance to a primordial Earth.
Cassini also found a global ocean on the moon Enceladus, with ice plumes spouting from its surface.
Enceladus has become a promising lead in the search for places outside Earth that could support life.
The spacecraft has produced 450,000 images and 635 gigabytes of data since it began probing Saturn and its 62 known moons in July 2004.
Slideshow (13 Images)

Cassini, a cooperative project between NASA, the European Space Agency and the Italian Space Agency, was launched into space in October 1997 from Cape Canaveral in Florida.
With the spacecraft running low on fuel, NASA crashed it into Saturn to avoid any chance of it someday colliding with and contaminating Titan, Enceladus or another moon that has the potential for indigenous microbial life.
Cassini started a series of 22 orbital dives in April, using Titans gravity to slingshot itself into the unexplored area between the planet and its rings.
The spacecraft studied Saturns atmosphere and took measurements to determine the size of the planets rocky core.
Scientists took to Twitter to share their goodbyes.
Farewell Cassini, how far youve come, astrophysicist Neil deGrasse Tyson said on Twitter.
On this eve, in fiery death, Saturn & you are one.
VIP (Vaporize In Peace): 2004-2017.
BAIKONUR COSMODROME, Kazakhstan (Reuters) - Two U.S. astronauts and a Russian cosmonaut arrived at the International Space Station on Wednesday, about six hours after their Soyuz spacecraft blasted off from Kazakhstan, a NASA TV broadcast showed.
Commander Alexander Misurkin of Roscosmos and flight engineers Mark Vande Hei and Joe Acaba of NASA lifted off from the Baikonur Cosmodrome at 3:17 a.m. local time on Wednesday (2117 GMT/1717 EDT on Tuesday).
Their spacecraft docked at 8:55 a.m..
The crew successfully performed a fast-track transit to the station, which orbits about 250 miles (400 km) above Earth, to begin a five-month mission.
Failure would have forced the spacecraft to take a two-day route for another attempt at docking.
Misurkin, Vande Hei and Acaba have joined NASA astronaut Randy Bresnik, Russias Sergey Ryazanskiy and Paolo Nespoli of the European Space Agency who have been aboard the orbital outpost since July.
To commemorate the upcoming 60th anniversary on Oct.4 of the launch of the first artificial satellite, Sputnik 1, the Soyuz crew used its small model as a zero gravity indicator during the flight on Wednesday.
(Reuters) - Green, purple, pink and yellow lights danced across the sky in striking aurora displays over northern Finland early on Friday.
Travel magazine All About Lapland posted a video on social media of the impressive light show seen from Pallas, in the Muonio region, adding it had rarely seen anything on this scale.
The Northern Lights are a result of collisions between electrically charged particles from the sun that enter the Earths atmosphere.
A strong geomagnetic storm was behind this weeks particularly stunning show.
Known as aurora borealis, or the Northern Lights, in the northern hemisphere, they go by aurora australis, or the Southern Lights, in the southern hemisphere.
LONDON, Sept 7 (Reuters) - The alcohol industry uses denial, distortion and distraction to mislead people about the risks of developing cancer from drinking, often employing similar tactics to those of the tobacco industry, a study said on Thursday.
Drinks industry organizations often present the relationship between alcohol and cancer as highly complex, implying there is no clear evidence of a consistent link, said the study led by scientists at the London School of Hygiene & Tropical Medicine (LSHTM) and Swedens Karolinska Institutet.
Other strategies include denying any relationship exists, or saying inaccurately that there is no risk with moderate drinking, the study found.
The industry also seeks to mention a wide range of other real and potential cancer risk factors in an effort to present alcohol as just one of many, it added.
Responding to the study, the Distilled Spirits Council, a U.S. alcohol trade association, said it was a highly selective review authored by researchers with anti-alcohol biases.
The Council does not recommend that people drink alcohol for potential health benefits, it said in a statement.
Drinking in moderation may pose health risks for some people, and some individuals should not drink at all.
The International Alliance for Responsible Drinking, which represents large brewers and distillers including Anheuser-Busch InBev and Diageo , said it disagreed with the studys conclusions.
We ... stand by the information that we publish on drinking and health, it said.
RISING RISK

The World Health Organization says drinking alcohol is a well-established risk factor for a range of cancers, including tumors of the mouth, liver, breast and colon and bowel.
And the risk of cancer rises with levels of alcohol consumed.
FILE PHOTO: A waiter serves beer in the traditional Schweizerhaus beer garden in Vienna, Austria June 21, 2017.
REUTERS/Leonhard Foeger/File Photo

The research team behind Thursdays study analyzed the information relating to cancer on the websites and documents of nearly 30 alcohol industry organizations around the world between September 2016 and December 2016.
The weight of scientific evidence is clear - drinking alcohol increases the risk of some of the most common forms of cancer, said Mark Petticrew, a professor of public Health at the LSHTM who co-led the study.
It has been argued that greater public awareness, particularly of the risk of breast cancer, poses a significant threat to the alcohol industry.
Our analysis suggests that the major global alcohol producers may attempt to mitigate this by disseminating misleading information.
Petticrews team identified three main industry strategies: Denying any link with cancer, or selective omission of the relationship; distortion by mentioning some risk of cancer, but misrepresenting or obfuscating its size; and distraction by seeking to draw focus away from the risks of alcohol and towards other cancer risks.
One of the most significant findings was that industry materials omitted or misrepresented evidence on breast and bowel cancer, both of which are linked to drinking.
When breast cancer was mentioned, 21 of the organizations studied gave no, or misleading, information about it, the study said.
Ian Gilmore, chair of the Alcohol Health Alliance UK, said the study clearly shows the alcohol industry misleading the public.
With only 1 in 10 people aware of the link between alcohol and cancer, people have both a need and a right to clear information about the health risks of drinking alcohol.
Petticrew said the studys findings, published in the journal Drug and Alcohol Review on Thursday, were important partly because the alcohol industry is often involved in spreading health information to people around the world.
CAPE CANAVERAL, Fla (Reuters) - NASA astronaut Peggy Whitson and two crewmates made a parachute touchdown in Kazakhstan on Saturday, capping a career-total 665 days in orbit, a U.S. record.
Whitson, 57, ended an extended stay of more than nine months aboard the International Space Station, a $100 billion research laboratory that flies about 250 miles (400 km) above Earth.
I feel great, the biochemist said during an inflight interview on Monday.
I love working up here.
Its one of the most gratifying jobs Ive ever had.
During her third mission aboard the station, Whitson spent much of her time on experiments, including studies of cancerous lung tissue and bone cells.
She also completed four spacewalks, adding to her six previous outings, to set a record for the most time spent spacewalking by a woman.
Two crewmates who launched with Whitson in November returned to Earth three months ago.
She stayed aboard to fill a vacancy after Russia scaled down its station staff from three to two cosmonauts.
Whitson returned to Earth with Jack Fischer, also with the National Aeronautics and Space Administration, and Russian cosmonaut Fyodor Yurchikhin, who had been aboard the station since June.
FILE PHOTO - The International Space Station (ISS) crew member, astronaut Peggy Whitson of the U.S. speaks prior to the launch of Soyuz MS-3 space ship at Baikonur cosmodrome, Kazakhstan, November 17, 2016.
REUTERS/Dmitri Lovetsky/Pool/File Photo

The crews Russian Soyuz capsule touched down in Kazakhstan at 9:21 p.m. EDT Saturday.
Im looking forward to seeing friends and family, Whitson said during another interview.
Slideshow (13 Images)

But the thing Ive been thinking about the most, kind of been fantasizing about a little bit, are foods that I want to make, vegetables that I want to saut, things that Ive missed up here.
In April, Whitson broke the 534-day U.S. record for cumulative time in space.
Only seven Russian men have logged more time, including Gennady Padalka, the world record-holder with 878 days in orbit.
Whitson, who grew up on a farm in Iowa, said she was inspired by the U.S. Apollo program that landed men on the moon, but it was not until later, when the first women become astronauts, that she set her sights on joining them.
Whitson, who became an astronaut in 1996, was the first woman to command the space station and also the first woman and first non-pilot to serve as chief of the NASA Astronaut Corps.
I am working on paying forward some of the advice and mentoring that I received on my journey, in hopes that one day those young people will do the same and look back on a life in which they leapt at the opportunities and broke their own records, she said.
FRANKFURT (Reuters) - European satellite launching firm Arianespace said it had called off the launch of two communications satellites seconds before lift-off on Tuesday, citing unspecified problems.
An Ariane 5 heavy-launcher rocket had been due to take off from Europes spaceport in French Guiana, carrying satellite Intelsat 37e for Intelsat and BSAT-4a for manufacturer Space Systems Loral.
Arianespace, majority-owned by a joint venture of Airbus and Safran, offers launches with Ariane 5, Soyuz and Vega rockets and says it has sent into orbit more than half of all telecommunications satellites now in service.
The company said engineers were trying to find out what caused the anomaly in the launch, which was to be the fifth Ariane 5 mission from the Guiana Space Center this year.
During the final seconds of the launch countdown for Arianespace Flight VA239, the checkout process detected an anomaly on the launcher as the Vulcain cryogenic main stage engine was being ignited, Arianespace said in a statement.
It said it would set a new launch date as soon as possible.
India's Polar Satellite Launch Vehicle (PSLV) C-39, carrying IRNSS-1H navigation satellite, lifts off from the Satish Dhawan Space Centre in Sriharikota, India, August 31, 2017.
REUTERS/P.
Ravikumar

NEW DELHI (Reuters) - Indias eighth navigation satellite imploded shortly after lift off on Thursday, state-run Indian Space Research Organisation (ISRO) said.
The IRNSS-1H satellite had been expected to join seven others in the Indian Regional Navigation Satellite System (IRNSS) to take the country a step further to developing its own global positioning system.
Satellite got separated internally but it imploded within the heat shield, in the fourth stage itself, ISRO Chairman A.S. Kiran Kumar told reporters in a televised news conference.
The heat shield is meant to protect the satellite from the heat generated by the friction against atmosphere during take-off.
Once a satellite is placed into orbit, it is expected to separate and fall off.
The IRNSS-1H satellite had been released from the Sriharikota Space Centre in southern India.
IRNSS helps navigate the countrys aerial and marine routes, as well as aid disaster management and vehicle tracking up to 1,500 kilometers (932 miles) around the mainland.
However, India lags behind the United States GPS, Russias GLONASS, Europes Galileo and Chinas Beidou systems that have dozens of satellites to provide information across the globe.
Field Museum scientists Pete Makovicky (L), Associate Curator of Dinosaurs, and Bill Simpson, Head of Geological Collections, use a cast of one of the T. rex SUEs gastralia -- a set of bones that look like an additional set of ribs -- to show where they will be positioned on her skeleton in Chicago, Illinois, U.S. in this undated handout photo obtained by Reuters August 29, 2017.
Zachary James Johnston/The Field Museum/Handout via REUTERS

(Reuters) - The worlds biggest T. rex is getting ready for a cutting-edge makeover.
The Field Museum in Chicago said on Wednesday it will take down and remount the 40-1/2-foot-long (12.3-meter) Tyrannosaurus nicknamed Sue, perhaps the worlds most famous dinosaur fossil, in a way that embodies the latest understanding of this ferocious Cretaceous Period predator.
The big T. rex will move to a new exhibition space in the museum, while a cast of the skeleton of the largest-known dinosaur, Patagotitan mayorum, will take the spot Sue now occupies in the museums Stanley Field Hall.
Patagotitan, a long-necked, four-legged plant-eater that was 122 feet (37.2 meters) long and weighed 70 tons, lived in Argentina 100 million years ago, more than 30 million years before T. rex stalked western North America.
The biggest land animal on record, it was a member of a dinosaur group called titanosaurs.
The museum next spring will unveil the fiberglass Patagotitan skeleton, which is being cast from fossils of seven Patagotitan individuals, and for two years will display some of the genuine fossils, including an 8-foot (2.4 meter) thighbone.
Named for the woman who discovered the fossils in South Dakota in 1990, Sue is the largest, most complete and best-preserved Tyrannosaurus rex ever unearthed.
The museum bought the fossils at auction for $8.4 million.
Sue will be taken down in February and put up again with noteworthy changes in anatomy and stance in its new exhibition hall in spring 2019, museum scientists said.
We are making several adjustments to the skeleton to reflect new and improved knowledge, said paleontologist Pete Makovicky, the museums associate curator of dinosaurs.
The most striking change, Makovicky said, will be the addition of gastralia, bones resembling an additional set of ribs spanning the belly that may have provided structural support to help the dinosaur breathe.
Adding these bones will illustrate just how massive Sue was and that it boasted a bulging belly, he added.
The size of two dinosaurs, the Argentine titanosaur Patagotitan mayorum and the North American predator Tyrannosaurus rex are compared with a human figure for scale, in this handout illustration obtained by Reuters August 29, 2017.
The Field Museum/Handout via REUTERS

The scientists concluded that the bone mounted as Sues wishbone was misidentified in 2000 and they will replace it with the dinosaurs actual wishbone, or furcula, the fused collar bones typical of meat-eating dinosaurs and their evolutionary descendants the birds.
They also will adjust the ribs to produce a slimmer, less barrel-shaped chest, and arrange the right leg so Sue is not crouching as much.
Often when you do something as expensive as mounting a vertebrate fossil skeleton for display you only get one shot at it.
Im happy were going to fix and update this incredible fossil, said paleontologist Bill Simpson, who heads the museums geological collections.
LIFESPAN AND BITE FORCE

Makovicky noted the accumulation of knowledge about T. rex and its cousins since 2000.
We now know more about tyrannosaur lifespans -- around 30 years; how they grew -- very fast as teenagers; and using computer models of Sue we revised their body mass upward to 9 or more tons, from 5 to 7 tons, Makovicky said.
Ongoing research is examining the molecular composition of cartilage preserved in T. rex bones, and recent studies have shown it possessed the most powerful bite of any land animal ever, Makovicky added.
When the Patagotitan skeleton is mounted, visitors will be able to walk underneath it and touch it.
Its head will reach the museums second-floor balcony nearly 30 feet (9 meters) up.
Another Patagotitan skeleton is displayed at the American Museum of Natural History in New York.
The museum said a $16.5 million gift from the Kenneth C. Griffin Charitable Fund, established by the founder and chief executive of hedge fund firm Citadel LLC, enabled it to carry out Sues makeover and add the Patagotitan.
The changes coincide with the museums 125th anniversary in 2018.
Image copyright BinBin Li Image caption The giant panda has evolved to live on a diet of bamboo

Despite signs that numbers of giant pandas are rising, suitable habitat has shrunk, according to satellite data.
The forests where the panda lives are in worse shape than in 1988, when it was first listed as endangered, scientists say.
Last year, the giant panda was downgraded from endangered to vulnerable on the IUCN Red List.
Habitat loss is the most serious threat to the animal, which is seen as an icon of global extinction efforts.
"What's new in this study is our ability to assess the status of the giant panda by using satellite imagery and then use that information to come up with recommendations of how better to manage this iconic threatened species," said Prof Stuart Pimm, of Duke University, North Carolina, US, who is a researcher on the study.
The news last year that the giant panda had been taken off the endangered list made headlines around the world.
The decision was made because numbers of wild pandas had risen in surveys.
However, with only around 1,800 left in the wild, establishing new reserves and extending existing ones is crucial for the animal's survival.
"I think we now understand we've got to keep an eye on the habitats where pandas live," said Prof Pimm.
"But it also points to the need to try and re-connect isolated panda habitats by building what we call biological corridors."
Shrinking forests

Chinese and US scientists used geographic mapping, remote sensing data and satellite imagery to assess changes across the panda's entire range from 1976 to 2013.
Their study, published in the journal, Nature Ecology & Evolution, suggests that suitable panda habitats have substantially reduced.
Earthquakes, human encroachment, agriculture, road building, tourism and logging of forests have had the effect of dividing the areas where pandas live into ever smaller fragments.
Image copyright Getty Images Image caption The giant panda is classed as vulnerable to extinction

"Habitat decreased nearly 5% from 1976 to 2001, but has increased since, said Weihua Xu of the Research Center for Eco-Environmental Sciences at the Chinese Academy of Sciences.
"However, the average size of the habitat patches decreased by 23% from 1976 to 2001.
It has increased only slightly since."
Some of the changes in the region are encouraging, such as stopping logging and establishing nature reserves, said Jianguo Liu of Michigan State University.
"But conservation is a dynamic process with humans and nature in a constant push and pull to survive and thrive, so new solutions always are in demand," he added.
The giant panda was once widespread throughout southern and eastern China.
There are now estimated to be about 1,800 giant pandas left in the wild in six mountain ranges in China's Sichuan, Shaanxi and Gansu provinces.
Follow Helen on Twitter.
Image copyright Copernicus Sentinel data (2017) Image caption Widening gap: The picture contains data gathered on 13 and 16 September

The giant berg A-68 looks finally to be on the move.
Recent weeks have seen it shuffle back and forth next to the Antarctic ice shelf from which it broke away.
But the latest satellite imagery now indicates the near-6,000 sq km block is swinging out into the Weddell Sea.
A wide stretch of clear water has opened up between the berg's southern end and the remaining Larsen shelf structure, suggesting A-68 is set to swing around and head north.
This is the direction the Weddell currents should take the iceberg.
Polar experts expect the trillion-tonne block to essentially bump along the shelf edge until it reaches the great eastward movement of ocean water known as the Antarctic Circumpolar Current.
This would then export what is one of the largest bergs ever recorded out into the South Atlantic.
How far A-68 actually gets along this predicted path is anyone's guess, however.
The berg already shows evidence of fragmentation at its edges.
These bits - they carry the designation A-68b, A-68c, etc - all still float close to their parent.
But in time they will get separated, and it is entirely possible that big segments with deep keels could get anchored in shallow waters and become semi-permanent "ice islands".
A-68 calved during mid-winter and it required radar satellites - such as Europe's Sentinel-1 spacecraft - with their unique ability to pierce cloud and darkness to keep track of developments.
With the return now to longer days in the Antarctic, opportunities are increasingly opening up for high-resolution optical satellites to take a close look at the state of the berg.
And new imagery from the Spanish Deimos-2 spacecraft shows how the initial sharp edges of the block's northern-western corner have been lost.
Image copyright Deimos Imaging, an UrtheCast Company

Scientists are not just looking at the berg; they also continue to monitor the Larsen Ice Shelf.
They are checking to see if its behaviour has changed since the calving.
The shelf is the floating protrusion of glaciers coming off the Antarctic landmass, and the ejection of such a large section of its structure could potentially trigger further fracturing or a change in the speed of ice flow.
So far, however, there is little evidence of either.
When A-68 moves clear of its birth position it will reveal seafloor that probably has not been free of ice cover for 120,000 years - during the peak of the last warm phase in Earth's history known as the Eemian.
The area has already gained protected status from the Commission for the Conservation of Antarctic Marine Living Resources (CCAMLR).
This gives scientists priority access and keeps fisheries activity at bay for a minimum of two years.
Previous research in locations uncovered by departing bergs has found new species.
Expeditions to visit A-68 this coming Antarctic summer season are in the planning stage.
Jonathan.Amos-INTERNET@bbc.co.uk and follow me on Twitter: @BBCAmos
Image copyright Getty Images Image caption The Australian government has committed to creating a national space agency

Australia will establish a national space agency, the government has said.
Although its space industry employs about 11,500 people, Australia is one of the few major developed countries that do not have a space agency.
Industry Minister Michaelia Cash said it was "crucial" that Australia capitalised on the growth of the global space industry.
The move follows a domestic industry review which called for a dedicated body to be established.
"The agency will be the anchor for our domestic co-ordination and the front door for our international engagement," Ms Cash said.
The government is expected to announce further details at the International Astronautical Congress in Adelaide this week.
The conference will be attended by thousands of global space experts, including the heads of other national agencies and private companies.
Media playback is unsupported on your device Media caption Why firms are spending millions to beat each other into orbit

The country is largely reliant on overseas nations like the United States for its satellite and earth observation data.
The SIAA has argued that Australia is well placed to expand its space technologies because of its location, large geographical size and relatively low population.
Britain will need to boost its generation of electricity by about a quarter, Scottish Power has estimated.
The energy firm said electric cars and a shift to electric heating could send demand for power soaring.
Its chief executive also said there would have to be a major investment in the wiring necessary to handle rapid charging of car batteries.
Keith Anderson was speaking as the firm reached the milestone of 2,000 megawatts of wind power capacity.
That equates to about an eighth of the British total.
The figure includes Whitelee wind farm, on Eaglesham Moor, south of Glasgow, which has more than 200 turbines.
Believed to be Europe's biggest wind farm, it is capable of generating enough power for all of Glasgow's homes.
Image copyright Thinkstock Image caption Whitelee is believed to be Europe's biggest wind farm

In the past 18 months, the Spanish-owned company has been installing nearly a quarter of the British total, but the pipeline of work is coming to an end.
Attention is turning to offshore wind.
But Mr Anderson told BBC Scotland there would have to be a renewed surge in the building of onshore wind turbines if consumer demand was to be met.
He warned that past experience with technology change had shown consumers could make the move faster than governments or companies expect.
Once the price of electric cars falls to that of petrol or diesel, which it is thought will happen between 2022 and 2025, there could be a rapid shift in buying patterns and electricity usage.
Earlier this month, First Minister Nicola Sturgeon announced a target of shifting from petrol and diesel-fuelled cars to battery power by 2032, while the UK government intends to make that shift by 2040.
'Worst position'

Mr Anderson said: "The worst position for this country to be in and the worst position for customers is that we get huge enthusiasm, people rushing out to buy electric cars because the price has come down, and then we can't allow people to plug them in because we haven't invested in the infrastructure.
"So one of the things we're looking at now is how we plan what has to happen to the distribution system."
The estimate of a 20%-30% increase in demand for electricity comes after years of gradually declining power use, much of that due to growing energy efficiency and the closure of older, energy-intensive industries.
Image copyright Getty Images

At the same time, old power stations - including Scottish Power's coal-burning plants at Longannet in Fife and Cockenzie in East Lothian - have been closed down.
The added challenge of cars is the change in technology from an eight-hour overnight charge to a rapid charge of 15 to 20 minutes.
If several car owners on a residential street plug those in at the same time, the system could not cope.
Mr Anderson said: "The system that takes the wires into the house, down the street, to local businesses - how do we make sure it can cope with that level of demand?
It'll take a long time to plan and deliver."
'Let's keep going'

Heating is the next frontier in the energy revolution, which has barely begun.
In place of gas and oil-fired boilers in each home, electric central heating can be powered by renewable generation.
What we're saying to the politicians, regulators and customers is: let's keep going - this [wind power] has been a huge success Keith Anderson, Scottish Power chief executive

However, it is likely to require not only removal of a boiler, but the replacement of radiators and hot water heating pipes throughout a home.
Mr Anderson said: "What we're saying to the politicians, regulators and customers is: let's keep going - this [wind power] has been a huge success.
"We have been able to develop these projects faster and faster, and to deliver them more efficiently, at much lower cost.
"Keep going, because that will bring costs down and make us more efficient for the future.
"If you stop now, the technology development stops, the innovation stops: the new jobs, the new roles, they all stop.
You stop that for two or three years, and trying to restart it becomes more difficult and more expensive."
Most of the recent onshore wind developments have been in Scotland, focused on the south west.
The UK government has allowed much less onshore wind developing, in response to anti-turbine campaigners.
It has also left onshore wind out of the auctions which offer generators a minimum price for their energy.
These auctions have helped drive down the cost of renewable power, with offshore wind nearly halving in price.
Image copyright Joan Costa Image caption The skeleton of a boy that shattered our view of Neanderthal brain development

A new study shows that Neanderthal brains developed more slowly than ours.
An analysis of a Neanderthal child's skeleton suggests that its brain was still developing at a time when the brains of modern human children are fully formed.
This is further evidence that this now extinct human was not more brutish and primitive than our species.
The research has been published in the journal Science.
Until now it had been thought that we were the only species whose brains developed relatively slowly.
Unlike other apes and more primitive humans, Homo sapiens has an extended period of childhood lasting several years.
This is because it takes time and energy to develop our large brain.
Previous studies of Neanderthal remains indicated that they developed more quickly than modern humans - suggesting that their brains might be less sophisticated.
But a team led by Prof Antonio Rosas of the Museum of Natural Sciences in Madrid found that if anything, Neanderthal brains may have developed more slowly than ours.
"It was a surprise," he told BBC News.
"When we started the study we were expecting something similar to the previous studies."
Image copyright Paleoanthropology Group MNCN-CSIC] Image caption The remains were discovered inside the El Sidrn cave in Asturias, Spain.
Prof Rosas and his team believe they are right and the previous studies are wrong because for the first time they were able to study a relatively complete skeleton of a child at a crucial stage in their development.
It was of a boy, who was nearly seven-and-a-half years old when he died.
His bones were found in the 49,000-year-old site of El Sidrn, in Spain.
The boy's remains are exceptionally well preserved and include a mix of baby and adult teeth, which enabled the team to accurately determine his age.
This brain is estimated to have been 87.5% of the size of an average adult Neanderthal brain upon death.
A modern human child at the same general age would have, on average, a brain that was 95% the size of an adult's.
The researchers also found that some of the small bones forming the boy's backbone were not fused.
In modern humans, these bones tend to fuse by the time children reach the age of six.
Image copyright Joan Costa Image caption The researchers were surprised to discover that Neanderthal brains develop more slowly

According to Prof Rosas, the finding reinforces the idea that Neanderthals were not that different from us.
The brutish picture of Neanderthals is an old one.
In the last few years there has been growing evidence to suggest that they were a distinct human species with some small differences.
Now we can say that their growth pattern is similar to ours, too.
The finding raises the intriguing possibility that the Neanderthals' slightly slower brain development meant that their brains might have been more advanced than ours.
But Prof Rosas prefers a more prosaic interpretation.
"Neanderthals have a larger brain and larger body and so it is logical to think that the brain of the Neanderthal continues to grow for a little longer to allow their brains and bodies to get to their adult size," he explained.
Before this finding, scientists believed that modern humans were the slowest growing species.
Now we know that Neanderthals took slightly longer, suggesting that both species inherited this growth pattern from a now extinct common ancestor.
Follow Pallab on Twitter
Image copyright Science Photo Library Image caption Duck-billed dinosaurs may have been tempted away from a vegetarian diet

The idea of plant-eating dinosaurs having a strict vegetarian diet has been called into question.
New evidence suggests that some dinosaurs snacked on shellfish and insects as well as plant food.
A study of fossilised droppings indicates duck-billed dinosaurs dined on crabs at certain times of the year.
Fossil remains of dinosaur dinners is rare, so this pescatarian diet may have been overlooked in the past.
The popular perception of what dinosaurs ate was simplistic, said Dr Karen Chin of the University of Colorado, Boulder, US, who led the research.
"Plant-eating dinosaurs had more complex diets than we assumed that they had, and these diets included feeding on some animals, including at least crustaceans, and this was more like diets of modern plant-eating birds," she told BBC News.
The new evidence comes from an area of southern Utah that is regarded as a treasure trove of fossils from the Late Cretaceous Period, when dinosaurs were coming to the end of their reign.
Image copyright Science Photo Library Image caption The hadrosaurs were duck-billed dinosaurs that flourished some 80-65 million years ago

Fossilised dinosaur droppings found on the Kaiparowits Plateau give new insights into what was on the menu for dinosaurs.
Fragments of shell and other remains show they consumed crustaceans such as crabs, which likely sheltered in rotting wood.
The dinosaurs probably actively hunted for crustaceans and insects in a "woody stew" rather than swallowing them accidently, the researchers said.
Animal products may have been a vital source of protein, particularly when they were about to lay their eggs.
Plant guzzlers

"This find really breaks the mould for what we expect a plant-eating dinosaur to do," said Dr Steve Brusatte of the University of Edinburgh, UK, who was not connected with the study.
"We think of them as these multi-tonne plant guzzlers, but some of them also indulged in other types of food.
"And I guess we shouldn't be too shocked because that's true of many plant eaters today, they'll ingest other stuff, sometimes accidentally, sometimes to supplement their diet with other nutrients."
The droppings were left by dinosaurs some 75 million years ago on what would then have been a landscape dotted with rivers and ponds.
Duckbilled dinosaurs were common in the area at the time.
The bones and teeth of the reptiles suggest they spent most of their time on land, though close to freshwater, feeding on tough plants such as ferns and conifers.
The discovery, detailed in the journal Scientific Reports suggests other plant-eating dinosaurs may have in fact been omnivorous.
"We'll just have to find more evidence and we're always on the look out for that," said Dr Chin.
Follow Helen on Twitter.
Image copyright JJM/Geograph Image caption The Shiant Islands could be declared free of non-native black rats next year

The calls of a small seabird have been recorded for the first time on a group of islands in The Minch.
Conservationists hope the sound of storm petrels' "churring" is an indication that an effort to eradicate rats on the Shiant Islands is working.
The black rats are not native to the islands off Lewis and are thought to be the descendants of rats that came ashore from shipwrecks in the 1900s.
Storm petrels are not found where there are rats, which eat their eggs.
Media playback is unsupported on your device Media caption Storm petrels were recorded for the first time on the islands in the summer

On the Shiants, colonies of puffins, razorbills and guillemots have been in decline, while Manx shearwaters and until now storm petrels have not been found at all.
RSPB Scotland, Scottish Natural Heritage and the Nicolson family, which owns the islands, secured funding to start the rat eradication work in 2015.
They hope it will be possible to declare the islands rat-free next March.
Storm petrels, which are slightly bigger than a sparrow, had been seen flying past the Shiants.
To encourage them to breed on the islands, conservationists played the sound of their calls from a loud speaker.
This summer, real storm petrels' churring was heard and the birds seen at a burrow by conservationists using night vision equipment.
Image copyright Ed Marshall/RSPB Images Image caption Conservationists say storm petrels are vulnerable to rats

Dr Charlie Main, senior project manager for the Shiant Isles Recovery Project, said: "The churring of a storm petrel is very distinctive and we're delighted that it's been recorded on the Shiants this summer.
"While we are still some way off the islands being officially declared rat-free, these calls indicate that all the biosecurity work we're doing to keep these islands predator-free and make them ideal breeding sites for seabirds is paying off."
She added: "The long-term aim is to allow a breeding colony of storm petrels to establish at the Shiants."
Dr Andrew Douse, policy and advice manager in ornithology at Scottish Natural Heritage, said the first recording of the birds was "very welcome".
He said: "Storm petrels only occur on islands without rats, which means that they are very vulnerable to the effects that arise from invasive species such as these.
"The Shiants are an ideal breeding location for storm petrels and hopefully they will go on to become an important stronghold for this species."
Image copyright SPL

Scientists have engineered an antibody that attacks 99% of HIV strains and can prevent infection in primates.
It is built to attack three critical parts of the virus - making it harder for HIV to resist its effects.
The work is a collaboration between the US National Institutes of Health and the pharmaceutical company Sanofi.
The International Aids Society said it was an "exciting breakthrough".
Human trials will start in 2018 to see if it can prevent or treat infection.
Our bodies struggle to fight HIV because of the virus' incredible ability to mutate and change its appearance.
These varieties of HIV - or strains - in a single patient are comparable to those of influenza during a worldwide flu season.
So the immune system finds itself in a fight against an insurmountable number of strains of HIV.
Super-antibodies

But after years of infection, a small number of patients develop powerful weapons called "broadly neutralising antibodies" that attack something fundamental to HIV and can kill large swathes of HIV strains.
Researchers have been trying to use broadly neutralising antibodies as a way to treat HIV, or prevent infection in the first place.
The study, published in the journal Science, combines three such antibodies into an even more powerful "tri-specific antibody".
Dr Gary Nabel, the chief scientific officer at Sanofi and one of the report authors, told the BBC News website: "They are more potent and have greater breadth than any single naturally occurring antibody that's been discovered."
The best naturally occurring antibodies will target 90% of HIV strains.
"We're getting 99% coverage, and getting coverage at very low concentrations of the antibody," said Dr Nabel.
Experiments on 24 monkeys showed none of those given the tri-specific antibody developed an infection when they were later injected with the virus.
Dr Nabel said: "It was quite an impressive degree of protection."
The work included scientists at Harvard Medical School, The Scripps Research Institute, and the Massachusetts Institute of Technology.
'Exciting'

Clinical trials to test the antibody in people will start next year.
Prof Linda-Gail Bekker, the president of the International Aids Society, told the BBC: "This paper reports an exciting breakthrough.
"These super-engineered antibodies seem to go beyond the natural and could have more applications than we have imagined to date.
"It's early days yet, and as a scientist I look forward to seeing the first trials get off the ground in 2018.
"As a doctor in Africa, I feel the urgency to confirm these findings in humans as soon as possible."
Dr Anthony Fauci, the director of the US National Institute of Allergy and Infectious Diseases, said it was an intriguing approach.
He added: "Combinations of antibodies that each bind to a distinct site on HIV may best overcome the defences of the virus in the effort to achieve effective antibody-based treatment and prevention."
Follow James on Twitter.
Image copyright Jessica Thompson Image caption Burials at Mount Hora in Malawi yielded DNA used in the study

DNA from ancient remains has been used to reconstruct thousands of years of population history in Africa.
Researchers sequenced the genomes of 16 individuals who lived between 8,000 and 1,000 years ago.
The data shows how the invention and spread of farming had a major impact on the genes of people in Africa - just as it did in Europe and Asia.
The findings are published in the journal Cell.
The results suggest that populations related to the indigenous people of southern Africa had a wider distribution in the past.
This southern African-like genetic background is found in hunter-gatherers from Malawi and Tanzania in the east of the continent.
These hunters lived between 8,100 and 1,400 years ago.
But the later spread of farmers from western Africa had a major impact on the genetic make-up of people in surrounding regions.
Further DNA analysis revealed the hunter-gatherers in eastern Africa had mixed extensively with the incoming farmers.
The researchers estimate that the mixing occurred between 8,000 and 4,000 years ago.
The study also found possible evidence of migration into Africa from the Middle East.
About 38% of the ancestry of a 3,100-year-old livestock herder from Tanzania was related to ancient farmers from the Levant region.
"These results document a prehistoric population landscape that we didn't know about," said co-author Pontus Skoglund, from Harvard Medical School, US.
"They document how farmer and herder migrations swept through eastern and southern Africa."
The researchers also found tentative evidence of adaptive evolution - changes driven by environmental pressure - for genes involved in taste in the ancient individuals.
These taste receptors are known to be important for detecting and learning to avoid poisonous plants.
Image copyright Fredex8

The UK and US have reached a deal to develop a special relationship for science.
An agreement between the two countries aims to make it easier for researchers to travel, collaborate and share facilities.
US science bodies are said to be "eager" to take advantage of research opportunities lost because of Brexit.
The deal is part of government efforts to develop research collaborations outside the EU.
BBC News revealed earlier this year when the deal was being negotiated that the aim was to develop a legal framework to allow a freer flow of people, research grants and tariff-free exchange of equipment between the two countries.
Possible strategic areas of collaboration include:

synthetic biology

information technology

GM research

Image copyright SANGER CENTRE Image caption America and Britain were key partners on the project to decode the human genome

The agreement, signed by the Science Minister Jo Johnson and his US counterparts, states that national laws will "seek to facilitate" freer movement of people and scientific equipment.
Speaking in Washington, Mr Johnson said that the deal would help to ensure that the UK would maintain its global lead in many areas of research.
"Our continued collaboration with the US on science and innovation is beneficial to both of our nations, and through this agreement we are sharing expertise to enhance our understanding of many important topics that have the potential to be world changing," the minister added.
The impetus for the deal came following the UK referendum result to leave the European Union.
British universities, in collaboration with small businesses, receive 850m in research grants each year from membership of the EU's research programmes.
EU membership also makes it easy to form collaborations.
There are fears that much of the funding and collaborative work with EU scientists will be in jeopardy once the UK leaves the EU, despite recent assurances from the Brexit Secretary, David Davis, that his aim is to foster even closer scientific relationships with the EU.
Prof Venki Ramakrishnan, president of the Royal Society, welcomed the deal with the US but remains concerned that Mr Davis' aspiration to have closer collaboration with the EU lacks any detail on how to achieve his aim.
"This agreement sends a welcome message that UK science remains outward looking.
International research collaboration allows the rapid exchange of new ideas and expertise and it also allows us to address problems that no one country can on its own.
It is an essential part of modern science," he said.
Image copyright NSF/LIGO Image caption A wider partnership could give UK researchers even more access to US facilities

British researchers are being encouraged to foster links with other nations.
While most if not all research leaders are still dismayed by the referendum result - some are beginning to see advantages for greater collaboration with the US.
For example, there is scope for greater freedom in research in synthetic biology and information technology because biotechnology and privacy regulation is less restrictive outside the EU.
For their part, US science bodies see Brexit as an opportunity for them.
US research leaders are anxious that American research groups fill any shortfall left by the UK's departure from the EU, rather than their rivals in India and China.
Mr Johnson announced that the government had pledged 65m to participate in a US-based and led international project to learn more about sub-atomic particles called neutrinos.
He said he hoped it would be the first of many more UK research collaborations with the US.
Media playback is unsupported on your device Media caption Pallab Ghosh looks at how the neutrino beam would be fired underground

The UK-led Dune project involves 150 scientists from 14 British universities and two laboratories run by the UK's Science and Technologies Facilities Council participating in a US-based effort involving 1,000 scientists from 31 countries.
The UK was likely to participate in Dune before the referendum result, but the project received greater support from ministers subsequently because it fitted well with the government's narrative to reassure the British scientific community that Brexit would free them to take up new opportunities outside the EU.
Echoing that message Sir Mark Walport, currently the government's chief scientific adviser and soon to be chief executive of UK Research and Innovation, the body that will oversee funding of civil government research, said that the agreement sent a "clear signal that UK researchers are outward looking and ready to work with the best talent wherever that may be".
"UK Research and Innovation is looking forward to extending partnerships in science and innovation around the world," he added.
Follow Pallab on Twitter
Image copyright Getty Images Image caption Barn owls rely on their hearing to hunt

Barn owls keep their acute sense of hearing into old age, scientists have discovered.
Previously, starlings have been found to have this ability, suggesting birds are protected from age-related hearing loss.
Understanding more about the "ageless ears" of barn owls could help develop new treatments for human hearing problems.
Birds are able to naturally repair damage to the inner ear.
Georg Klump of the University of Oldenburg, Germany, a researcher on the study, said owls keep their hearing into very old age.
"Birds can repair their ears like (humans) can repair a wound," he said.
"Humans cannot re-grow the sensory cells of the ears but birds can do this."
It appears that humans lost these regenerative abilities at some point in evolution.
Like all mammals, people commonly suffer from hearing loss in old age.
By the age of 65, humans can expect to lose more than 30 dB in sensitivity at high frequencies.
Commenting on the study, Dr Stefan Heller of Stanford University School of Medicine, said work was underway to investigate differences between birds and mammals.
"To truly utilise this knowledge, we need to conduct comparative studies of birds and mammals that aim to find the differences in regenerative capacity, a topic that is actively pursued by a number of laboratories worldwide," he said.
The research, published in the journal, Royal Society Proceedings B, was carried out on seven captive barn owls.
The birds were trained to fly to a perch to receive a food reward in response to sounds.
Even the oldest owl, which reached the ripe old age of 23, showed no signs of age-related hearing loss.
Barn owls typically only live to the age of three or four in the wild.
The birds rely on their hearing to hunt prey at night.
Follow Helen on Twitter.
Image copyright Kathy Niakan Image caption The genetic machinery needed to modify the DNA is injected into the embryo

The blueprint for life - DNA - has been altered in human embryos for the first time in the UK.
The team at the Francis Crick Institute are unravelling the mysteries of the earliest moments of life.
Understanding what happens after a sperm fertilises an egg could lead to ways of improving IVF or explain why some women miscarry.
The embryos were modified shortly after fertilisation and allowed to develop for seven days.
The researchers are exploring one of the most astounding of transformations.
We have all journeyed from a single fertilised egg to a human being - built from myriad different tissues ranging from bone to those needed to read this page.
The first few steps on that journey are as critical as they are poorly understood.
Image copyright Kathy Niakan Image caption The embryo divides and develops from a single fertilised egg (top left) to a blastocyst (bottom right)

Breakthroughs in manipulating DNA have allowed the team at the Crick to turn off a gene - a genetic instruction - suspected to be of vital importance.
The easiest way of working out how something works is to remove it and see what happens.
So the researchers used the gene-editing tool Crispr-Cas9 to scour the billions of letters of genetic code, find their genetic target and break the DNA to effectively disable it.
They were targeting a gene.
You are unlikely to have heard of it, but OCT4 is a superstar in early embryo development.
Its complete role is not understood but it acts like an army general issuing commands to keep development on track.
The researchers used 41 embryos that had been donated by couples who no longer needed them for IVF.
After performing the genetic modification, the team could watch how the embryos developed without OCT4.
Media playback is unsupported on your device Media caption In a first for UK scientists, human embryos have been genetically modified.
Over the course of the first seven days, a healthy, normal embryo goes from one cell to about 200.
It also goes through the first steps of organising itself and handing out specialised jobs to different cells.
The embryo forms a hollow sphere called a blastocyst, with some cells destined to go on to form the placenta, some the yolk sac and others, ultimately, us.
But without OCT4 the blastocyst cannot form.
It tries - but implodes in on itself.
From the embryo's perspective it is a disaster but for scientists it has given unprecedented insight.
Image copyright Kathy Niakan Image caption The cells coloured green in the blastocyst have high levels of OCT4 and are the ones that go on to form the human body

It is the first time human embryos have been edited to answer questions about fundamental biology.
Dr Kathy Niakan, a group leader at the Crick in London, told the BBC: "When it seemed it was working we were quite excited about the possibility that this would open up.
"This is basic research which is providing us with a foundation of knowledge about early human development."
By deepening understanding of the earliest moments in life, it could help explain what goes wrong in infertility.
During IVF, of 100 fertilised eggs, fewer than 50 reach the blastocyst stage, 25 implant into the womb and only 13 develop beyond three months.
This study alone, published in the journal Nature, cannot explain what is going wrong or why some women miscarry.
But by interrogating all the genes suspected of playing a role in our inception, it could lead to new advances.
Image copyright crick institute Image caption Dr Kathy Niakan in the Crick laboratories where the embryos were modified

Dr Niakan told the BBC: "If we knew the key genes for an embryo to develop successfully that would, I would hope in the future, lead to improvements in IVF technology and give us really important insights into why some pregnancies fail."
One option for IVF is to have a better way of testing which embryos are going to be successful.
Or it may be possible to boost embryos during IVF by growing them in a different culture media - a fertiliser for fertilised eggs.
Ethical debate

These experiments have been legal since 2008 in the UK, where it is possible to manipulate such embryos for 14 days as long as they are not implanted.
But while this application of the technology is answering fundamental questions of science, other research groups are trying to remove genes that cause disease.
That is provoking deep ethical debate.
Dr Sarah Chan, a bioethicist at the University of Edinburgh, told the BBC: "I don't think this study should raise any ethical concerns.
"It is very clear that the aim of the research was basic science and that there was never any intention to create genetically modified human beings.
"That said if we could one day use gene editing in human embryos for medical purposes, the potential benefits could be huge, but before we took such a step we would want to make sure that we'd had a really robust and wide-ranging public dialogue on all of the ethical issues involved."
Dr Rob Buckle, the chief science officer at the UK Medical Research Council, said: "Genome editing technologies are having a game-changing effect on our ability to understand the function of critical human genes.
"As genome editing techniques develop it's vital that this work continues within a robust yet adaptable regulatory framework so that its full potential can be realised in a scientific, ethical and legally rigorous way."
Follow James on Twitter.
Image copyright Colin Richards / UHI Image caption Ring of Brodgar is a famous Orkney site

Rivalries in Orkney more than 4,500 years ago led to competition between communities including over how people were buried, according to new research.
Scientists were able to gather much more precise estimates of the timing and duration of events in the period around 3200-2500 BC by examining more than 600 radiocarbon dates.
The study challenges many previously-held ideas about Neolithic Orkney.
The study has been published in the journal Antiquity.
It was led by Prof Alex Bayliss from Historic England, with Prof Colin Richards of the University of the Highlands and Islands in Kirkwall as co-author.
It is part of a wider project called The Times of Their Lives.
Ritual clues

The study concludes that seemingly rapid changes in settlements and monuments indicate that there were rivalries and tension between social groups.
This was played out in how they buried their dead and in their communal gatherings and rituals.
The study covered famous Orkney sites including Skara Brae and Maeshowe.
Key dates indicated by the study

Orkney was probably first colonised in 3600 BC

Settlement peaked in the period 3100-2900 BC

There was a phase of decline 2800-2600 BC, measured by the number of stone houses in use

Settlement resumed in 2600-2300 BC, and it could have been about this time that the Ring of Brodgar itself was erected.
Prof Bayliss said: "This study shows that new statistical analysis of the large numbers of radiocarbon dates that are now available in British archaeology really changes what we can know about our pasts.
"People in the Neolithic made choices, just like us, about all sorts of things - where to live, how to bury their dead, how to farm, where and when to gather together - and those choices are just beginning to come into view through archaeology.
"It's an exciting time to be an archaeological scientist."
Image copyright Colin Richards / UHI

Prof Richards said: "Our study shows how much remains to be discovered in Orkney about the Neolithic period, even though it may appear well known."
Prof Alasdair Whittle of Cardiff University is the lead investigator for The Times of Their Lives.
Prof Whittle said: "Visitors come from all over the world to admire the wonderfully preserved archaeological remains of Orkney, in what may seem a timeless setting.
"Our study underlines that the Neolithic past was often rapidly changing, and that what may appear to us to be enduring monuments were in fact part of a dynamic historical context."
Image copyright AFP Image caption Renewable energy in Nicaragua

The 2015 Paris agreement's ambitious goal of limiting global warming to 1.5C remains within reach, a study suggests.
The study is one of several to address the "carbon budget", which - among other things - determines how much CO2 the planet can emit and still reach a given limit for global warming.
It indicates the 2015 target, perceived by some as tough, could be met with very stringent emissions cuts.
It used computer models that project climate behaviour into the future.
The aim of the Paris deal was "holding the increase in global average temperature to well below 2C above pre-industrial levels and pursuing efforts to limit temperature increase to 1.5C."
But scientists admit they were taken by surprise by the ambition of the 1.5C figure.
The results of the work with computer models have been published in Nature Geoscience.
This type of work necessarily contains uncertainties regarding the way the Earth's climate will respond in future and how quickly societies can move away from fossil fuel use.
But the study authors say: "Pursuing 'efforts to limit the temperature increase to 1.5C' is not chasing a geophysical impossibility".
Co-author Michael Grubb, from University College London, said: "This paper shows that the Paris goals are within reach, but clarifies what the commitment to 'pursue efforts to limit the temperature increase to 1.5C' really implies."
Those commitments would require strengthening the nationally determined contributions (NDCs) - the pledges to cut emissions contained in the Paris agreement.
Previous estimates of the remaining 1.5C carbon budget, based on the Intergovernmental Panel on Climate Change's (IPCC) Fifth Assessment of the climate, were around four times lower.
But unlike those figures, which relied on one line of evidence, the new study uses multiple approaches to examine the same question, arriving at a rather different result.
Co-author Prof Pierre Friedlingstein, from the University of Exeter, said: "This is very good news for the achievability of the Paris targets."
Prof Myles Allen, another author, from the University of Oxford, told BBC News: "In the main body of the IPCC assessment, what it would take to meet a 1.5C goal wasn't assessed in any detail.
To be honest, it wasn't thought to be the policy priority at the time.
"Perhaps it should have been, but that was the view of the academic community then.
But the ambition of Paris caught a lot of people by surprise."
Analysis by David Shukman, BBC Science Editor:

The climate models are exaggerating.
The predictions are too alarmist.
The Tuvaluans and other islanders are safer than we thought.
These are among the conclusions that some might reach from this latest work.
In reality, nothing is quite that straightforward.
The models are simulated approximations of possible futures.
Inevitably they are going to be at least slightly adrift of reality, either in the amount of warming or its timing.
They come with caveats and margins of error.
In many ways, it's remarkable that these computer constructs are even roughly on track.
And models designed to come up with very broad potential outcomes for the end of the century may not be fine-tuned enough to give more detailed forecasts year-by-year.
The authors themselves are anxious that their research is not misunderstood.
The need for urgent action to reduce emissions is unchanged, they say.
It's just that the most ambitious of the Paris Agreement targets is not as unachievable as many once thought, that there is time to act, though the task remains a monumental one.
Myles Allen added: "For a two in three chance of keeping temperatures within 1.5C, we'd have to reduce emissions in a straight line to zero from where we are now over the next 40 years.
"It's possible, but extremely challenging.
So if people are saying: can we now relax?
That's not the right message to take at all."
Different take

Scientists agree urgent action will be needed to tackle the effects of rapid temperature increase over the next century.
But a study earlier this year in the journal Nature Climate Change suggested the allowable carbon budget had probably been overestimated.
It said the "pre-industrial baseline" used to benchmark present day warming was probably older than the IPCC had assumed.
Therefore, the degree of warming since that baseline was probably greater than had been believed.
On Twitter, one of the authors of that report, Prof Michael Mann, said the latest research in Nature Geoscience, "doesn't account for [the] pre-industrial baseline issue we examined".
He added: "There is some debate about [the] precise amount of committed warming if we cease emitting carbon immediately.
We're probably very close to 1.5C."
Meanwhile, another paper in Nature Geoscience, by Gunnar Myhre, from the Center for International Climate and Environmental Research, in Oslo, and colleagues, suggests the greenhouse effect caused by human-induced CO2 emissions is now half-way to doubling compared with pre-industrial conditions.
Although the concentrations themselves have not yet reached the halfway mark, this is being described as an iconic watermark.
Follow Paul on Twitter.
Image copyright Getty Images Image caption The large and the small are most at risk of extinction, on land and in water

The biggest and the smallest of the world's animals are most at risk of dying out, according to a new analysis.
Size matters when it comes to extinction risk, with vertebrates in the so-called "Goldilocks zone" - not too big and not too small - winning out, say scientists.
Action is needed to protect animals at both ends of the scale, they say.
Heavyweights are threatened mainly by hunting, while featherweights are losing out to pollution and logging.
"The largest vertebrates are mostly threatened by direct killing by humans," said a team led by Prof Bill Ripple of Oregon State University in Corvallis, US.
"Whereas the smallest species are more likely to have restricted geographic ranges - an important predictor of extinction risk - and be threatened by habitat degradation."
Image copyright Jurgen Leckie Image caption The great hammerhead shark is under threat from illegal fishing

The research adds to evidence that animals are dying out on such a scale that a sixth extinction is considered under way.
This has prompted efforts to determine the key drivers of extinction risk.
One clue is body size.
Research on birds and mammals has shown that those with larger bodies are more likely to go extinct.
Yet, when the researchers made a data base of thousands of birds, mammals, fish, amphibians and reptiles at risk of extinction, they found disproportionate losses at the large and small ends of the scale.
"Surprisingly, we found that not only the largest of all vertebrate animal species are most threatened, but the very tiniest ones are also highly threatened with extinction," Prof Ripple told BBC News.
Image copyright Dave Young Image caption Warty swamp frog: This frog is believed to be in decline across much of its range

Large charismatic animals, such as elephants, rhinos and lions have long been the target of protection efforts.
However, fish, birds, reptiles and amphibians that are the giants of their kind, such as the whale shark, Somali ostrich and Chinese giant salamander, tend to be overlooked.
Meanwhile, small species at risk - such as frogs and shrews - receive very little attention.
"I think, for the smallest species, first of all we need to bring higher awareness to them, because the larger ones get a lot of attention, but the smaller ones get very little," said Prof Ripple.
In the study, researchers from the US, UK, Switzerland and Australia compared body mass and extinction risk for more than 25,000 vertebrate species.
Of these, around 4,000 are threatened with extinction, as assessed by the Red List of the International Union for the Conservation of Nature.
Vertebrates with the smallest and the largest bodies were found to be most at risk of disappearing, whether they were on land or living in oceans, streams or rivers.
Image copyright R Hutterer Image caption The Canarian shrew is a tiny endangered mammal living only on the Canary Islands

Threats facing the heaviest included:

Regulated and unregulated fishing

Hunting and trapping for food, trade or medicines

The lightest were mainly at risk from:

Pollution of lakes, streams and rivers

Farming

Logging of forests

Development.
Image copyright Factcatdog Image caption The Bavarian pine vole is thought to be critically endangered

The researchers say that while different approaches are needed for the conservation of large versus small species, there is an urgent need to step up efforts for both.
"Ultimately, reducing global consumption of wild meat is a key step necessary to reduce negative impacts of human hunting, fishing, and trapping on the world's vertebrates," they write in the journal, Proceedings of the National Academy of Sciences, which published the study.
"Indeed, based on our findings, human activity seems poised to chop off both the head and tail of the size distribution of life."
Extinction can be a natural process, affecting a handful of species each year.
However, estimates suggest the world is now losing species at hundreds of times the "background" rate.
Co-researcher Thomas Newsome of the University of Sydney in Australia said for large animals lessening the negative impacts of hunting, fishing and trapping was key.
"But it's ultimately slowing the human population growth rate that will be the crucial long-term factor in limiting extinction risks to many species," he said.
Follow Helen on Twitter.
Image copyright Wildlife Justice Commission

Criminal networks smuggling rhino horn out of Africa are turning it into jewellery to evade its detection in airports, an investigation has found.
Wildlife trade monitoring network Traffic revealed an "emerging trend" of making and smuggling beads, bracelets and bangles and rhino horn powder.
The lead investigator told BBC News the trade in rhino horn was now "morphing" into a market for luxury items.
At least 7,100 rhinos are estimated to have been killed in Africa since 2007.
Today, about 25,000 of the animals remain.
Julian Rademeyer from Traffic explained that the production of rhino horn "trinkets" mirrored some of the patterns seen in the trade in ivory.
"It's very worrying," he told BBC News.
"Because if someone's walking through the airport wearing a necklace made of rhino horn, who is going to stop them?
"Police are looking for a piece of horn and whole horns."
Status symbol

The primary destinations for smuggled rhino horn remain the same; the largest markets are in China and Vietnam.
But this investigation also found that smuggling routes constantly changed and adapted, becoming more complex in order to avoid countries and airports where law enforcement resources were being focused.
This shift in how horn is processed before it is moved could make it more difficult to detect.
"This is quite a preliminary assessment," explained Mr Rademeyer, "but it's vital that there's information sharing about these new trends - particularly with law enforcement."
He added that the market for medicinal rhino horn - believed by many to be a cure for a range of illnesses, from rheumatism to cancer - seemed to have "reduced somewhat".
But owning rhino horn - particularly for wealthy men in Vietnam - is also seen as a status symbol.
"It's about power - about showing off your wealth," said Mr Rademeyer.
"It's been called the Ferrari factor - having something says you are wealthy and that you're untouchable [by the law]."
Image copyright TRAFFIC

Susie Offord-Woolley, managing director of the charity Save the Rhino International, said this kind of information was "essential" in order that law enforcement officers could be trained to identify rhino horn jewellery.
"The fact they're carving [the horn] up now means these gangs are getting more concerned about security, and that's a good sign," she added.
At the current rate of poaching, Save the Rhino says that rhinos could be extinct in the wild within the next 10 years.
"That's what we're all trying to avoid," said Ms Offord-Woolley.
And while this is a fight to save a species, she added, "this also affects so many people".
She added: "In last 10 years, 1,000 rangers have been killed in Africa while on patrol protecting rhinos.
"So this is an issue for people's lives, as well."
Follow Victoria on Twitter
Image copyright AFP Image caption Scientists are emphasising that the big cats' new status does not mean they are safe from extinction

Has the chilling threat of extinction worn off at last for the long-endangered snow leopard?
Not exactly - but the iconic big cats' conservation status has been improved from "endangered" to "vulnerable".
The decision was announced by the International Union for Conservation of Nature (IUCN) - the global standard for assessing extinction risk.
Experts have warned that the species still faces serious threats from poaching and habitat destruction.
The elegant yet elusive creatures, which live in the mountains of central Asia, were first listed as endangered by the IUCN in 1972.
The status change followed a three-year assessment process by five international experts.
Dr Tom McCarthy, who runs the Snow Leopard Programme at big cat charity Panthera, was one of them.
"To be considered 'endangered,' there must be fewer than 2,500 mature snow leopards and they must be experiencing a high rate of decline," he explained.
"Both are now considered extremely unlikely, which is the good news, but it does not mean that snow leopards are 'safe' or that now is a time to celebrate.
"The species still faces 'a high risk of extinction in the wild', and is likely still declining - just not at the rate previously thought."
Image copyright AFP/Getty Images Image caption A baby snow leopard pictured at the Tierpark zoo in Berlin

Being classed as "vulnerable" means a species has under 10,000 breeding animals left, with a population decline of at least 10% over three generations.
The Snow Leopard Trust, which aims to protect the big cat through community projects, strongly opposes the status change.
It plans to challenge the decision with the IUCN.
"We believe it could have serious consequences for the species," it wrote in a blog post.
Snow leopard researchers believe the species' decline may have been slowed by conservation projects - including some to protect farm animals from the predators, which are sometimes killed in revenge for livestock losses.
The number of protected areas within the snow leopards' habitat has also increased significantly in recent decades.
Snow leopard stats

The rarely-sighted cats live in the craggy peaks of central Asia - including the Himalayas, and Russia's remote Altai mountains

Their habitat covers more than 1.8 million sq km / 694,980 sq miles, across 12 countries

Scientists say they are threatened by poaching for their fur, infrastructure developments, and climate change

Usually found at elevations of 3,000-4,500m (11,480-14,760ft)

Solitary creatures, they usually hunt at dawn and dusk and are able to kill prey up to three times their own weight

Mostly feed on wild animals, but will also prey on livestock

Their spotted coats change with the seasons - from a thick, white fur to keep them warm and camouflaged in winter, to a fine yellow-grey coat in summer

Retaliatory killings by farmers are not uncommon, but are rarely reported
Image copyright Reuters Image caption The Paris deal commits the signatories to keeping rising global temperatures "well below" 2C above pre-industrial levels

The US has insisted it will leave the Paris climate accord, despite reports that it may be softening its stance.
Following a meeting of environment ministers on Saturday, the EU climate commissioner, Miguel Arias Canete, said Trump officials had indicated the US would either stay in the 2015 accord or review its terms.
But the White House insisted there had been "no change" in the US position.
In June President Donald Trump said the US would withdraw from the deal.
He said it was part of his "solemn duty to protect America" and he would seek a new deal that would not disadvantage US businesses.
But opponents say withdrawing from the accord is an abdication of US leadership on a key global challenge.
The Paris agreement commits the US and 187 other countries to keeping rising global temperatures "well below" 2C above pre-industrial levels and "endeavour to limit" them even more, to 1.5C.
Only Syria and Nicaragua did not sign up to the deal.
What did Trump originally announce in June?
Speaking in the White House Rose Garden, he characterised the Paris agreement as a deal that aimed to hobble, disadvantage and impoverish the US.
He claimed the agreement would cost the US 6.5 million jobs and $3tn (2.2tn) in lost GDP - while rival economies like China and India were treated more favourably.
Media playback is unsupported on your device Media caption Trump: The world won't laugh any more at US

"In order to fulfil my solemn duty to protect America and its citizens, the United States will withdraw from the Paris climate accord... but begin negotiations to re-enter either the Paris accord or a really entirely new transaction on terms that are fair to the United States," he said.
During his visit to France in July, however, Mr Trump hinted that the US could shift its position on the deal - but did not elaborate.
"Something could happen with respect to the Paris accord... We'll see what happens."
What is now being reported?
On Saturday, the Wall Street Journal quoted Mr Arias as saying that Trump administration officials said the US would not pull out of the agreement, and were offering to re-engage in the deal.
The WSJ said the shift in the position came at a meeting of environment ministers from about 30 countries at a gathering in Montreal, Canada.
That meeting was attended by a US observer.
Media playback is unsupported on your device Media caption Matt McGrath explains why we should care about climate change

The US "stated that they will not renegotiate the Paris Accord, but they [will] try to review the terms on which they could be engaged under this agreement," Mr Canete said.
He said that "there would be a meeting on the sidelines of next week's UN General Assembly with American representatives "to assess what is the real US position", according to the AFP news agency.
"It's a message which is quite different to the one we heard from President Trump in the past," Mr Canete added.
At the same time, Chilean Environment Minister Marcelo Mena tweeted (in Spanish): "I was in the meeting, and the [US] negotiator effectively did not close the door on continuing in the agreement, and ruled out looking for a new agreement."
But in a statement later on Saturday, White House spokeswoman Sarah Huckabee Sanders said: "There has been no change in the United States' position on the Paris agreement,

"As the president has made abundantly clear, the United States is withdrawing unless we can re-enter on terms that are more favourable to our country."
How could this change things?
Bloomberg reported that the US is "no longer seeking to withdraw from the pact and then renegotiate it, but rather wants to re-engage with the Paris Agreement from within".
While the White House insists its stance has not changed, deciding not to withdraw from the Paris deal and instead focus on negotiating while remaining a signatory would represent a significant about-turn.
The Los Angeles Times said staying in the Paris deal would be "one of the most controversial" reversals of the Trump presidency.
It would also risk angering Mr Trump's more conservative supporters at a time he is facing criticism for engaging with Democratic leaders, the liberal magazine Mother Jones wrote.
Media playback is unsupported on your device Media caption "Long live Cassini" - the project team says farewell

The American-led Cassini space mission to Saturn has just come to a spectacular end.
Controllers had commanded the probe to destroy itself by plunging into the planet's atmosphere.
It survived for just over a minute before being broken apart.
Cassini had run out of fuel and Nasa had determined that the probe should not be allowed simply to wander uncontrolled among Saturn and its moons.
The loss of signal from the spacecraft occurred pretty close to the prediction.
Here at mission control, at the Jet Propulsion Laboratory (JPL) in Pasadena, California, the drop-off was timed at 04:55 PDT (11:55 GMT; 12:55 BST).
How the last hours unfolded

Nasa's Earl Maize addressed fellow controllers: "Congratulations to you all.
This has been an incredible mission, an incredible spacecraft and you're all an incredible team.
I'm going to call this the end of mission.
Project manager off the net."
The statement brought restrained applause and some comforting embraces.
The loss of signal indicated that the probe was tumbling wildly in the planet's gases.
Travelling downwards at over 120,000km/h, it could have survived the violence for no more than about 45 seconds before being torn to pieces.
Incineration in the heat and pressure of the plunge was inevitable - but Cassini still managed to despatch home some novel data on the chemical composition of Saturn's atmosphere.
Cassini: A mission of 'astonishing discovery'

Sorry, your device is unsupported.
Tap To Progress

Swipe to go back Click Arrows To Progress



Loading .
Tap To Progress Images: Nasa, Kongle Go back to start

And so ends one of the most successful space missions in history.
In its thirteen years at Saturn, Cassini has transformed our understanding of the sixth planet from the Sun.
It has watched monster storms encircle the globe; it witnessed the delicate interplay of ice particles move through the planet's complex ring system; and it revealed extraordinary new insights on the potential habitability of Saturn's moons.
Titan and Enceladus were the standout investigations.
The former is a bizarre place where liquid methane rains from an orange sky and runs into huge lakes.
Cassini put a small European robot called Huygens on Titan's surface in 2005.
It returned a remarkable image of pebbles that had been smoothed and rounded by the action of that flowing methane.
Cassini also spied what are presumed to be volcanoes that spew an icy slush and vast dunes made from a plastic-like sand.
On Enceladus, the observations were no less stunning.
Media playback is unsupported on your device Media caption "Where Cassini took its final plunge": Linda Spilker explains the last images

Image copyright NASA Image caption Controllers' screens: The loss of signal was indicated by the absence of a green spike (right)

This moon was seen to spurt water vapour into space from cracks at its south pole.
The H2O came from an ocean held beneath the icy shell of Enceladus.
When Cassini flew through the water plumes, it showed that conditions in the sub-surface ocean were very probably suitable for life.
Today, scientists are already talking about how they can go back with another, more capable probe to investigate this idea further.
Image copyright NASA/JPL-CALTECH/SSI/JASON MAJOR Image caption Cassini's cameras were switched off for the last few hours leading to the plunge

A great many of those researchers have been gathered this week at the nearby campus of the California Institute of Technology.
They watched a feed from the control room at JPL on giant screens.
Jonathan Lunine, from Cornell University at Ithaca, New York, spoke for many when he said: "I feel sad but I've felt sad the whole week; we knew this was going to happen.
And Cassini performed exactly as she was supposed to and I bet there is some terrific data on the ground now about Saturn's atmosphere."
And Linda Spilker, the Nasa Cassini project scientist, added: "When I look back on the Cassini mission I see a mission that was running a 13-year marathon of scientific discovery.
And this last orbit was just the last lap.
And so we stood in celebration of successfully completing the race.
And I know I stood there with a mixture of tears and applause."
Although the probe has gone its science lives on.
It has acquired a huge amount of data that will keep researchers busy for decades to come.
A lot of it has barely even been assessed.
"Linda Spliker and I were joking earlier that those last few seconds of the Cassini mission - our first 'taste' of the atmosphere of Saturn - might be a number of PhD theses for students to come," observed Michael Watkins, the director of JPL.
"So, even in those last few seconds, it will continue its re-writing of the textbooks."
Image copyright NASA/JPL-CALTECH/SSI Image caption Pictures of Titan were pulled off the probe's memory before the plunge

Media playback is unsupported on your device Media caption Why scientists are so excited about Saturn's icy moon Enceladus

Saturn's moons: The strange and the wonderful

Image copyright NASA/JPL-CALTECH/SSI

Cassini has photographed some of Saturn's 62 moons: the two-tone Iapetus (1) with its walnut-like equatorial ridge; Mimas (2), which has a giant crater that instantly makes everyone think of the "Death Star" from the Star Wars movies; Hyperion (3), which displays clusters of bizarre pock marks akin to a sponge or wasps' nest; Atlas (4), which resembles a flying saucer; the potato-like Prometheus (5); and Pan (6), which has a shape that would not look out of place in a ravioli dish.
Image copyright NASA/JPL-CALTECH/SS Image caption A last look at Enceladus going over the northern limb of Saturn

Image copyright NASA/JPL-CALTECH/SSI Image caption Also in the end group of images was this fabulous shot of the rings

The Cassini-Huygens mission is a joint endeavour of Nasa, and the European and Italian space agencies.
The BBC has plenty of coverage of the ending of the mission on both TV and radio.
Inside Science ran a preview of the climax on Radio 4.
A Horizon documentary will also review the mission and the final hours in a special programme to be broadcast on Monday 18 September at 21:00 BST on BBC Two.
And you can still watch the Sky At Night programme Cassini: The Gamechanger on the iPlayer.
There is currently a problem with an interactive element on this page.
Please try loading this page again in a little while.
Jonathan.Amos-INTERNET@bbc.co.uk and follow me on Twitter: @BBCAmos
Image copyright RHS/Mike Sleigh Image caption Bright blue berries on the shrub, harlequin glorybower

Berries are appearing on plants and hedgerows early this year because of the unusual weather patterns.
The combination of a warm, dry spring, followed by July and August rains, has led to a plethora of berries, according to horticulturists.
"Berries are a vital part of gardens and wildlife, and things have come together this year to make an abundant and beautiful crop," said Guy Barter, chief horticulturist at the Royal Horticultural Society (RHS).
Plants that are already bearing berries include spindle bushes (Euonymus) and firethorn (Pyracantha), while crab apples are also ripening early.
The fruits are likely to coincide with the appearance of autumn colour on leaves.
Image copyright Anna Brockman/RHS Image caption Skimmia japonica: The glossy green fruit ripen to bright red in autumn

Image caption Firethorn (Pyracantha)

"At some stage, the autumn colours will form and you will get these wonderful colour combinations of reds, blacks, yellows and purples - something to look forward to," he added.
Trevor Dines of the charity, Plantlife, said there have been near-perfect conditions for good fruit in our hedgerows this year.
The dry warm spring encouraged pollinating bees, wasps and flies to be out at peak flowering times in April and May.
Then, the warm, wet summer was perfect for fruit development, with water around to swell the berries.
Meanwhile, autumn colour is also on display in some areas.
"With the return to wetter conditions over summer, it's been a bit of an extended growing season and so it's not surprising that we're now seeing fruit set and autumn colours arriving three to five weeks earlier than normal," said Dr Dines.
"Oak trees in north Wales are already starting to turn colour - you'd normally not see that until late October."
Image copyright RHS Image caption Berries can be seen on many plants at RHS Wisley in Surrey

Berries are a valuable source of food for wildlife, particularly birds.
Thrushes, blackbirds, redwings and fieldfares feast on berries throughout the winter.
The seeds pass out through the bird's gut and are often deposited far away, helping to spread plants far and wide.
Follow Helen on Twitter.
Image copyright Queen's University Belfast Image caption The flexible organic supercapacitor could last three times longer than conventional batteries

Queen's University Belfast scientists have designed a new flexible organic battery that could revolutionise how medical implants are powered.
Devices such as pacemakers are currently fitted with rigid metal based batteries, which can cause discomfort.
The charge in the batteries is set to last three times as long as in their conventional counterparts.
As it is decomposable, the organic battery is also expected to have environmental benefits.
Research leader Dr Geetha Srinivasan from Queen's University's Ionic Liquid Laboratories (QUILL) research centre said the device was also non-flammable and had no leakage issues.
She said the "flexible supercapacitor" could be used to power body sensors such as pacemakers.
Image copyright Queen's University Belfast Image caption Dr Geetha Srinivasan said the new battery would be safer than the batteries currently used in pacemakers

"In medical devices such as pacemakers and defibrillators there are two implants, one which is fitted in the heart and another which holds the metal based, rigid batteries - this is implanted under the skin," said Dr Srinivasan.
"The implant under the skin is wired to the device and can cause patients discomfort as it is rubs against the skin.
"For this reason batteries need to be compatible to the human body and ideally we would like them to be flexible so that they can adapt to body shapes."
Foldable laptops?
Dr Srinivasan said the new battery would be safer than the batteries currently in use.
"It avoids using flammable solvents, therefore you don't have the hazard of explosion," she said.
Image copyright Queen's University Belfast Image caption The flexible supercapacitor could relieve discomfort for those who have medical implants

The technology could also have a non medical application in foldable phones or laptops of the future, the designs of which are currently constrained by rigid batteries.
"Everyone wants to go lightweight and everyone wants to be flexible," Dr Srinivasan said.
"If the battery goes flexible the whole electronics and equipment can go flexible, it's interesting and exciting."
While current batteries contain toxic materials that are complicated to recycle, organic batteries would simply decompose over time.
The new device would be manufactured with organic composites using "natural feedstock" (biomaterials such as cellulose) rather than expensive metals or semiconductors.
Image caption The new batteries would be manufactured with natural organic composites such as cellulose

But there is no danger of the organic batteries decomposing in the human body as they only start to break down at temperatures above 270C.
With the right funding in place, Dr Srinivasan said the devices could easily be commercialised - so that it could be powering phones or similar devices within the next five years.
Image copyright Thinkstock Image caption Culling will be carried out in 21 areas of England this year

Licences have been issued for badger culling in 11 new areas in Devon, Wiltshire, Somerset, Dorset and Cheshire.
Culling is part of the government's 25-year strategy to eradicate bovine TB, but opponents say there is no evidence it is effective.
A badger vaccination programme to stop the spread of TB is restarting.
The Badger Trust condemned the policy as "politically motivated' and an "insult to the nation's intelligence".
The new licences have been granted by Natural England.
It follows a decision to extend culling in areas of Gloucestershire and Somerset.
The government said it will launch a new advisory service for farmers in high risk areas this autumn advising them how to limit the spread of the disease.
Farming Minister George Eustice said: "Bovine TB not only has a devastating impact on our beef and dairy farms, but causes harm and distress to infected cattle.
"Vaccination is just one part of our comprehensive strategy, which also includes tighter cattle controls, improved biosecurity and badger control in areas where bTB is rife, to tackle the reservoir of disease in wildlife."
'Political aggression'

The government's chief vet Nigel Gibbens said: "Proactive badger control is currently the best available option and the licensing of further areas is necessary to realise disease-control benefits across the high risk area of England, rather than at local levels."
Dominic Dyer, chief executive of the Badger Trust, said 32,247 badgers could be killed under these new licences in the next six weeks.
"The government is simply pandering to its core voters in the farming sector," he said.
"No credible scientist has ever suggested that culling badgers will make any significant impact on lowering TB in cattle and there is now clear evidence the policy is failing badly.
"The government is simply imposing its will in an act of political aggression against both science and the will of the people."
The Department for Environment, Food and Rural Affairs (Defra) said that in 2016, operations in Somerset, Gloucestershire, Dorset, Devon, Cornwall, Herefordshire and Wiltshire were all successful in meeting their targets for culling badgers.
In total, culling will be carried out in 21 areas in the south-west, west and north-west of England this year.
Image caption A Portuguese man-of-war, which was one of a group of six, washed up at Gwithian

Large numbers of potentially fatal Portuguese man-of-war have washed up on a Cornish beach, prompting its closure.
RNLI lifeguards erected do not swim red flags at Perranporth beach earlier because of the "unusually large number" of the creatures.
The jellyfish-like organisms, which have long purple tentacles, have also been seen in Wales this month, says the Marine Conservation Society (MCS).
With mild sea temperatures of 16C there were fears of swimmers being stung.
The RNLI said it placed red flags at Perranporth beach between 10:00 and 13:30 BST to signal that the water was out of bounds, while lifeguards took advice on the level of danger to beachgoers.
More on the man-of-war sightings and other stories from Cornwall

Man-of-war were spotted at Newgale, Pembrokeshire, on 8 September and the next day on beaches near the holiday destination of Newquay.
Image copyright Rachel Wyatt Image caption A leatherback turtle was found washed up at Portreath

They have also been seen at Porthmelon Beach on the Isles of Scilly and on the Cornish beaches of Portheras Cove and Summerleaze, Widemouth, Perranporth, Hayle, Holywell Bay and Praa Sands.
Six were also reported at Gwithian.
Dr Peter Richardson from the MCS said a man-of-war's tentacles, which are usually about 10m (30ft) long, "deliver an agonising and potentially lethal sting".
"They are very pretty and look like partially deflated balloons with ribbons but picking one up could be very nasty," he said.
The man-of-war retain their sting when they are wet, even if they look dead, he warned.
He advised anyone who was stung to get the tentacles away from the body as soon as possible.
What is a Portuguese man-of-war?
Image copyright Joanna Clegg Image caption The man-of-war can be tempting to children because it looks like a deflated balloon

The ( Physalia physalis ) is not a jellyfish, but a floating colony of organisms dependent on one another for survival

) is not a jellyfish, but a floating colony of organisms dependent on one another for survival Its gas-filled bladder (sometimes known as the sail), enables it to float on the ocean surface and drift with the current

Its sting - delivered from tentacles which can reach up to 50m below the surface - is extremely painful for humans and can be fatal in rare circumstances

Hundreds of swimmers are stung every year, especially when huge numbers appear in coastal waters

Leatherback turtles have also been washed up, Dr Richardson said.
A leatherback turtle was found at Portreath on 9 September and another one has been reported in Pembrokeshire.
The NHS recommends using tweezers or a clean stick, and gloves if possible, to remove man-of-war tentacles.
If symptoms become more severe, or a sensitive part of the body has been stung, you should seek medical help.
The MCS is asking people to report any sightings which could rise as man-of-war are driven across the Atlantic by recent storms.
Image copyright RNLI Image caption The RNLI said it wasn't uncommon to see man-of-war after windy conditions

What else would you like to know about man-of-war?
Ask us your questions using the form below and we could be in touch.
Image copyright Stanford University Image caption The study created composite faces judged most and least likely to belong to homosexuals

A facial recognition experiment that claims to be able to distinguish between gay and heterosexual people has sparked a row between its creators and two leading LGBT rights groups.
The Stanford University study claims its software recognises facial features relating to sexual orientation that are not perceived by human observers.
The work has been accused of being "dangerous" and "junk science".
But the scientists involved say these are "knee-jerk" reactions.
Details of the peer-reviewed project are due to be published in the Journal of Personality and Social Psychology.
Narrow jaws

For their study, the researchers trained an algorithm using the photos of more than 14,000 white Americans taken from a dating website.
They used between one and five of each person's pictures and took people's sexuality as self-reported on the dating site.
The researchers said the resulting software appeared to be able to distinguish between gay and heterosexual men and women.
In one test, when the algorithm was presented with two photos where one picture was definitely of a gay man and the other heterosexual, it was able to determine which was which 81% of the time.
With women, the figure was 71%.
"Gay faces tended to be gender atypical," the researchers said.
"Gay men had narrower jaws and longer noses, while lesbians had larger jaws."
But their software did not perform as well in other situations, including a test in which it was given photos of 70 gay men and 930 heterosexual men.
When asked to pick 100 men "most likely to be gay" it missed 23 of them.
In its summary of the study, the Economist - which was first to report the research - pointed to several "limitations" including a concentration on white Americans and the use of dating site pictures, which were "likely to be particularly revealing of sexual orientation".
'Reckless findings'

On Friday, two US-based LGBT-focused civil rights groups issued a joint press release attacking the study in harsh terms.
"This research isn't science or news, but it's a description of beauty standards on dating sites that ignores huge segments of the LGBTQ (lesbian, gay, bisexual, transgender and queer/questioning) community, including people of colour, transgender people, older individuals, and other LGBTQ people who don't want to post photos on dating sites," said Jim Halloran, chief digital officer of Glaad, a media-monitoring body.
"These reckless findings could serve as a weapon to harm both heterosexuals who are inaccurately outed, as well as gay and lesbian people who are in situations where coming out is dangerous."
Image caption Campaigners raised concerns about what would happen if surveillance tech tried to make use of the study

The Human Rights Campaign added that it had warned the university of its concerns months ago.
"Stanford should distance itself from such junk science rather than lending its name and credibility to research that is dangerously flawed and leaves the world - and this case, millions of people's lives - worse and less safe than before," said its director of research, Ashland Johnson.
The two researchers involved - Prof Michael Kosinski and Yilun Wang - have since responded in turn, accusing their critics of "premature judgement".
"Our findings could be wrong... however, scientific findings can only be debunked by scientific data and replication, not by well-meaning lawyers and communication officers lacking scientific training," they wrote.
"However, if our results are correct, Glaad and HRC representatives' knee-jerk dismissal of the scientific findings puts at risk the very people for whom their organisations strive to advocate."
'Treat cautiously'

Previous research that linked facial features to personality traits has become unstuck when follow-up studies failed to replicate the findings.
This includes the claim that a face's shape could be linked to aggression.
One independent expert, who spoke to the BBC, said he had added concerns about the claim that the software involved in the latest study picked up on "subtle" features shaped by hormones the subjects had been exposed to in the womb.
"These 'subtle' differences could be a consequence of gay and straight people choosing to portray themselves in systematically different ways, rather than differences in facial appearance itself," said Prof Benedict Jones, who runs the Face Research Lab at the University of Glasgow.
It was also important, he said, for the technical details of the analysis algorithm to be published to see if they stood up to informed criticism.
"New discoveries need to be treated cautiously until the wider scientific community - and public - have had an opportunity to assess and digest their strengths and weaknesses," he said.
Video

Mount Agung on the Indonesian island of Bali is an active volcano that is at risk of its first eruption in 54 years.
Locals are being evacuated but most tourists are so far unfazed.
Video by Simon Atkinson
Video

The winners have been announced for the Insight Astronomy Photographer of the Year.
The competition received 3,800 entries from amateurs and professional photographers from all over the world.
One of the judges is Dr Marek Kukula.
He tells Dan Damon about the dreamers and scientists behind the images.
(IMAGE & CREDIT: The Rho Ophiuchi Clouds  Artem Mironov (Russia) - STARS & NEBULAE WINNER & OVERALL WINNER.)
How long do elephants spend sleeping?
Researchers find that African elephants in the wild sleep an average of two hours a day and regularly go nearly two days without sleep.
Video

Flies are notoriously hard to swat because they see around four times faster than humans.
They effectively watch you coming in slow-motion.
In fact, scientists discovered that so-called 'killer flies' have the fastest vision of any animal.
Find out more with CrowdScience: Spider silk and super fly senses.
Video

The space race of the Cold War pitted the world's two superpowers against one another to explore what lies beyond Earth.
Now anyone with enough money and enterprise can get there.
But this new race isn't between countries - it's between companies.
Find out more at The Disruptors #thedisruptors @BBCMoney

Producer: Philippa Goodrich; Reporter: Tim Bowler
Video

The small satellite industry is the fastest growing part of the space sector "and they all require launch" says New Zealand's Rocket Lab boss Peter Beck.
His firm aims to disrupt the established space sector by launching its commercial satellite-carrying rockets once a week.
Find out more at The Disruptors #thedisruptors @BBCMoney

Producer: Adrienne Murray; Cameraman: Mauricio Olmedo-Perez
Image caption Could a genetic mutation explain some dogs' insatiable appetite?
When it comes to man's best friend, science may finally have solved the mystery of their gluttony - some Labradors, it seems, are genetically predisposed to being hungry.
That's according to scientists who were discussing their ongoing mission to improve our favourite pets' health at the British Science Association Festival in Brighton.
Several research teams in the UK are on a mission to improve canine health.
Researchers at the University of Cambridge have studied the appetite of Britain's favourite dog breed, and suggest Labradors are genetically at risk of becoming overweight.
Roughly a quarter of British households own a pet dog, and Labrador retrievers remain our most popular canine companion.
However, this stereotypically "greedy" breed often suffers size-related health problems.
Blame the owners?
"Obesity is a serious issue for our dog population," says Dr Eleanor Raffan from the Wellcome Trust-MRC Institute of Metabolic Science.
"It has the potential to have a massive impact on pet welfare."
In research supported by the Dogs Trust, Dr Raffan and her colleagues have analysed DNA from the saliva of Labradors across the UK.
They found that particularly greedy individuals possess a gene mutation responsible for increasing their appetite.
"We found around a quarter of pet Labradors have at least one copy of this mutation in the gene," Dr Raffan explains.
Their increased appetite manifests itself as a "food obsession", familiar to dog-owners as begging or scavenging for food.
In the past, the onus has been on owners to restrict the diet of their pets to prevent excessive weight gain.
But Dr Raffan's research suggests the propensity for large appetites, and hence potential obesity, is hardwired into some individuals.
"We hope to shift the paradigm away from owner-blaming" says Dr Raffan.
"It's a bit more nuanced than just owners needing to be careful."
Freedom from hunger

Dr Raffan cautions against any attempt to breed this "greedy mutation" out of Labrador lines.
While it might predispose the dogs to obesity, a strong focus on food may also explain why Labradors are so easy to train and are such loyal human companions.
"If we try to get rid of the mutation, we might find we change the personality of the breed, and that would be a real shame," she explains.
Yet their results raise an ethical conundrum.
Owners and veterinary surgeons are responsible for providing five core so-called freedoms to animals in their care, including freedom from pain and disease, and freedom from hunger.
Obesity is a disease, and negatively impacts upon canine quality of life.
"But equally, being hungry is a welfare issue," says Dr Raffan.
"And these dogs are genetically hungry."
Dr Raffan hopes future research will improve the satiety of their diets, allowing a feeling of "fullness" without the potential for excessive weight gain.
Bearing the weight

Being overweight undoubtedly reduces a dog's quality of life, and can also affect their ability to cope with arthritis and other underlying joint disorders.
At the University of Liverpool, scientists are using state-of-the-art imaging technology to study diseases affecting the knee joints of Labradors.
Damage to the canine cruciate ligament, similar to the injuries commonly suffered by professional human athletes, is the most common orthopaedic problem seen in veterinary practices.
Injury to the knee ligaments is also more common in heavier dog breeds

"We're trying to understand how the shape of the Labrador body and the way they walk might contribute to knee problems," says Prof Eithne Comerford, a specialist in musculoskeletal biology.
Using high-speed x-ray cameras, the researchers film their canine patients walking through the lab, and watch their knee bones slide and twist in real-time.
The team hopes to understand how walking contributes to the risk of ligament injury and rupture in Labradors, with the ultimate goal of reducing lameness and suffering within the breed.
"This data will also help veterinary surgeons and engineers design better treatments for ligament damage in Labradors, like customised knee implants," explains biomechanist Dr Karl Bates from the University of Liverpool.
Both research groups rely heavily on the good will of Labrador owners, both for collecting samples and entering their pets into experimental trials.
In addition to tackling diagnosed health issues, researchers hope to change the public's perception of what "desirable" traits should characterise our favourite breeds.
"There is a real danger when we breed dogs to be cuddlier and cuter," warns Dr Raffan.
"I think people have seen so many overweight Labradors, they start to assume it's normal".
Dr Charlotte Brassey is a BBSRC Future Leader Fellow at Manchester Metropolitan University, and British Science Association Media Fellow 2017.
Twitter: @cbrassey
Image copyright Zhong-Jian Liu and Li-Jun Chen Image caption The orchid was found at Wutongshan Mountain in Shenzhen, China

The orchid is known for its beauty and once changed hands for vast sums.
Now, scientists are gaining an insight into how the plant prized for its beauty colonised almost every habitat on Earth.
A team in China has unpicked the genetic blueprint of an orchid that grows wild in the mountains of southeast China.
The orchid in question, from the subfamily, Apostasiodea, split off from modern species millions of years ago.
Researchers led by the Orchid Conservation and Research Centre of Shenzhen sequenced the genome of the orchid and compared it with more modern species.
Image copyright Zhong-Jian Liu and Li-Jun Chen Image caption The whole genome of another orchid was re-sequenced for this study

The data, published in the journal, Nature, "provides a reference for studying orchid evolution" and suggests distinctive features found only in orchids played a key role "in the tremendous radiation of the group", they say.
The orchid is one of the biggest families of flowering plants.
Many are grown for their beautiful flowers, while others are of economic importance, such as the source of the food flavouring, vanilla.
Commenting on the study, Dr Trevor Dines, of the wild plant conservation charity, Plantlife, said orchids have a host of unique features that make them special and instantly recognisable.
"Whether you're looking at a big, blousy Moth orchid from the supermarket or a tiny rare Bog orchid on a remote Snowdonia hillside, the flowers have the same underlying blueprint," he said.
"This research reveals that elements of this blueprint appeared right at the very start of the evolution of the orchid family, and may well have helped in their spectacular subsequent evolution into the 26,500-28,000 species we know of today."
Image copyright Zhong-Jian Liu and Li-Jun Chen Image caption The flowers of Apostasia shenzhenica

Some of the unique features of orchids include masses of pollen, known as pollonia, exceptionally light seeds and the ability to grow upon other plants, using them for support.
All these features are found in the 50 or more species of wild, native orchids in the UK, from the Lady's-slipper orchid to the Bog orchid.
Follow Helen on Twitter.
Image copyright Getty Images

The US solar industry has seen dramatic growth in the past few years, but a request for a rare trade action has led to a fierce fight over the future of the industry - and one that wouldn't exist without the presidency of Donald Trump.
Phil Brodhagen runs a solar installation company in Colorado Springs, and his customers - local homeowners and businesses in a military-friendly town - love American-made products.
Until they see the price.
"They want to go solar, but they do have a limit on how much they can spend."
he says.
"They'd love an American product, but if they can't afford it, they'll either not get a system at all, or go for the cheaper one."
Brodhagen is one of hundreds of business owners across the US paying very close attention to a case in front of the US International Trade Commission.
And he's worried about the outcome.
"It will hurt this industry," he says.
"It's going to be me laying off people as well as everyone else."
On Friday, the commission is expected to rule on whether imported solar products have seriously injured US solar product manufacturers, enough to impose higher tariffs on imports worldwide.
The petition was brought by two solar manufacturers who are based in the US, but owned by overseas companies.
Suniva and SolarWorld have argued their financial troubles - as well as a series of other US solar manufacturer bankruptcies - are due to a massive oversupply of solar cells and panels imported from overseas, primarily from Chinese companies.
Image copyright Getty Images

They point to dozens of US companies like theirs that have gone out of business since 2012.
"Quite simply, we need the commission's help to save solar manufacturing in the United States," Juergen Stein, chief executive of SolarWorld Americans told the commission in August.
But SolarWorld and Suniva find themselves fiercely opposed by much of the solar industry in the US, including the largest trade group, the Solar Energy Industries Association (SEIA).
SEIA has argued increasing the prices of panels through tariffs will set back the solar industry for years, hurting companies that buy and install solar panels, or make solar-related products.
The trade group estimates a loss of as many as 88,000 jobs, or a third of the current solar work force, if Suniva and SolarWorld's requests come to pass.
The group accuses the two companies of using the rare trade action to save themselves, at the expense of the rest of the industry.
What's at stake?
For both sides, the immediate future of the fast-growing solar industry in America.
Bret Sowers, a utility-scale solar farm developer, calls the trade case an "eminent threat" to his business.
Projects like his are reliant on how low a price per watt cost they can offer utility firms.
Their competition is not just other solar firms, but coal and wind, natural gas and nuclear energy.
New solar capacity doubled between 2015 and 2016 and such large-scale projects drove more than half the growth.
Image copyright Reuters Image caption An employee makes a final inspection on panels during a tour of an REC solar panel manufacturing plant in Singapore

"We have close to $2bn in investment we've planned across the southeast," he says, based on prices continuing their downward trend.
If he can't deliver the prices he expected, those solar farms won't be built.
"That's hundreds of construction jobs gone," he says, and layoffs at his company.
Sowers is specifically frustrated because US plants at SolarWorld and Suniva were not building the larger, 72-cell panels at the kind of scale his projects need.
"The two dots don't really connect.
They were making cars and I'm buying trucks - and now they're claiming the trucks are hurting the cars."
James Marlow, who runs a similar Georgia company, is frustrated with the petition, even though he just finished a project with Suniva panels.
"They used to be the home town team," he says of the firm, originally spun out from Georgia Tech and headquartered in the state.
In 2015, in an effort to expand, a Hong Kong-based energy firm purchased more than half the company, but Suniva filed for bankruptcy earlier this year, and weeks later, brought the trade petition.
SolarWorld, whose parent company also filed for insolvency in Germany, joined the petition shortly thereafter.
Marlow says he supports bringing back manufacturing to America, but thinks that means a whole set of policies to deal with what's a "drastically larger" issue.
"It's why most of our clothes are made in Asia and why this cell phone I'm talking to you on made in Asia - it's not just one action."
Media playback is unsupported on your device Media caption A stretch of road has been paved with solar PV (photovoltaic) panels in France.
He attended the 15 August arguments in front of the trade commission on the case and said interest was intense.
There were two overflow rooms for people to listen.
An official told him they hadn't seen that many people come to hear a case since NAFTA.
If the trade commission finds in favour of the manufacturers, it can make recommendations, but it is up to the president to decide.
And President Trump is eager to impose tariffs, especially in an industry in which he could be seen to be tough on Chinese manufacturing.
He reportedly has said "I want someone to bring me some tariffs" because "China is laughing at us".
Image copyright Getty Images

Once the trade commission makes its initial ruling, it will have several more weeks to make a recommendation to Trump.
The president then can decide to take the recommendation or not.
Solarworld had earlier successes with two requests for increased tariffs on Chinese manufacturers for similar unfair trade practice accusations.
But that wasn't enough, says Tim Brightbill, a lawyer representing SolarWorld, because Chinese firms shifted production to other countries to get around the tariffs.
He also claims the potential job loss numbers are overblown.
"These predictions were made before when SolarWorld brought actions against China, that somehow jobs would be lost.
But the opposite happened," Brightbill says.
The situation is especially odd considering both SolarWorld and Suniva are owned by parent companies that could be harmed by the tariff.
Brightbill says "it just shows that SolarWorld is committed to manufacturing here," even if it involves putting a tariff on a German-produced panel.
They may not be alone.
While the US solar industry is holding its breath, foreign manufacturers are starting to think about setting up shop in the US, especially if the commission recommends a broad tariff.
Both sides see the dispute as a turning point for the industry - and both think the president should be on their side.
"If the Trump administration wants to create jobs," James Marlow says.
"They should join with the solar industry."
The question for President Trump will be - which part?
Image copyright Cold Spring Harbor Laboratory Image caption Crick's lecture was said to have altered the logic of biology

Sixty years ago this week, one of the greatest British scientists, Francis Crick, gave a lecture in London in which he accurately predicted how genes work, setting the course for the genetic revolution we are now living through.
Here, evolutionary biologist Professor Matthew Cobb from Manchester University unpicks the predictions that set a new course for how we understand the very stuff we are made from.
In one lecture, it has been said that Francis Crick "permanently altered the logic of biology".
Only four years earlier, he and the young American Jim Watson had solved the double helix structure of DNA, using data gathered by Rosalind Franklin.
Aged 41, Crick was still five years away from winning the Nobel Prize for this work, but he had a reputation as a powerful and profound thinker.
He gave his lecture - "On protein synthesis" - at University College London for the Society for Experimental Biology.
In it, Crick spoke about how genes do what they do.
At the time, this subject was still very murky - some scientists were not even convinced that genes were made of DNA.
But Crick delivered four predictions about genes - and their link to the proteins that build our bodies.
In each of these ideas, he was right.
Cracking the code

Image copyright Science Photo Library

Crick started with the main thing that genes do: they control the production of proteins.
The problem Crick explored was that the DNA in a gene is simply a chemical code - a string of something called bases - A, C, T, G.

Crick had to explain how the cell could get from this one-dimensional sequence of bases in DNA to the complex three-dimensional structures of proteins.
Even more puzzling was the fact that proteins can fold themselves into nearly any shape.
Crick's answer was simple: the order of bases in the gene - what he called "genetic information" - corresponded to the order of the amino acids that make up each protein, and nothing more.
There was no structural information about the protein that was encoded in the gene, he claimed.
He called this the sequence hypothesis.
Somehow, the cell "read" the information in the gene and assembled the amino acids together like beads on a string.
The resulting protein folded itself - spontaneously - into its final 3-D structure.
We still cannot easily predict the structure of a protein from the order of its amino acids, but Crick's sequence hypothesis holds good.
Central dogma

To explain how exactly cells assemble proteins, Crick predicted there must be some small molecules - he called these "adaptors" - that could recognise each of the 20 different amino acids in the body, and would bring them to where they could be turned into a protein in the right order.
As Crick gave his talk in London this molecule was being identified in an American laboratory.
It is now called transfer RNA.
It is the biological messenger that reads and "translates" the genetic code in the cell's protein-building factory.
Image copyright Wellcome Library, London Image caption Crick drew a diagram to explain the flow of information from DNA to proteins

The most controversial and influential part of the lecture though was what was called the central dogma.
Crick explained that as proteins are synthesised, information is taken from the DNA molecule, first into an RNA molecule, and is then used to make a protein.
Before the lecture, he drew a little diagram to explain what he meant.
The arrows show what Crick called the flow of information going from DNA to RNA to protein.
DNA and RNA could also copy themselves, so there are also arrows going from DNA to DNA and from RNA to RNA.
Because the experimental data were not clear, Crick accepted that it might just be possible that DNA could directly lead to the production of proteins, so he drew an arrow there, too (this is not in fact the case).
The most important point was that, as Crick put it, once the information had gone from DNA into a protein, it could not get back into your DNA.
There was no biochemical route for a protein to change your DNA sequence.
Crick thought it might be possible for information to go from RNA to DNA, and this later turned out to be the case, when it was discovered that some RNA viruses can get into our DNA.
But the route from protein to DNA is impossible.
This central dogma emphasises that our DNA sequence cannot be changed by our proteins, or by how they are changed by experience.
Over the last 60 years this has proved to be correct.
Despite the excitement about what is called epigenetics, which explains how genes can be turned on and off by the environment, this never leads to a change in our actual DNA sequence.
Crick's dogma was absolutely right.
Crick later cheerfully admitted that when he coined the phrase, he didn't know what a dogma was.
What he really meant was that it was a basic assumption about how genes worked.
Whatever its name, it still guides scientists today.
Crick's final brilliant prediction was to suggest that in the future biologists would use sequence data to understand evolution, by comparing the sequences of different species.
In 1957, when Crick was speaking, protein sequences were known from only five species, while DNA sequencing was science fiction and 20 years in the future.
But this is exactly what happened, and we can now understand how organisms evolved in unprecedented detail, by comparing their sequences, just as Crick suggested.
Crick's lecture, which was published the following year, continues to be read and cited by scientists all over the world.
It is a monument of clear and penetrating thinking by one of the 20th century's greatest minds.
In all his key predictions, Francis Crick was right, and he did indeed change the logic of biology.
Professor Matthew Cobb is a zoologist, historian and author based at Manchester University
Image copyright Clayton Conn

Photographer Clayton Conn took these pictures in Mexico City, his home since 2009.
His neighbourhood - Portales, in the borough of Benito Jurez - suffered gravely in Tuesday's earthquake, the most lethal tremor to hit Mexico in a generation.
"The area I live in was hit pretty hard, so a lot of the people - a lot of my neighbours are devastated."
This image shows the building two doors down from his apartment.
It originally had five storeys.
"The two bottom floors have been completely collapsed," he says.
"The neighbours said that there are people still inside.
But the building was still sort of... kind of standing... From what I understand, people are still in there.
And those neighbours, they couldn't touch the building.
They fear it could continue collapsing."
Mr Conn was out with his girlfriend when the earthquake struck at 13:14 local time (18:14 GMT).
"I had my camera with me," he explains, "so we spent the day in the streets documenting, and also trying to get home."
Image copyright Clayton Conn

Image copyright Clayton Conn

Image copyright Clayton Conn

A couple of blocks away, he saw crowds of people staring across the rubble of another building, an eight or 10-storey apartment complex.
Neighbours wearing bicycle helmets for protection had joined the rescue effort alongside police and disaster relief workers.
"When they felt like there was a sign of someone, or they felt movement or that someone could be trapped down there, they would raise their fists - and that's the signal.
"So everyone - a crowd of a thousand people - immediately raise their fists, and there's almost complete silence.
You can just hear the generators from the vehicle they've brought in to shine light.
You couldn't hear a thing.
In that silence, they can try to communicate with anyone who is trapped below."
Image copyright Clayton Conn

Image copyright Clayton Conn

"A lot of people at first didn't realise the extent of the damage, so at first people were sort of joking in parks, and other areas where you're supposed to congregate when this occurs, to be safe.
"Once people started to realise that this was much more serious than maybe a small earthquake - because in Mexico City we feel earthquakes very frequently - some that are strong... A lot of people were in a state of shock - but a lot of people were reacting in a very heartening way.
"People were grabbing stuff from their houses, their offices, and just mobilising themselves.
"Within 20 minutes of the earthquake I was seeing people who looked like office workers carrying shovels and pickaxes - I have no idea where they got them from!
"A lot of people wearing their button-up shirts and pants - and a lot of younger people, artists, student types, running around with their shovels."
Image copyright Clayton Conn

Image copyright Clayton Conn

"The authorities weren't able to immediately attend all the places that were affected, all the buildings.
There's not enough rescue teams.
So there was a degree of shock and nervousness, but also - that sense of empathy that was really strong as well.
"People were handing out food... it was a really hot day.
"I only saw one woman that started crying.
I saw lots of people with sad faces, but also faces that were not completely overwhelmed.
Image copyright Clayton Conn

Image copyright Clayton Conn

The new earthquake struck on the 32nd anniversary of a magnitude 8 quake that killed up to 10,000 people and left 30,000 others injured.
That 1985 tremor caused serious damage to Mexico City and its surrounding areas, with more than 400 buildings collapsed and thousands more damaged.
Every year, the authorities carry out earthquake drills as part of the commemoration.
The photographer says they were held 40 minutes to an hour before the real-life disaster.
"There's an alarm that goes off - an earthquake alarm.
It's pretty effective.
It gives you, usually, about 40 seconds to a minute of heads-up...

"You hear this dramatic alarm and you leave the building.
But yesterday [when the real quake hit], the alarm didn't go off immediately - it went off midway through the earthquake.
"That might have caught a lot of people off guard."
This picture shows troops from the Mexican military, who mobilise in response to natural disasters - be it a hurricane or an earthquake.
The yellow armband signifies that that they're responding to an event, and providing aid to possible victims.
Image copyright Clayton Conn

All pictures copyrighted.
Image copyright Science Photo Library

Try to swat a fly and it will soon become clear that they're faster than you.
Much faster.
But how on Earth do these tiny creatures - with their minuscule brains - outwit us so easily?
You've probably pondered it after chasing a fly around your house and flailing your shoe with repeated, unsuccessful swats.
How does it move so fast?
Can it read my mind?
It was the question put to the BBC World Service CrowdScience team for our most recent episode addressing the apparent super powers of tiny animals.
The answer is that, compared with you and me, flies essentially see the world in slow motion.
To illustrate this, have a look at a clock with a ticking hand.
As a human, you see the clock ticking at a particular speed.
But for a turtle it would appear to be ticking at twice that speed.
For most fly species, each tick would drag by about four times more slowly.
In effect, the speed of time differs depending on your species.
This happens because animals see the world around them like a continuous video.
But in reality, they piece together images sent from the eyes to the brain in distinct flashes a set number of times per second.
Humans average 60 flashes per second, turtles 15, and flies 250.
It's all relative

The speed at which those images are processed by the brain is called the "flicker fusion rate".
In general, the smaller the species, the faster its critical flicker fusion rate - and flies, in particular, put us to shame.
Professor Roger Hardie, from the University of Cambridge, investigates how flies' eyes work, and he has an experiment to determine their flicker fusion rate.
Image copyright SPL Image caption "How d'ya like them apples?"
For flies, time drags more slowly than for people

"The flicker fusion rate is simply how fast a light has to be turning on and off before it's perceived or seen as just a continuous light" says Prof Hardie.
Roger inserts tiny glass electrodes into the living light sensitive cells of their eyes - photoreceptors - before flashing LED lights at faster and faster speeds.
Each flash of the LED produces a tiny electrical current in the photoreceptors that a computer can graph onto a screen.
Tests reveal the fastest fly records distinct responses to flickering up to 400 times per second, more than six times faster than our own rate.
The fastest vision of all is found in a species literally called a "killer fly".
It's a tiny predatory species found in Europe that catches other flies out of the air with super-fast reactions.
In her "fly lab" at Cambridge University, Dr Paloma Gonzales-Bellido demonstrates the killer flies' hunting behaviour by releasing fruit fly prey into a special filming box with a female killer fly.
Media playback is unsupported on your device Media caption Some flies see six times faster than us, catching prey in mid-air in less than a second.
Paloma records the behaviour at 1,000 frames per second using slow motion video cameras with a recording buffer.
The attached computer constantly saves the video, over-writing itself every twelve seconds.
When the fly moves, Paloma clicks a button to permanently save the last 12 seconds.
"Our reaction time is so slow that if we were to stop it when we think something is happening it would have happened already," says Dr Gonzales-Bellido.
Essentially, we can't even click a button before the behaviour has happened, it's that fast.
Fly vs fly

With the killer flies and their prey in the filming box, initially the killer fly just sat around motionless, but as one of the fruit flies flew about 7cm above it, there was a flash of movement and suddenly the killer fly was at the bottom of the box chomping into the quivering fruit fly.
Only looking at the slowed-down footage on the computer did it become clear what happened; the killer fly took off, circled the fruit fly three times as it tried to grab it repeatedly, before succeeding in capturing the elusive fruit fly with its front legs.
The whole behaviour from take-off to landing took just one second.
It appears as a flash to our eyes, so conversely, the swatting hand of a human must appear at a snail's pace.
Image copyright Other Image caption The killer fly's eyes contain many more mitochondria than in the eyes of other fly species

Image caption Paloma Gonzales-Bellido uses a special filming box to study killer flies

To enable this incredible speed of the killer fly, which is faster even than other fly species, the light-detecting cells in the killer fly eyes contain many more mitochondria (the "batteries" of biological cells) than are present in the same cells of other flies.
These are the batteries of the cell, so the speedy vision must take more energy than slow vision, explaining why all eyes aren't just set to the highest flicker fusion rate.
The carnivorous diet of the killer fly provides the large amounts of energy it needs to power these high-energy cells.
But even if we had the same number of mitochondria in the cells or our own eyes, we wouldn't have the same vision speed because flies' light-sensitive cells have a totally different design to those of vertebrates.
Behind the structural differences in the eyes of flies is their evolutionary origin.
Arthropods and vertebrates, the groups holding flies and humans, evolved their eyes entirely separately around 700-750 million years ago.
String theory

Flies' eyes evolved to pick up light with a series of tiny string-like structures that lie horizontal to the path that light travels through the eye.
These structures react to light mechanically whereas vertebrates have long tube-like cells facing the light, with chemicals that react to light at the base.
This structure in the fly eye is something Roger studies in his lab.
"It's more sensitive in terms of being able to give a large signal to the tiniest amount of light and it can also respond faster than the rods and cones in the vertebrate eye," he explains.
Image caption Roger Hardie studies the structure of the fly visual system

There are a few reasons for this higher sensitivity, but what Prof Hardie discovered is that they respond mechanically to light, as opposed to chemically as in cones and rods.
Mechanical responses enable faster neural signals.
On top of that, there's a limit to the speed at which neural impulses can travel and the smaller nerve distances from fly eye to fly brain speeds up processing compared to larger vertebrates.
Some vertebrates experience much faster vision than our own.
Whether the species is able to fly seems to correlate with faster vision, as does being small.
This may be because small flying animals have to react so quickly during flight to avoid approaching obstacles.
'Slow motion swats'

The fastest vision of all is found in species that catch flies in the air.
Back with vertebrates, when investigating the vision of the pied flycatcher, a small perching bird that catches flies in flight, scientists at Uppsala University in Sweden discovered that it was able to identify a light flashing on and off 146 times per second from a continuous light source.
The birds were trained to associate a flashing light source with a tasty treat, and would accurately identify the flashing light up to this rate, placing their flicker fusion rate at 146.
That's about twice the rate humans can see but still not as fast as the average fly.
This means the birds, like flies, experience each tick of the clock more slowly than humans.
There is an evolutionary pressure on the flycatchers to experience the ticking hand of the clock as slowly as possible in order to outwit their speedy prey.
Over evolutionary time, birds that experienced 'slower ticking' could react faster to their prey, allowing them to eat more, raise more chicks and pass this speedy vision to future generations.
The flies that have been chased by the fast-sighted birds will be evolving faster reactions to get away.
Creating an evolutionary arms race that has gone on longer even than the existence of birds.
Prey flies have been evolving faster vision and reactions to escape predatory flies like the killer fly since they evolved flight.
Next time you try inanely to swat a fly, try not to be so disheartened.
Your lumbering, slow motion swats are being thwarted by hundreds of millions of years of natural selection letting the flies watch your attempts in slow motion.
Between you and the fly, time, it seems, is relative.
Listen to 'CrowdScience' on the BBC World Service, the programme whose listeners inspired this article, and send your science questions to 'CrowdScience@bbc.co.uk' :
The low-lying Caribbean islands inhabited by Panama's indigenous Guna people are threatened by rising sea levels and increasingly unpredictable weather.
But unlike many island communities facing such problems, the Guna have an escape plan.
The tiny port of Carti on the mainland of Panama is the jumping-off place for day trippers who come to swim, splash and snorkel around the idyllic-looking islands that dot the horizon.
Motor boats buzz in and out carrying smiling visitors wearing life jackets and sun hats.
It's one of Panama's premier tourist destinations.
The islands - almost one for every day of the year - make up the Guna Yala autonomous region, together with a strip of territory on the mainland.
Most Guna communities live on the archipelago, and have done for centuries, after they were driven offshore by disease and venomous snakes.
But now many believe that only a move back to the mainland can secure their future.
It is the people of Gardi Sugdub - Crab Island - who are in the vanguard of the relocation project.
A kilometre inland from the port, where the earth is the colour of rusty nails, they have set aside 17 hectares (42 acres) for the development of a new village - La Barriada.
Unlike other communities around the globe threatened by the vagaries of climate change, the Guna people have a huge advantage: they already own the land they want to relocate to.
Victoria Navarro is one of those from Gardi Sugdub who pictures a new, more spacious existence on dry, higher land.
"I can imagine the community here in La Barriada," she says, looking out across the area of low, tropical vegetation that features a stream, and a small hill.
"My grandchildren want to play soccer and volleyball, but there's no place for them to do that on the island.
Here they can be free like the birds."
Back in 2010, Victoria and her island neighbours began to clear this land, close to the area where they grow crops, in preparation for construction of the new village.
"Everyone came and participated," she remembers.
"It was a very happy time."
Around the same time, with finance from the Inter-American Development Bank, Panama's national government began work on a huge new school adjacent to La Barriada.
The $9m complex, designed for students from across the archipelago, is almost complete.
A little further down the hill, $11m was invested in a new health centre.
Everything looked hopeful, especially when the government made a commitment in 2015 to build 300 houses in La Barriada - the Guna may own all this land, but they do not have the financial resources to develop so many homes.
Find out more

Listen to Linda Pressly's report on Panama's vanishing islands for Crossing Continents at 11:00 on Thursday on BBC Radio 4

Or catch up later on the BBC iPlayer

However, today work on the school and hospital has halted, as a result of a litany of contractual hiccups - and crucially, a failure to plan for adequate supplies of water and electricity.
Work never began on the 300 houses.
The Guna are disappointed but undeterred, and they continue to lobby and to raise funds.
Victoria is an optimist.
She still comes every so often to La Barriada to help re-clear those 17 hectares.
Even so, this patch of land is slowly being reclaimed by the jungle - much as Victoria's home is gradually being swallowed by the Caribbean Sea.
Image caption The La Barriada health centre stands half finished as the jungle regrows around it

Her island, Gardi Sugdub, is just 400m long and 150m wide, but it is occupied by about 2,000 people.
Every inch of the island is built on, unless it's part of a sandy footpath.
Victoria herself lives in a compound with 50 of her extended family - 17 people share her simple bamboo home.
The lack of space for a growing population once seemed to be a bigger problem than the rising sea level, which has been creeping up at a rate of between 2.3mm and 2.5mm per year - roughly an inch every 10 years.
But efforts to enlarge the island may have made its inhabitants more vulnerable to the effects of climate change.
Media playback is unsupported on your device Media caption Our island's being swallowed by the sea

Delfino Davies, who works as a guide for visiting tourists, lives with the rest of his extended family in six simple bamboo houses constructed in a line radiating from the centre of the island out to the shoreline.
It made sense to his forefathers to push the shore further away.
"My grandfather, Charlie Davies, when he came here it was a small island so he reclaimed land.
He brought the stone here, and extended his land," he says.
This has happened across the Guna archipelago.
People have infilled around the edges of the islands using stone, rubbish, and - most controversially - coral.
Image caption Coral is sometimes used for infilling and extending the shoreline

"Coral reefs stop wave action.
So when you remove the coral, even down to 3m in depth, you have no protection.
This has created chaos for people," says Dr Hector Guzman, a research scientist at the Smithsonian Tropical Institute in Panama City.
"The Carti area was where we got the most dramatic data about the destruction of the coral reef when we compared aerial photos taken in the 1960s and then again in 2003."
This means that the islanders are now particularly vulnerable to storm surges, and when the rain and wind come, Victoria Navarro often finds herself ankle-deep in water at home.
"I never sleep well - we're awake 24 hours a day," she says.
Image caption Victoria with her husband Raulio Harris in her overcrowded compound

In 2008, severe storms tore across the island over two long weeks.
Although a move to the mainland had already been mooted, it was in the wake of that destruction that people on Gardi Sugdub began to put together a plan.
The Guna are very well-organised.
Sailas, spiritual and civic leaders, take decisions based on input from the community - on Gardi Sugdub there are meetings almost every day.
And a committee dedicated to the relocation plan is responsible for driving it forward and liaising with government agencies.
"This project's going to be a model for the rest of the Guna people," says Blas Lopez, a sociologist and community activist.
"But some of the other island communities don't think it will happen.
They see the government hasn't supported us, so they're waiting to see if it will become a reality.
If we achieve our dream, other communities will move back to the mainland too."
On the much smaller nearby island of Gardi Muladup, a lack of space means pigs snuffle and feed in pens constructed over the ocean.
Carlos Perez, one of the sailas for this community of 500, is a sprightly 102 years old.
And he is worried.
"We can't control the water," he says.
"In January and February there are very strong winds, and huge waves."
This island has even less protection than Gardi Sugdub.
Image caption Carlos Perez: The people of Gardi Mudalup also want to move onshore

"We're a solitary island - there are no other islands in front of us, so we're much more vulnerable to flooding."
Carlos Perez says his people want to move back to the mainland too.
They have a piece of land on the mainland they call Red Mountain, and they hope to follow the people of Gardi Sugdub.
Not everyone is looking forward to a new life on dry land, though.
Even though her home floods, Antoneta Reurter, the mother of six children, says she has no plans to leave Gardi Sugdub.
In fact, she is hoping she will acquire more space for her family if her neighbours move to La Barriada.
And she does not trust predictions that some of the islands on the archipelago could be flooded in a decade.
"I don't believe the scientists," she says.
"I don't think the islands will disappear - only God can decide that.
If people are corrupt and behave badly, God can send a hurricane or an earthquake and maybe then the islands could go."
Her view - that other-worldly powers may punish the Guna for wickedness - is not one shared by the director of education on Gardi Sugdub, Francisco Gonzales.
"We're more and more affected by climate change," he says.
"And what we're seeing lately isn't the same as what we've seen before - the weather can be really good, then there's a sudden change.
The sea's rising, there are floods in the streets and high winds damage the school's roof.
When that happens we have to send the children home to keep them safe."
Five hundred students squash into the island's classrooms in shifts.
The new school next to the illusory La Barriada on the mainland should have opened three years ago.
When - yet again - classes failed to commence this year, the people protested, blocking the main road leading inland from the port of Carti, and demanding an end to false promises.
Now it seems the government of President Juan Carlos Varela may be listening.
"We hope we can re-start work on the school, and complete it in the first quarter of 2018," Jorge Gonzalez, the minister who has taken the lead on the relocation project, told the BBC.
"We're going to try and find the economic resources, and get electricity to the school and the health centre."
And what about La Barriada, and the construction of those 300 homes he promised back in 2015?
"They're in the budget for this year and next - the Ministry of Housing's in the process of contracting a company than can build them.
We're hoping they will be delivered in 2018.
That's a reality."
And there are signs the government's commitment is genuine - housing officials have since visited La Barriada to inspect the land.
So perhaps, after all, the Guna's foresight will pay off, and their efforts will provide a model for other communities confronting climate displacement in the region and beyond.
Photographs taken by Simon Maybin

Join the conversation - find us on Facebook, Instagram, Snapchat and Twitter.
Image copyright Getty Images Image caption Investigations of murder cases would benefit from the data gathered at a body farm

You are dead.
Now what?
It's the start of a fascinating and eventful - if gory and smelly - journey, at least for your body as it decomposes.
Understanding decomposition can hold the key to solving murders, finding missing people and crucially recognising them, and that is why "body farms" exist.
Body farms are essentially outdoor laboratories where experiments using donated human cadavers investigate taphonomy - the science of decomposition.
Worldwide there are several such facilities: one in Australia, the others are in the US.
But now UK scientists, including Dr Anna Williams from the University of Huddersfield, are lobbying for one in the UK.
This page contains some images a number of readers might find disturbing.
At the British Science Association's annual Science Festival this week in Brighton, Dr Williams presented on the importance of body farms to science and why she believes a UK facility is needed.
"Much of what we know about human decomposition was discovered in US body farms," she said.
Image copyright SPL Image caption Insect clues: The rate at which blowfly pupae grow is dependent on temperature

"We know that the sequence of events in decomposition proceeds along the same path regardless of where the body is, but the timing is very different depending on many factors - moisture, temperature and insects are probably the most important."
But more nuanced factors may also influence decomposition - "such as bacteria already on, and in, the body; whether the person was obese, had been on antibiotics, was diabetic, or even whether they were a vegetarian or not."
So decomposition is anything but simple.
And add in to the mix the fact that the bodies of murder victims can be found on a woodland floor, sealed in a suitcase, buried in a shallow grave, encased in concrete, burnt, dismembered, naked, clothed, wrapped in plastic, and so on.
Traumatic injury is also variable: gunshot wound, stabbing, hanging the list goes on.
Body farm advocates point out the benefits of such facilities, including training dogs to sniff out dead bodies, recognising facial features and ancestry after decay, and even helping to work out how fingerprints change and whether DNA can be recovered after varying intervals of decomposition.
But what about the classic detective question on finding a dead body: "Time of death"?
This is much more difficult to pin down than TV dramas would have you believe, especially a few weeks and more after death.
Medical examiners often use insect colonisation on the body, but this is notoriously unreliable to apply from place to place as it depends on fickle local weather conditions.
Image copyright Anna Williams Image caption Pigs are used in UK experiments, but is their decomposition very different from humans?
Exciting new data published last year in the journal Plos One suggests that the succession of bacteria that come and go, feeding on the decaying body, may help scientists to more accurately pinpoint post-mortem interval.
This discovery was made by analysing bacteria scraped from the nose and ear canals of decomposing cadavers at the world's first body farm in Tennessee.
In the UK, all decomposition experiments use animals - usually pigs.
This does have some advantages.
Most obviously, rotting animals in our countryside is not as objectionable to people as rotting humans.
Indeed, it might be a challenge getting a local community to accept a body farm in its area.
The other advantage is that when pigs are used, multiple experiments can be set up where the conditions prior to decomposition can be varied to show different outcomes.
This has not been the case with human experiments.
As Dr Patrick Randolph-Quinney from the University of Central Lancashire explained, the number of human bodies used in experiments has rarely exceeded three or four individuals, and this limited number will not catch all of the possible outcomes of decomposition.
"It's little more than anecdotal observation without any real understanding or prediction of underlying processes - you might call it 'anecdata'," he told BBC News.
But, on the flipside, there are big disadvantages to using pigs.
Firstly, we really don't know whether pigs decompose similarly to humans, and whether they are a good substitute to use.
This is being actively investigated in the newly opened Australian body farm.
Media playback is unsupported on your device Media caption Australian body farm: More than 500 people have donated their bodies

The results are eagerly awaited.
But as Dr Williams said in her presentation at the science festival: "There can be no better substitute for humans in understanding human decomposition".
Dr Williams firmly believes that a UK body farm facility will allow forensic science to flourish in Britain, producing new data on decomposition bespoke to our climate and situation.
But she cautioned: "We need academics to collaborate and share data across the UK, and across the world - that way experiments have the best chance of being rigorous with larger sample sizes."
Dr Randolph-Quinney has a further ambition: "If we imagine a game of 'fantasy taphonomy', where we had enough money and resources to investigate human decomposition properly, we wouldn't necessarily use outdoor facilities."
"We might build a grave in the lab, where we could adequately control experimental variables such as temperature, humidity, and recover all the products such as body fluids, DNA, organic gases that a body gives off after death.
"This would allow us to model and predict the underlying processes in a scientific way.
We can't do that at present."
Either way human body donations are required.
This may not be a big problem: at both US and Australian facilities there is a waiting list of living donors ready, upon death, to give their body to forensic science.
I was sent for 'gay cure'

Before the legalisation of homosexuality in July 1967, gay men in the UK lived in fear of arrest, beatings and blackmail.
Image copyright AFP/Getty Image caption Houston is now the flood capital of the United States

Parts of the Houston region have been hammered by more than 50in of rainfall since Hurricane Harvey made landfall, setting new records for the US.
But why has the city become America's flood capital?
In April 2016 "historic" flooding hit Houston, with 17.6in (44.7cm) of rain dumped on the city in a single day.
The flood came only 11 months after another massive storm struck the city, dropping over a foot of rain.
Together, these two events caused 16 deaths and more than $1bn (777m) in damages.
Both pale in comparison to Hurricane Harvey, the impact of which has secured Houston's unenviable reputation as the US city most severely affected by floods.
Understanding why the risk to life and property is rising is crucial not only to the future of America's fourth-largest city, but to others around the world which share many of its problems.
Urban sprawl

Houston's unbridled, rapid growth is a primary factor.
The population of its metropolitan area is close to 6.8 million people and, with predictions of some of the country's fastest growth for the coming years, it is expected to top 10 million by 2040.
Interactive See how Houston has developed since 1984 2017 1984

Growth itself is not a problem, as it can create economic, social and environmental benefits for cities.
But poorly planned growth which fails to carefully manage land use - for housing, businesses and the infrastructure like roads, parks and sewers that cities need - can create problems and even lead to disasters.
Houston has long favoured light touch controls, which has led to haphazard development.
The city now covers an enormous area of more than 1,500 sq km.
It is the archetype of urban sprawl, where land is made readily accessible for real estate development on the city's ever expanding periphery.
Loss of habitat

This unplanned growth has led to many problems.
One is that vast acres of wetlands and prairie land - which soak up large amounts of rainfall - have been paved over.
Between 1992 and 2010, for example, White Oak Bayou in north-west Houston lost about 70% of its original wetlands.
During heavy rain the city's growing expanses of concrete generate runoff that clogs and sometimes overwhelms its complex network of waterways.
This includes creeks and bayous, as well as flood controls like levees and detention basins.
Successful wetland and prairie land protection would not itself have a major effect in reducing flooding caused by unprecedented rainfall like that delivered by Harvey.
Yet, protecting and restoring the natural areas provides an important contribution to making Houston less vulnerable to more moderate storms.
They also bring environmental benefits - providing fish and wildlife habitats, and cleaning polluted runoff that can sometimes enter from the city.
The artificially blue-coloured areas show water detected before and after the storm.
Interactive See how flood waters caused by Hurricane Harvey have covered low-lying areas After Before

A city for cars

Another problem is that investment in flood control infrastructure - things like channels, dams and reservoirs - has failed to keep pace with the expansion of the city.
Houston is a car-oriented city, with multi-billion dollar projects supporting one of the most advanced systems of roads and highways in the world.
Image caption Houston is a city built for cars

The goal is to keep traffic moving.
The mismatch between spending on flood prevention and roads can bring new challenges, as fresh tarmac is laid for motorists.
One example is the partially completed State Highway 99.
Once completed this 290km (180-mile) loop will be the longest in the nation - encircling the Houston area.
Further urban sprawl on the edges of the city will inevitably follow and without careful planning, large new tracts of impervious concrete will be laid.
The runoff generated can cause problems for residents when heavy rains arrive.
For example, since the 1980s, rainfall has increased 26% in the Brays Bayou watershed, but runoff has increased by 204%.
Another study suggests that an additional 3,500 households in the Sims Bayou watershed in the south of the city were exposed to flooding as a result of increased runoff.
What can be done?
The tragedy wrought by Hurricane Harvey offers an opportunity to plan for the rebuilding of a more resilient city.
The expansion of the city has led to short-term financial rewards for developers and builders, while local government has benefitted from an expanded tax base.
But they have not shared the risk presented by flooding - the costs of which are mostly passed on to residents and the national government.
Image copyright Getty Images Image caption More than 3,000 people have been rescued in Houston alone

To survive on the Texas Gulf Coast in the coming century, the city and its surrounding region will have to make careful planning decisions to guide growth.
Better co-ordination between federal, state and local government bodies is sorely needed.
The city will also have to consider how it defends itself against storms.
Making huge investments in flood control infrastructure will be necessary if they are to keep pace with the rapid expansion of the population.
Around the world

While Harvey offers a dramatic example, devastating storms are becoming more frequent and severe around the world.
Rapid and poorly planned urbanisation, rising sea levels, and subsidence put the world's coastal cities at increased risk of flooding.
One World Bank study forecasts that global flood damage for large coastal cities could cost $1 trillion a year by 2050 unless action is taken.
There are cities that offer a hint of what can be achieved - both in the US and elsewhere.
Faced with more coastal storms and rising sea levels on the Atlantic Coast, Norfolk, Virginia, has adopted long-term strategies for guiding future land use and development.
For example, areas at low risk of flooding where there has so far been limited development will be transformed into high density and mixed income neighbourhoods.
In contrast, high risk areas with established neighbourhoods are to gradually retreat from shorelines, with housing buyouts, while key sewer and water utilities, and roads will be maintained rather than expanded.
Image copyright Getty Images Image caption Rotterdam's Maeslant surge barrier is part of the city's extensive flood prevention system

Rotterdam, in the Netherlands, provides another model.
Ninety percent of the densely populated city lies below sea level.
Its pioneering solutions to flooding entail living with the water, rather than trying to contain it.
It has installed underground garages, green roofs that absorb water and water plazas that serve as a kind of aquatic town square, while simultaneously acting as huge storage reservoirs when extreme rainfall occurs.
From the Norfolk and Rotterdam perspectives, flooding and climate change are not obstacles to economic development, but opportunities.
About this piece

This analysis piece was commissioned by the BBC from an expert working for an outside organisation.
Philip Berke is Professor of Land Use and Environmental Planning at Texas A&M University.
Edited by Duncan Walker
Image copyright Science Photo Library Image caption Chemist Thomas Midgley insisted that tetraethyl lead was safe

Leaded petrol was safe.
Its inventor was sure of it.
Facing sceptical reporters at a press conference in October 1924, Thomas Midgley dramatically produced a container of tetraethyl lead - the additive in question - and washed his hands in it.
"I'm not taking any chance whatever," Midgley declared.
"Nor would I... doing that every day."
Midgley was - perhaps - being a little disingenuous.
He had recently spent several months in Florida, recuperating from lead poisoning.
Some of those who'd made Midgley's invention hadn't been so lucky, which is why reporters were interested.
50 Things That Made the Modern Economy highlights the inventions, ideas and innovations which have helped create the economic world in which we live.
It is broadcast on the BBC World Service.
You can find more information about the programme's sources and listen online or subscribe to the programme podcast.
On the Thursday of the week before Midgley's press conference, at a Standard Oil plant in New Jersey, a worker named Ernest Oelgert started hallucinating.
By Friday, he was running around the laboratory, screaming in terror.
On Saturday, with Oelgert dangerously unhinged, his sister called the police.
He was taken to hospital and forcibly restrained.
By Sunday, he was dead.
Within the week, so were four of his colleagues - and 35 more were in hospital.
Only 49 people worked there.
'The loony gas building'

None of this surprised workers elsewhere in Standard Oil's facility.
They knew there was a problem with tetraethyl lead.
As Gerald Markowitz and David Rosner note in their book Deceit and Denial: The Deadly Politics of Industrial Pollution, the lab where it was developed was known as "the loony gas building".
Nor should it have shocked Standard Oil, General Motors or the DuPont Corporation, the three companies involved with adding tetraethyl lead to gasoline.
Image copyright Science Photo Library Image caption An aerial photograph of DuPont's Deepwater factory site, where tetraethyl lead was developed

The first production line in Ohio had already been shut down after two deaths.
A third plant elsewhere in New Jersey had also seen fatalities.
Workers kept hallucinating insects - the lab was known as "the house of butterflies".
Better working practices could make tetraethyl lead safe to produce.
But was it really sensible to add it to petrol, when the fumes would be belched out on to city streets?
About a century ago, when General Motors had first proposed adding lead to petrol - in order to improve performance - scientists were alarmed.
They urged the government to investigate the public health implications.
Midgley breezily assured the surgeon general that "the average street will probably be so free from lead that it will be impossible to detect it or its absorption", although he conceded that "no actual experimental data has been taken".
Image copyright Science Photo Library Image caption Chemist Thomas Midgley with the Delco laboratory test engine

General Motors funded a government bureau to conduct some research, adding a clause saying it had to approve the findings.
Risky, but useful?
The bureau's report was published amid the media frenzy over Oelgert's poisoned workmates.
It gave tetraethyl lead a clean bill of health and was met with some scepticism.
Under pressure, the government organised a conference in Washington DC in May 1925.
The debate there exemplified the two extremes of approach to any new idea that looks risky, but useful.
In one corner: Frank Howard, vice-president of the Ethyl Corporation - a joint venture between General Motors and Standard Oil.
He called leaded petrol a "gift of God", arguing that "continued development of motor fuels is essential in our civilization".
Image copyright Alamy Image caption Dr Alice Hamilton argued the benefits of adding lead to petrol were outweighed by the risks

In the other corner: Dr Alice Hamilton, the country's foremost authority on lead.
She argued leaded petrol was a chance not worth taking.
"Where there is lead," she said, "some case of lead poisoning sooner or later develops, even under the strictest supervision."
Hamilton knew that lead had been poisoning people for thousands of years.
In 1678, workers who made lead white - a pigment for paint - were described as suffering ailments including "dizziness in the head, with continuous great pain in the brows, blindness, stupidity".
The Romans used lead in water pipes.
Lead miners often ended up mad or dead - and some correctly intuited that low-level, long-term exposure was also unwise.
"Water conducted through earthen pipes is more wholesome than that through lead," wrote the civil engineer Vitruvius, 2,000 years ago.
"This may be verified by observing the workers in lead, who are of a pallid colour."
Pollution v progress

Many societies still grapple with the general question on which Howard and Hamilton disagreed: how much pollution is a price worth paying for progress?
There's some evidence that as countries get richer, they tend initially to get dirtier and later clean up.
Economists call this the "environmental Kuznets curve", and it makes intuitive sense.
If you're poor, you prioritise material gains.
As your income grows, you may choose to spend some of it on a nicer, safer environment.
Image copyright Alamy Image caption The Roman civil engineer Vitruvius warned against the dangers of lead 2,000 years ago

But was lead-free petrol really such an expensive luxury?
True, the lead additive solved a problem: it enabled engines to use higher compression ratios, which made cars more powerful.
However, it was not the only way to solve the problem.
Ethyl alcohol had much the same effect and wouldn't mess with your head, unless you drank it.
Midgley knew this, having combined petrol with practically every imaginable substance, from iodine to camphor to melted butter.
Why did the petrol companies push tetraethyl lead instead of ethyl alcohol?
Researchers who have studied the decision remain puzzled.
Cynics might point out that any old farmer could distil ethyl alcohol from grain.
It couldn't be patented, or its distribution profitably controlled.
Tetraethyl lead could.
The crime connection

The US didn't tax lead in petrol until the 1970s, then finally banned it as part of clean air legislation, as the country moved down the far side of the environmental Kuznets curve.
Two decades later, in the 1990s, rates of violent crime started to go down.
There are many reasons why this might have happened, but the economist Jessica Reyes had an intriguing thought.
Children's brains are especially susceptible to chronic lead poisoning.
Is it possible that kids who didn't breathe leaded petrol fumes grew up to commit less violent crime?
Image copyright Alamy

Reyes could test her hypothesis: different US states phased out leaded petrol at different times.
By comparing the dates of clean air legislation with subsequent crime data, she concluded that more than half the drop - 56% - was because of cars switching to unleaded petrol.
Other researchers have found similar links between lead water pipes and urban homicide.
Disputed science and delayed regulation

You can put a dollar figure on the value of crime reduction, Reyes found.
It's about 20 times higher than the cost of de-leading petrol - and that's before you count other downsides of children breathing lead, like worse performance in school.
How did the US get this so wrong for so long?
Image copyright A Periam Photography / Alamy Stock Photo Image caption Asbestos continued to be widely used in construction despite the emerging evidence of its dangers

It's a tale of disputed science and delayed regulation, much like you could tell about asbestos, or tobacco, or other products we now know slowly kill us.
The problem is that people who want to ban things aren't always disinterested visionaries like Hamilton.
Sometimes they're obstructive cranks.
The only way to tell the difference is by conducting studies.
And, as Gerald Markowitz and David Rosner point out, "For the next four decades, all studies of the use of tetraethyl lead were conducted by laboratories and scientists funded by the Ethyl Corporation and General Motors".
More from Tim Harford:

How Diesel's engine changed the world

Battery bonanza: From frogs' legs to mobiles and electric cars

Why the falling cost of light matters

How a razor revolutionised the way we pay for stuff

And what of the scientist who first put lead in petrol?
By all accounts, Midgley was a genial man who may even have believed his own spin about the safety of a daily tetraethyl lead handwash.
But, as an inventor, his inspirations seem to have been cursed.
His second major contribution to civilisation was the chlorofluorocarbon, or CFC, which improved refrigerators, but destroyed the ozone layer.
In middle age, afflicted by polio, Midgley applied his inventor's mind to lifting his weakened body out of bed.
He devised an ingenious system of pulleys and strings.
They tangled around his neck, and killed him.
Tim Harford writes the Financial Times's Undercover Economist column.
50 Things That Made the Modern Economy is broadcast on the BBC World Service.
You can find more information about the programme's sources and listen online or subscribe to the programme podcast.
New evidence proves that the ancient Egyptians constructed the Great Pyramid at Giza by transporting 170,000 tons of limestone in boats.
It has long been known that the rock was extracted eight miles away in Tura and that granite used in the monumental structure was quarried 533 miles away in Aswan.
However, archaeologists have disagreed over how the material was transported to Giza, now part of modern-day Cairo, for construction of Pharaoh Khufus tomb in 2600 BC.
Now that mystery could be a step further to being solved after the discovery of an ancient scroll of papyrus, a ceremonial boat and a network of waterways, reported the Mail on Sunday.
The new evidence shows that thousands of laborers transported 170,000 tons of limestone along the River Nile in wooden boats built with planks and rope.
2,000-YEAR-OLD TOMBS FROM ROMAN PERIOD FOUND IN EGYPT

The 2.5-ton blocks were ferried through a system of specially designed canals before arriving at an inland port built just yards away from the base of the Great Pyramid.
The papyrus scroll is the only firsthand record of how the pyramid was built, and was written by an overseer named Merer.
Click for more from The Sun.
In early 2015, a team of scientists from the United Kingdom, Bulgaria, Sweden, the United States and Greece set off to investigate the effects of climate change and the impact of sea level changes in the Black Sea since the end of the Earths last glacial cycle 12,000 years ago.
What they discovered by chance during their studies was more than they could have ever imagined: 60 shipwrecks dating back 2,500 years, including artifacts from the Byzantine Era, the Middle Ages and the Ottoman Empire.
This week, after nearly three years at sea, the scientists who participated in the Black Sea Maritime Archaeology Project docked their research vessel in the port of Burgas, Bulgaria, and displayed dramatic 3-D printed replicas of those shipwrecks, which represent more than a thousand years of maritime history.
SUNKEN WWII SHIP MAY CONTAIN $130M IN NAZI GOLD

This assemblage must comprise one of the finest underwater museums of ships and seafaring in the world, said the projects chief investigator, Jon Adams, professor of maritime archaeology at the University of Southampton in England.
The ships were found lying hundreds or thousands of meters deep, many with their masts still standing and their rudders in place.
Cargoes of amphorae and ships fittings, their carvings and tool marks as distinct as the day they were made, were found lying on the decks.
Many of the ships revealed structural features, fittings and equipment that were known to have existed, but had never actually been seen.
We have never seen anything like this before, said Dr. Kroum Batcherov of the University of Connecticut.
This is history in the making unfolding before us.
The wrecks were found using robotic laser scanning, acoustic and photogrammetric techniques.
The earliest one found so far dates back to the Classical period, around 400-500 BC.
ARCHAEOLOGISTS IN NORWAY MAY HAVE UNCOVERED A VIKING BOAT GRAVE

We dived on one wreck, a merchant vessel of the Byzantine period dating to the 10th century, Adams said.
It lies at a depth of 93m that puts it into the diving range, so we took the opportunity to visually inspect certain structural features firsthand.
The condition of this wreck below the sediment is staggering, the structural timber looks as good as new.
This suggested far older wrecks must exist, and indeed, even in the few days since the dive, we have discovered three wrecks considerably older, including one from the Hellenistic period and another that may be older still.
Dr. Kalin Dimitrov, director of the Centre of Underwater Archaeology in Sozopol, Bulgaria, added:

"During the third season of the Black Sea MAP we continued filling in the blanks of the mosaic of ancient seafaring with the discovery and documentation of outstandingly well-preserved ships.
The vessels represent the Roman and Byzantine periods and the time of ancient Greek colonization.
The discovered shipwrecks will undoubtedly rewrite the history of ancient shipbuilding."
The start to the end of the world is coming this fall, according to doomsday writer David Meade.
Meade, who said he is using astronomy and the Biblical book of Revelations, has predicted that Oct. 15 will be the start of the tribulation  the seven-year period that brings the demise of the world.
Hold on and watch  wait until the middle of October and I dont believe youll be disappointed, said Meade, who also predicted a magnificent sign in the skies will occur on Sept. 23.
Meade certainly isnt the first person who predicted the end of the world.
Heres a look at some other recent times the world was supposed to end.
October 7, 2015

Chris McMann, the leader of the eBible Fellowship group in Philadelphia, warned that the world would be destroyed by a fire on Oct. 7, 2015.
When that didnt happen, McCann told The Guardian it was surprising.
eBible Fellowship appears to still be in existence.
December 21, 2012

Conspiracy theorist, numerologists and self-described prophets looked to Dec. 21, 2012 as the day the world would officially end  known as the Mayan Apocalypse.
They predicted the world would end with a massive solar storm that would knock out the power grid or from a comet blasting into Earth.
May 21, 2011

Harold Camping, a former evangelist, convinced so many people that the world would end on May 21, 2011 with a series of earthquakes that many quit their jobs and donated money to Campings Family Radio network.
After the world was still very much in tact on May 22, Camping changed the date to Oct. 21, 2011.
And when the world still wasnt destroyed then, Camping apologized for his sinful statements.
He died in 2013.
April 29, 2007

Former Southern Baptist pastor Pat Robertson has predicted the world will end multiple times  including on April 29, 2007.
He made the prediction in his book The New Millennium in 1990.
January 1, 2000

There was widespread concern and theories that computers couldnt handle the start of the new Millennium and the world would end.
That idea was also promoted by Jerry Falwell, the southern pastor who died in 2007.
Falwell warned that the new century would be Gods instrument to shake this nation, to humble this nation.
Tim LaHaye and Jerry Jenkins, authors of the fiction Left Behind series about the end times, also at one point warned that the Y2K could have brought about the destruction of the world.
The end of the world is starting this fall, according to a doomsday writer and researcher.
David Meade, author of Planet X: The 2017 Arrival, has predicted that Sept. 23 will bring a magnificent sign in the skies over Jerusalem, a historical event signaling an upcoming Tribulation Period of seven years.
He also asserted that a Planet X will cause the greatest catastrophic infliction of life upon mankind, since Noahs Ark,  citing the biblical story of a great flood that wiped out much of the Earth and humanity.
The Planet X will cause volcanic eruptions, a short stoppage of the Earths rotation, change in climate, tidal waves and earthquakes.
Read on for a look at who Meade is and when he predicts the world to end.
So when does he say the world will end?
According to Meades website, Sept. 23 is not actually the end of the world, but the day that a magnificent sign in the skies over Jerusalem will appear that will be a historical event signaling an upcoming Tribulation Period of seven years.
The tribulation period in Christianity is considered to be the seven year period when the anti-Christ comes into power and Gods wrath is released on those still on Earth.
TV PROGRAMS IN CALIFORNIA INTERRUPTED WITH END-OF-THE-WORLD PREDICTION

I dont know when the Rapture will happen.
I expect nothing to happen in September, Meade said.
However, the writer also opined that the seven-year Tribulation Period will begin on Oct. 15, citing the use of astronomical calculations and the book of Revelations in the Bible as his sources.
October is the month to watch!
The major signs that converge on September 23 are indeed amazing, but those are celestial events.
They are time markers.
The mainstream media states that something visible will occur on these dates.
I dont believe that, Meade said.
The actual event of the beginning of the Tribulation occurs on October 15.
Thats when the action starts, he continued.
Hold on and watch  wait until the middle of October and I dont believe youll be disappointed.
What are his religious beliefs?
Meade said on his website he was raised Catholic.
Catholics believe the Bible.
We were taught the Book of Revelation is true, Meade said.
The Popes believe it is true.
Protestants know it is true  it is taught in Sunday school.
This is not new information.
TEXAS INVESTORS BUILD LUXURIOUS HOMES FOR DOOMSDAY SCENARIOS

In an interview with the Washington Post, Meade denied that he has ever called himself a Christian numerologist.
What is his educational background?
Meade told the Washington Post that he studied astronomy at a university in Kentucky.
However, he declined to say which school for safety reasons, the Washington Post reported.
A biography of Meade on a Planet X News website said he went to the University of Louisville.
What does NASA say about Planet X?
NASA debunked claims that another planet is putting Earth in imminent danger and called claims of wayward planets such as Planet X an Internet hoax.
If Planet X was real and headed for an encounter with the Earth in 2012, astronomers would have been tracking it for at least the past decade, and it would be visible by now to the naked eye, NASA said.
Obviously, it does not exist.
Over the past two weeks, Mexico has experienced a lot of shaking.
On Sept. 8, a magnitude-8.1 earthquake struck 54 miles (87 kilometers) southwest of Pijijiapan, which sits just above the Mexico-Guatemala border.
Eleven days later, a magnitude-7.1 quake struck 3 miles (5 km) east of Raboso, near Mexico City.
And today (Sept. 21), another quake  a magnitude 4.8  hit just outside Pijijiapan.
While Mexico's position along major tectonic fault lines makes it a hotbed of seismic activity, the frequency of these powerful earthquakes begs the question: Are these quakes happening more often?
[The 10 Biggest Earthquakes in History]

Not likely, said Gavin Hayes, a research geophysicist at the U.S. Geological Survey's National Earthquake Information Center.
"Mexico is very prone to earthquakes," he said, "so earthquakes of this size in Mexico are not unusual.
Getting two in a row of this size so close together is unusual but not unexpected."
In the grand, slow-moving world of tectonic plates, Mexico is situated at an unfortunate location: It rests at the southern edge of the North American Plate, putting it right at the point where it meets the Pacific Plate, the Cocos Plate and the Caribbean Plate.
The quakes occur because all of these plates are moving in different directions, and as they collide or rub against each other, this movement can unleash destructive forces.
While these tectonic events usually occur along coastlines, like near Pijijiapan, the Cocos Plate has a unique conformation that explains why so many earthquakes are hitting Mexico City, which lies farther inland, according to the U.S. Geological Survey (USGS).
While the North American landmass is slowly moving west, the Cocos Plate is traveling northeast.
As they push against each other, the Cocos Plate, which carries the seafloor and is denser than plates carrying land, is forced underneath, into the Earth's mantle, according to the USGS.
But Hayes said that although most of these above-below collisions, called subduction zones, involve one point of descent, the Cocos Plate sinks a bit and then flattens out for a long expanse before it begins to sink again.
Because the location at which it sinks is spread out, the resulting earthquakes often occur farther inland than they would at a typical subduction zone.
"I think this perhaps facilitated the shaking we saw two days ago," Hayes said.
Some large earthquakes can trigger large aftershocks, but that's almost certainly not what happened here, according to Hayes.
For one, the two epicenters are too far away from each other to be causally related.
Even though both earthquakes occurred on the same subduction slab that goes beneath Central America, they were caused by different fault lines, he said.
As such, it was more of a coincidence than anything else that both fault lines were "ready to go," Hayes said.
But because there are so many fault lines along the subduction zone that runs down the coast of Mexico, Hayes thinks it's reasonable to assume that there will be more large earthquakes in the region in the future, but not any more than one might normally expect.
"It's still a significant hazard," he said.
Original article on Live Science.
The end is still nigh -- just not as nigh as it was earlier this week, a Doomsday writer says.
David Meade, who claimed the world is ending Saturday when a mysterious planet collides with Earth, is now backtracking on the calamitous claim.
Meade said the world won't end on Sept. 23 after all, but instead Saturday will only mark the beginning of a series of catastrophic events to occur over several weeks.
The world is not ending, but the world as we know it is ending, he told the Washington Post.
A major part of the world will not be the same the beginning of October.
Meade said his prediction is based on verses and numerical codes found in the Bible, specifically in the apocalyptic Book of Revelation.
He said recent events, such as the solar eclipse and Hurricanes Irma and Harvey, are omens of the approaching apocalypse.
The significant number is 33, according to Meade.
Jesus lived for 33 years.
The name Elohim, which is the name of God for the Jews, was mentioned 33 times [in the Bible], he said.
Its a very biblically significant, numerologically significant number.
Im talking astronomy.
Im talking the Bible...and merging the two.
Sept. 23 is also 33 days since the Aug. 21 solar eclipse.
Meade has also built his theory on the so-called Planet X, which is also known as Nibiru, which he believes will pass Earth on Sept. 23.
This will cause volcanic eruptions, tsunamis and earthquakes, he claims.
NASA has repeatedly said Planet X does not exist.
Meades prediction has been dismissed by people of faith including the Roman Catholic and Protestant branches of Christianity.
Ed Stetzer, a professor and executive director of Wheaton Colleges Billy Graham Center for Evangelism, slammed Meades theory on Friday, calling it fake news and asked Christians to be critical.
Its simply fake news that a lot of Christians believe the world will end on September 23, Stetzer wrote in Christianity Today.
Yet, it is still a reminder that we need to think critically about all the news.
A mass extinction which wipes out humanity will be underway by the year 2100, scientists have claimed.
By the end of the century, its feared that so much carbon will have been added to the oceans that the planet will have passed a threshold of catatastrophe which leads to the destruction of our species.
In the past 540 million years the planet has endured five such wipeouts  including the extinction of the dinosaurs.
The worst took place 252 million years ago and is known as the Great Dying.
This disaster killed off more than 95 per cent of marine life when the seas suddenly became more acidic.
Now geophysicist Professor Daniel Rothman says we are seeing a disturbing parallel today  this time because of man-made global warming.
He came up with a simple mathematic formula which predict that the oceans will soon hold so much carbon that a mass extinction is inevitable.
It showed the critical extra amount required is about 310 gigatons  which is the best case scenario projected by the Intergovernmental Panel on Climate Change (IPCC).
And it's well below the worst of more than 500 gigatons - which would far exceed the line.
In all scenarios, the study found by the end of the century the carbon cycle will either be close to - or well beyond - the threshold for catastrophe.
Although mass extinction won't soon follow at the turn of the century the world may have tipped into "unknown territory".
Professor Rothman, of the Massachusetts Institute of Technology, says it would take some time - about 10,000 years - for such ecological disasters to play out.
He said: "This is not saying disaster occurs the next day.
"It's saying - if left unchecked - the carbon cycle would move into a realm which would be no longer stable and would behave in a way that would be difficult to predict.
"In the past this type of behaviour is associated with mass extinction."
In the modern era CO2 emissions have risen steadily since the 19th century but deciphering whether this could lead to mass extinction has been challenging.
Humans have emitted 1,540 billion tonnes of CO2 since the industrial revolution - equivalent to burning enough coal to form a square tower 72 feet wide stretching 240,000 miles from Earth to the Moon.
Half of these have remained in the atmosphere causing a rise in levels at least 10 times faster than any known natural increase during Earth's long history.
Most of the other half has dissolved into the ocean - causing acidification.
Will this lead to the destruction of humanity?
Your grandchildren will probably find out, unless something changes now.
This story originally appeared in The Sun.
Turks and Caicos, parts of the Dominican Republic, and the southeastern Bahamas are under a hurricane warning as Hurricane Maria continues to rage in the Caribbean.
When the threat of a hurricane looms, its important for residents to know if hurricane warnings or watches have been issued for the areas they live in -- as well as what the two terms indicate.
Whats the difference?
Hurricane warnings and watches have slightly different meanings concerning hurricane conditions, or sustained winds of 74 mph or above, the National Oceanic and Atmospheric Administrations National Ocean Service (NOS) says.
A warning means that hurricane conditions are expected whereas a watch means that conditions are possible, the office explains.
When are hurricane warnings and watches issued?
Hurricane warnings and watches are issued 36 hours and 48 hours, respectively, before tropical-storm-force winds may strike, according to the NOS.
Because hurricane preparedness activities become difficult once winds reach tropical storm force (sustained winds of 39 to 73 mph), the hurricane warning is issued 36 hours in advance of the anticipated onset of tropical-storm-force winds to allow for important preparation, it says.
CATEGORY 2 HURRICANE PUMMELS NAPLES, FLORIDA

What does it mean when tropical storm warnings and watches are issued for areas?
Warnings mean tropical storm conditions are expected, while tropical storm watches indicate that theyre possible, the National Weather Service (NWS) says online.
What else should I know about weather warnings?
During a weather warning, it is important to take action: grab the emergency kit you have prepared in advance and head to safety immediately, the NWS advises, adding that warnings are more urgent than watches.
Fox News' Travis Fedschun and The Associated Press contributed to this report.
The Church of Latter Day Saints in Salt Lake City, UT.
has recent acquired the near 200-year-old printer's manuscript of the Book of Mormon, making it the earliest surviving copy in existence.
Elder Steven E. Snow, Church historian and recorder, said the Book is a revered text in the community.
We hold the Book of Mormon to be a sacred text like the Bible," Snow said in a press release.
"The printers manuscript is the earliest surviving copy of about 72 percent of the Book of Mormon text, as only about 28 percent of the earlier dictation copy survived decades of storage in a cornerstone in Nauvoo, Illinois.
ARCHAEOLOGISTS UNCOVER THE LOST ROYAL PALACE OF HENRY VIII

George Schweich, a grandson of early Church member David Whitmer, Mormon.
Schweich inherited the manuscript from his grandfather and sold it in 1903 to the Reorganized Church of Jesus Christ of Latter Day Saints.
The modern day Church purchased the manuscript on Sept. 18 for $35 million, which was provided by "generous donors."
Plans are currently under way to display the manuscript to the public at the Church History Library in the coming months, the release added.
2017 FOX News Network, LLC.
All rights reserved.
This material may not be published, broadcast, rewritten, or redistributed.
All market data delayed 20 minutes.
Research at the University of York has revealed that genes are controlled by 'nano footballs' - structures that look like footballs but 10 million times smaller than the average ball.
By placing tiny glowing probes on transcription factors - special chemicals inside cells which control whether a gene is switched 'on' or 'off' - researchers gained a remarkable new insight into the way in which genes are controlled.
Crucially, they discovered that transcription factors operate not as single molecules as was previously thought, but as a spherical football-like cluster of around seven to ten molecules of roughly 30 nanometres in diameter.
The discovery of these nano footballs will not only help researchers understand more about the basic ways in which genes operate, but may also provide important insights into human health problems associated with a range of different genetic disorders, including cancer.
The research, supported by the Biotechnology and Biological Sciences Research Council (BBSRC) and published in eLife was carried out by scientists from the University of York, and the University of Gothenburg and Chalmers University of Technology, Sweden.
The researchers employed advanced super-resolution microscopy to look at the nano footballs in real time, using the same type of yeast cells utilised in baking and brewing beer.
Professor Mark Leake, Chair of Biological Physics at the University of York who led the work, said: "Our ability to see inside living cells, one molecule at a time, is simply breathtaking.
"We had no idea that we would discover that transcription factors operated in this clustered way.
The textbooks all suggested that single molecules were used to switch genes on and off, not these crazy nano footballs that we observed."
The team believe the clustering process is due to an ingenious strategy of the cell to allow transcription factors to reach their target genes as quickly as possible.
Professor Leake said: "We found out that the size of these nano footballs is a remarkably close match to the gaps between DNA when it is scrunched up inside a cell.
As the DNA inside a nucleus is really squeezed in, you get little gaps between separate strands of DNA which are like the mesh in a fishing net.
The size of this mesh is really close to the size of the nano footballs we see.
"This means that nano footballs can roll along segments of DNA but then hop to another nearby segment.
This allows the nano football to find the specific gene it controls much more quickly than if no nano hopping was possible.
In other words, cells can respond as quickly as possible to signals from the outside, which is an enormous advantage in the fight for survival."
Genes are made from DNA, the so-called molecule of life.
Since the discovery that DNA has a double helix shape, made in the 1950s by pioneering biophysics researchers, much has been learned about transcription factors which can control whether a gene is switched on or off.
If a gene is switched on, specialised molecular machinery in the cell reads off its genetic code and converts it into a single protein molecule.
Thousands of different types of protein molecules can then be made, and when they interact that can drive the building of all of the remarkable structures found inside living cells.
The process of controlling which genes are switched on or off at any particular point in time is fundamental to all life.
When it goes wrong, this can lead to serious health problems.
In particular, dysfunctional switching of genes can result in cells which grow and divide uncontrollably, which can ultimately lead to cancer.
This new research may help provide insights into human health problems associated with a range of different genetic disorders.
The next stages will be to extend this research into more complicated types of cells than yeast - and ultimately into human cells.
Analyses of temperate rain forests located on the central coast of British Columbia, Canada suggest that for centuries, humans have intentionally used fire to manage plant-life.
The findings are published in the Journal of Biogeography.
When researchers reconstructed 700 years of temporal and spatial aspects of fire activity, they recorded 16 fires from 1376-1893.
No fire activity was detected after 1893, coinciding with the relocation of indigenous groups from the study area.
"Old growth temperate rain forests are often considered pristine and untouched landscapes, but new science is confirming what First Nations have known since time immemorial--that these forests were carefully managed with fire to increase the abundance of specific plants" said Kira Hoffman, lead author of the study.
"These were slow-moving ground fires that left the majority of trees alive and kept the forest open and clear of brush, not the large, uncontrolled wild fires that we've become accustomed to today."
Scientists at Rutgers University-New Brunswick used a genetic engineering technique for the first time to create brain cells from the blood cells of individuals in a three-generation family with Tourette syndrome to help determine what causes the disease.
"This is so important to the future research of Tourette's and other neuropsychiatric disorders because before this technique was discovered we were unable to study brain-type nerve cells of living patients," said Jay Tischfield, senior author of the study published in Molecular Psychology and MacMillan Distinguished Professor of Genetics.
"I think this technique will give us a better understanding of what sorts of genes cause this disease.
Also, these cells could be used to screen drugs that might be effective for treatment."
While the technique - which led to a Nobel Prize in 2012 for the Japanese and British scientists who discovered it -- has been used to investigate the genetic link of other psychiatric or neurological diseases like schizophrenia and Lou Gehrig's disease, this is the first time the procedure was used in researching a cause of Tourette syndrome, which has no precise treatment and cannot yet be diagnosed by genetic testing.
Tischfield, and doctoral student Nawei Sun, and their colleagues, converted blood cells of members of the same family, those who had Tourette's and those who did not, into induced pluripotent stem cells, or iPS cells, which were then converted into brain nerve cells.
Pluripotent stem cells are capable of maturing into any type of adult cell - from a heart muscle cell to a nerve cell - and offer scientists an opportunity to study inherited diseases in the tissues that are most affected.
Through sequencing all of the protein encoding DNA in many members of this large family, the Rutgers group concluded that the observed Tourette disorder and obsessive compulsive disorder was likely due to a mutation in the PNKD gene that was present in family members that were diagnosed with the disorders.
Scientists found that the brain-specific form of the PNKD protein was present at lower levels in those with Tourette's due to the mutation and believe, while it might not be the case in most people with Tourette syndrome, it is the cause for this particular family.
More research needs to be done to determine how such a mutation could cause these disorders, Tischfield said.
"What we were able to show is the malfunction of the gene and how disrupting it could cause Tourette's," said Tischfield.
"As we move forward, we may find other families with this same genetic mutation."
Tourette syndrome - a disorder characterized by both vocal and body tics - is linked to problems in the basal ganglia, the part of the brain responsible for voluntary motor control, procedural learning and eye movement, as well as cognitive and emotional function.
It is characterized by grimacing, eye blinking and shoulder shrugging and often accompanied by co-occurring conditions, such as depression, obsessive-compulsive disorder (OCD) or attention-deficit hyperactivity disorder (ADHD).
In the United States one out of every 100 - 150 people exhibit Tourette syndrome, with males affected three to five times more often than females.
Scientists at Rutgers and others involved with them on this research estimate that there are approximately 400 genes in which a mutation could increase risk for Tourette disorder.
Rutgers, home to the NJ Center for Tourette Syndrome (NJCTS) Cell & DNA Sharing Repository, in collaboration with the National Institute of Mental Health Repository and Genomics Resource, makes genetic material and cells available to researchers around the world investigating Tourette disorder.
Ticks are nasty little survivors, outlasting even dinosaurs as they resist drought, tolerate cold and go months without a meal.
They carry a host of diseases that they spread by plunging their barbed mouths into you like a grisly oil derrick.
They're hard to remove and even harder to kill.
Is your skin crawling yet?
Researchers at the University of Cincinnati are examining the tick's defenses, looking for ways to prevent tick-borne illnesses such as Lyme disease.
Biology students and professors in UC's McMicken College of Arts and Sciences are studying the distribution of ticks in southwest Ohio, the diseases they carry and their ability to withstand Midwest winters.
Improving our understanding of these cringe-inducing parasites could help hikers, dog walkers and nature enthusiasts stay healthy.
"There are still so many things we don't know about ticks," said UC assistant professor Joshua Benoit, who is supervising the research.
"They're known for transmitting even more diseases than mosquitoes."
Four species are found in southwest Ohio.
They are shaped like a watermelon seed but can vary in size from a poppy seed to the head of a push pin, with eight legs and a hard, protective shell.
Hungry for a meal, they climb to the tip of a blade of grass or twig and wave their extra-long, hook-tipped forelegs in the air, a behavior called "questing," until a rabbit, deer or human brushes past.
They can detect carbon dioxide from their would-be host so they are ready to latch onto fur - or denim - if given a chance.
Ticks have ferocious barbed mouthparts like ratchets that allow them to pierce deeply into the skin and remain embedded while feeding on the host's blood.
They transmit diseases through their saliva in the form of spirochetes, corkscrew-shaped bacteria that whirligig through the host's bloodstream.
Students in UC's Department of Biological Sciences regularly collect ticks at parks in Hamilton and Butler counties.
Ticks are expensive to cultivate in a lab.
They require a live host such as a mouse or rabbit.
So for some studies, UC buys ticks from a lab at Oklahoma State University.
Surveillance research begins at UC's Center for Field Studies northwest of Cincinnati next to Miami Whitewater Forest.
The field station is home to other UC research projects examining the natural world from native birds to butterflies.
On a recent morning, UC postdoctoral fellow Andrew Rosendale and students Alicia Fieler, Benjamin Davies and Madisen Kimbrel dragged flags made of fleece over bushes and meadows full of summer wildflowers to collect ticks for their study.
Davies, 19, of West Chester, Ohio, and a graduate of Lakota West High School, initially found ticks revolting for the same reasons everyone does.
UC's biology labs also study cockroaches and mosquitoes.
"Among those three species, ticks were the best choice," he said.
"And working at the field station is a good way to get outside and experience nature."
Fieler, 21, a graduate of the Cincinnati-area Oak Hills High School, wants to become a nurse practitioner.
She has contributed to three published research papers on ticks so far as an undergraduate.
The fieldwork at UC is giving her valuable experience both inside and outside the lab, she said.
"I've learned so much - lab techniques and even simple things like safety protocols and basic lab procedures, how to run assays and bring all the data together," she said.
"Having this background in science will help me with future research as well."
Kimbrel, 20, a biology major from Monroe, Ohio, said studying a topic that has potential medical implications is a good research fit since she wants to pursue a career in medicine.
"It's great because you can take what you've learned in the classroom and apply it to the real world," she said.
Back at professor Benoit's biology lab, the students measure the levels of lipids, proteins and glycogen in the tick specimens.
Higher fat content is an indication of good tick nutrition.
And they can identify what diseases, if any, the ticks carry.
Once tick eggs hatch, the baby parasites seek a blood meal in each of their next three lifecycles: larvae, nymph and adult.
"They can transmit the disease at any stage of their life if they get infected.
It's much more common to be bitten by an adult around here.
But they can pick up the disease as a larvae or a nymph as well," UC's Rosendale said.
And they live for up to six years.
Rosendale, co-author of numerous studies on ticks, said the goal of the research is to find ways to slow or halt the spread of disease.
"We want to find better ways to kill them by studying the mechanisms of resistance of ticks to pesticides," he said.
Fossil evidence suggests ticks fed on dinosaurs and other Cretaceous creatures 90 million years ago.
Today, ticks contract Lyme and other diseases from animals such as mice that serve as reservoirs for the bacteria.
"Ticks carry countless diseases.
New ones are identified all the time," Benoit said.
Besides Lyme, ticks in many Midwestern and Northeastern states are known to carry diseases such as anaplasmosis, ehrlichiosis and Rocky Mountain spotted fever.
The bite of some lone star ticks in Kentucky and Indiana has been linked to an immune response that causes some people to become allergic to red meat, a disorder called alpha-gal.
"It's a bizarre condition.
People talk about it but it's relatively rare," he said.
Doctors have to be vigilant about tick-borne illnesses, even those not commonly observed in this area, said Dr. Carl J. Fichtenbaum, an infectious disease specialist at the University of Cincinnati Medical Center and the Cincinnati Veterans Affairs Medical Center.
"Lyme is uncommon here.
We see a handful of cases each year," he said.
"Someone goes to Cape Cod for vacation and gets bitten by a tick and comes back with an infection."
Typically, patients with Lyme complain of flu-like symptoms: headache, fever and achy joints or muscles, he said.
"They may get the characteristic bullseye rash around the bite, but that doesn't happen in everyone," he said.
Unlike mosquitoes, which transmit viruses such as West Nile or Zika in their bite, ticks transmit bacteria when they bite us, Fichtenbaum said.
"With tick-borne diseases, we don't have immunity to bacterial infections.
We can get infected over and over again.
Our body isn't capable of fending it off," he said.
A simple course of antibiotics cures most tick-borne diseases.
Fichtenbaum said surveillance efforts such as UC's ongoing study are invaluable in helping doctors understand the likelihood of patients contracting diseases from local ticks.
"I think it's great to sample the population of animals and insects to understand zoonosis, disease transmitted by live animals," he said.
"It's always good if we're keeping track of what's out there so we know the possibilities."
So far Lyme has not been found in the ticks Benoit's team has collected locally.
But one species, the dog tick, is a known carrier of Rocky Mountain spotted fever, which causes headache and fever and, sometimes days later, a spotted rash.
It's prevalent in Midwest and mid-Atlantic states.
Left untreated, tick-borne diseases can cause a cascade of problems from hearing loss to neurological complications.
Lyme disease affects far more people in the United States than any pathogen carried by mosquitoes in the United States.
About 30,000 cases of Lyme disease are reported each year to the Centers for Disease Control and Prevention.
The disease was observed in patients across 40 states in 2015.
But this does not capture all diagnosed cases - or the many suspected cases that go undiagnosed each year, the agency said.
A 2015 study by the CDC used medical-insurance claims between 2005 and 2010 to estimate that 329,000 people contract Lyme each year in the United States.
"The vast majority of research into tick-borne illness is where Lyme disease is prevalent - in the Northeast and upper Midwest," UC's Rosendale said.
"It hasn't received a lot of attention in Ohio because historically Lyme disease hasn't been as common.
We didn't have the blacklegged tick here.
But recently we've been seeing more and more of that species and so Lyme disease is becoming more of an issue."
Anyone who has plucked a tick off a pants leg knows they are tough.
But UC research is demonstrating just how hardy they are.
A UC study published in 2016 by the journal Ticks and Tick-Borne Diseases suggests that ticks have little problem surviving a typical Ohio winter.
Benoit, Rosendale and their student co-authors examined the cold tolerance of the American dog tick by exposing them to temperatures as cold as -9 degrees Fahrenheit.
They determined that no ticks survived more than two hours at that temperature.
But ticks could withstand temperatures just above 0 degrees and most survived temperatures in the 20s for at least two weeks.
The same group co-authored another study in the Journal of Experimental Biology that found that ticks could withstand long periods of drought, enduring dehydration even when they go without food for as long as 18 weeks.
Some tick species appear to be expanding their range, which means the diseases they carry could be an emerging concern, Rosendale said.
"We expect to see more reported cases of Lyme disease," he added.
"Warmer temperatures and milder winters are allowing them to survive.
We're seeing a change in population distribution and increases in numbers as well."
Local parks are paying close attention to the UC research.
As part of the permit, UC agreed to share its surveillance findings.
"It is a public health issue.
We want to be aware of what we have in our parks - if there are diseases present and how people can protect themselves," said Zurijanne Carter, natural resources specialist for the Great Parks of Hamilton County.
The park district occasionally gets calls from residents concerned about disease after finding a tick, she said.
"The research UC is doing would go a long way toward knowing more about ticks in Ohio," Carter said.
"It's similar research in how we monitor mosquito-borne illness."
Benoit has published research on a variety of parasites, including tsetse flies.
He traveled to Antarctica this year to study a different kind of arthropod, mites.
But ticks abide even there, in one of the most hostile and remote parts of the planet.
They live off - what else?
- penguins.
It has become an almost essential element of academic life, from college lecture halls to elementary classrooms: the group assignment.
Dreaded by some, loved by others, group projects typically aim to build teamwork and accountability while students learn about a topic.
But depending on the assignment and the structure of the groups, a project can turn out to be a source of great frustration -- for instructor and students alike -- or the highlight of the school year.
Now a University of Washington-led study of college students has found that the social dynamics of a group, such as whether one person dominates the conversation or whether students work with a friend, affect academic performance.
Put simply, the more comfortable students are, the better they do, which yields benefits beyond the classroom.
"They learn more," explained Elli Theobald, a postdoctoral researcher in the Department of Biology and the lead author on the study, published July 20 in PLOS ONE.
"Employers are rating group work as the most important attribute in new recruits and new hires.
If students are able to demonstrate that they have worked successfully in groups, it would seem that they should be more likely to land the job."
Theobald is part of the UW's Biology Education Research Group lab, formed by several faculty members in the Department of Biology about a decade ago to research how to most effectively teach biology to undergraduates.
A separate study by the BERG lab on group work, published in the July issue of Active Learning in Higher Education, finds that college students, when given a choice of whom to sit and work with in a large classroom setting, gravitate toward those who appear most like them -- whether by gender, race and ethnicity, or academic skills.
Over the years, research spanning K-12 through post-secondary education has pointed to the value of group work in fostering collaborative skills and in cementing learning through interaction.
In the sciences, labs are a common, though not the only, form of group work, Theobald said.
As with many disciplines, STEM fields lend themselves to readings, worksheets and other activities that can be completed by multiple people working together.
For this study, researchers compared survey responses and test scores stemming from two different project styles -- single-group and "jigsaw" -- with three assignments each during two sections of an introductory biology class at the UW.
Each of the 770 students enrolled in one of the two sections of the course experienced each project style at least once.
In a single-group activity, student groups completed a worksheet together, relying on their notes and textbooks.
In a jigsaw, student groups were assigned specific sections of the worksheet; students then were shuffled to new groups in which each person in the group had completed a different section of the worksheet and could teach their new groupmates what they had learned.
Students took an eight-question test after each assignment.
The study found that students who reported a "dominator" in the group fared worse on the tests than those who didn't express that concern.
It also found that students who said they were comfortable in their group performed better than those who said they were less comfortable.
The jigsaw activity appeared to result in more collaboration: Students were 67 percent less likely to report a dominator in jigsaws than in single-group activities.
"This suggests that jigsaw activities with intentional structure more effectively promote equity than group activities with less intentional structure," researchers wrote.
The nearly 770 students who completed all the assignments, tests and surveys had formed two- and three-person groups with those who sat near them in class.
(Jigsaw assignments later shuffled initial groups.)
Two-thirds of participants were female; people of color, including students who identify as Asian, Under-Represented Minority, and International, made up more than half of respondents.
While the gender and racial and ethnic makeup of the participants informed the study, Theobald said, researchers don't have details on who worked with whom so as to extrapolate from the composition of groups.
For instance, were the experiences of women who worked with men different from those of women who worked in all-female groups?
If a group contained only one person of color, what was that person's experience compared to the rest of the group?
That kind of information is ripe for further research, Theobald said.
However, one noticeable data point emerged: International and Asian American students were six times as likely to report a dominator than white American students.
"Not all students experience group work the same way," researchers wrote in the study.
"If one student dominates a conversation, it can be particularly jarring to students from cultural backgrounds that place more emphasis on introspection and thinking on one's own as opposed to a direct relationship between talking as a way to work through ideas."
Though the data was collected from college students, the findings translate to other settings, Theobald said.
She pointed to a study Google conducted to determine what made groups successful -- establishing group routines and expectations ("norms") and adding a brief window at the beginning of work time for casual talk.
Such findings, along with those of the UW study, can inform employers as well as K-12 teachers about productive group work, she said.
The younger the students, the more structure a teacher is likely to have to establish, Theobald added.
But when teachers make an assignment sufficiently interesting and complex, and manage student behavior, there is a potential for students to work together happily and productively.
"If we can get our groups to be more comfortable, students should learn better and work better," Theobald said.
###

The National Science Foundation funded the study.
Co-authors on the paper were Alison Crowe, principal lecturer in biology, and Benjamin Wiggins, faculty coordinator for biology instruction, both at the UW; Sarah Eddy of Florida International University; and Daniel Grunspan of Arizona State University.
For more information, contact Theobald at ellij@uw.edu.
Internationally subsidised agricultural insurance is intended to protect farmers in developing countries from the effects of climate change.
However, it can also lead to undesirable ecological and social side effects, as UFZ researchers and their US colleagues at the University of Oregon have explained in a review article in the latest issue of Global Environmental Change.
The article also contains recommendations for improved insurance schemes which in future should also take account of ecological and social aspects in addition to economic issues.
The effects of climate change are felt particularly acutely in developing countries.
A range of international initiatives develop and promote risk insurance.
One example is the G7 climate risk insurance initiative InsuResilience, which aims to insure 400 million people in developing countries against climate-related risks by 2020.
The initiative includes "agricultural insurance", which is designed to insure farmers against major losses, for example as a result of extreme drought.
"Agricultural insurance can be a secure and extremely helpful tool for farmers in affected areas," according to Dr Birgit Mller, socio-ecological modeller at UFZ.
"However, in their current format, the insurance policies are not always well thought out.
They can bring about unwanted environmental and social side effects, and so do little to help farmers adjust to long-term changes in environmental conditions."
For the current review article, Birgit Mller worked with Professor Leigh Johnson, geographer at the University of Oregon, and her UFZ colleague David Kreuer to collect empirical and model studies from around the world to provide a comprehensive overview of the potential impact of agricultural insurance.
"Previous studies have concentrated primarily on economic aspects.
Little attention has been devoted to the socio-ecological system as a whole," says Mller.
"But one thing is becoming clear: agricultural insurance can have a range of unwanted side effects, for example changes to farmers' land use strategies."
Small-scale farmers in developing countries traditionally grow a wide range of crops in their fields to ensure that at least one crop can survive a potential drought.
However, farmers are frequently reverting to monocultures because the agricultural insurance is often linked to specific crops and does not take effect if farmers cultivate a different crop.
And this has far-reaching ecological consequences: a decline in agricultural biodiversity, deterioration in soil quality, increased use of fertilisers and pesticides, which in turn increases the risk of water pollution.
However, even if agricultural insurance is not linked to specific crops, farmers with insurance cover may be inclined to grow riskier crops which promise high yields but also bring greater losses in an emergency.
Because the farmers have insurance, it is not absolutely necessary to adopt a sensible cultivation strategy.
Apart from ecological effects, the scientists also reveal some potential social side effects of agricultural insurance, such as the weakening of networks of small farmers in developing countries.
As a general rule, farmers help each other in the wake of major crop failures.
Agricultural insurance can lead to an insured farmer no longer helping another farmer who could have taken out insurance.
"Agricultural insurance and the resulting changes in land use strategies can cause this kind of unintended ecological and social feedback, which can in turn lead to further problems and costs," warns Leigh Johnson.
"In the long term, this could have a far-reaching impact on individual farms."
In their review, the researchers therefore put forward proposals on how to improve the design of agricultural insurance in future.
For example, the insurance policies should take effect only in emergencies such as extreme droughts; farmers would deal with medium droughts using their own risk management measures.
Mller: "Lessons have already been learned in the USA in terms of agricultural insurance, where the insurance premium is only subsidised by the state if a minimum quantity of cultivated crops is maintained and management has not been extended to ecologically valuable marginal areas," explains Mller.
"We hope that our review will contribute to the development of cohesive insurance schemes which take account of environmental and social aspects - it is important that development funds are channelled into well thought-out concepts that are effective and economic in the long term."
CINCINNATI -- Universal screening of newborns for hearing loss before they leave the hospital is not enough to improve language skills of children who are deaf and hard of hearing, according to a new study.
Research scientists at Cincinnati Children's Hospital Medical Center say that at least 40 percent of children with a hearing loss have the capacity for higher language levels - beyond what test scores indicate.
"We have focused efforts for children who are deaf or hard of hearing on obtaining a language level that is often considered in the normal or average range on standardized assessments," says Jareen Meinzen-Derr, PhD, an epidemiologist at Cincinnati Children's and lead author of a new study.
"But their language skills are not good enough if we account for their cognitive abilities."
Dr. Meinzen-Derr says there is a mismatch between the cognitive level children test at and the expectations for their language skills.
With a slightly modified evaluation approach, therapists would be able to recognize these mismatches at very young ages, she says, and intervene to bring these children up to their ability.
"We should not be satisfied with language levels that are lower than a child's cognitive ability."
The study is published online in the Journal of Developmental and Behavioral Pediatrics.
Researchers conducted the study in two phases.
The first occurred between 2011 and 2014 and enrolled children between the ages of 3 and 6 with mild to profound hearing loss in both ears (bilateral).
The second phase began in 2014 and enrolled children up to 3 years old, also with mild to profound bilateral hearing loss.
A total of 152 children were enrolled.
All children received a language assessment and a neurocognitive assessment.
A speech-language pathologist administered a standardized test to assess language skills.
Forty-one percent had a significant disparity between their language scores and their nonverbal cognitive scores, which the researchers defined as a "language underperformance."
"We believe that all children have potential," says Meinzen-Derr.
"Our measurements give us a target for that potential so that we at least know what the language goals for a child should be.
This also has implications for other aspects of development, such as social development and - further down the road - academics.
This is why we are pushing for better intervention, because we know we can do better for children who are deaf or hard of hearing."
###

The study was funded by grants from the March of Dimes (#12FY14-178), the Health Resources and Services Administration (R40MC21513), NIDILLR (#90IF0122), and the Center For Clinical and Translational Science and Training (NIH 8UL1-TR000077).
(BOSTON) -- Researchers are constantly expanding their arsenal of methods to decipher the spatial organization of biological structures.
Using microscopes, they can now visualize individual macromolecular components within DNA, protein, or other complexes.
However, this resolution typically requires sophisticated equipment applied to specially-processed samples, and it is difficult to simultaneously watch many types of molecules, especially at high density and throughput, or dynamic interactions.
Circumventing the need for expensive microscopes, some recent biochemical approaches attach barcoded DNA probes to molecular targets and then fuse those in nearby pairs together, often by DNA ligation.
These DNA "records" are later read out for analysis.
Because these methods destroy the DNA probes in the process of pairing, however, the information acquired from each molecular target cannot include more than one interaction, neither multiple at once nor one changing over time.
Such methods can severely limit the quality of any subsequent computational reconstruction, and make reconstruction of individual complexes impossible.
To overcome these limitations, a team at Harvard's Wyss Institute of Biologically Inspired Engineering led by Core Faculty member Peng Yin, Ph.D., has now developed a DNA nanotechnology-based method that allows for repeated, non-destructive recording of uniquely barcoded molecular pairings, rendering a detailed view of their components and geometries.
In the future, the approach could help researchers understand how changes in molecular complexes control biological processes in living cells.
The study is published in Nature Communications.
"Our method, which we call "Auto-cycling Proximity Recording" (APR), essentially acts as a continuous biochemical recorder of the molecular structures," said Yin, who is also Professor of Systems Biology at Harvard Medical School.
"APR allows us to look at many proximities simultaneously and repeatedly, and with minimal distrurbance to structure.
By assessing the full complement of all such pairs in many cycles, we can create a detailed view of a molecular structure and even observe different structural states of the same targets."
As proof-of-principle, the team designed multiple DNA probes in silico, and synthesized and attached them to molecular targets contained in the prescribed geometries of DNA origami nanostructures.
Through this newly-engineered, DNA-directed biochemical mechanism, a record in the form of a barcoded DNA strand is synthesized on the structure if and only if two of these DNA probes are in close enough proximity to each other ("proximity recording").
Records are released as they are synthesized, and later collected for sequence analysis.
Unlike other biochemical methods, each individual APR target can yield over 30 DNA records ("auto-cycling"), allowing robust data collection.
After collecting all DNA records, the team compiled their sequences and successfully reconstructed the geometry of the synthetic nanostructures.
Thus, the approach functions as a 'DNA nanoscope', which uses specifically engineered DNA biochemistry to visualize target pairs in a molecular object.
Expanding on these new capabilities, the Wyss researchers were even able to document changes in the state of individual nanostructures, raising the possibility that the approach could be used to correlate structural transitions in molecular complexes with their biological functions.
"By using antibodies and other widely used agents to direct DNA probes to molecular targets, we could apply APR technology to decode the components and geometries of biological complexes," said Thomas Schaus, M.D., Ph.D., a Wyss Institute Staff Scientist who as the study's first author together with Yin developed APR.
"The fact that individual DNA records carry unique, sequenceable barcodes and that the method is scalable may enable us to one day follow, individually, thousands or millions of macromolecules in a biochemical pathway."
"The development of APR as a nanotechnological means to decipher molecular structures without the need for elaborate and expensive microscopes really illustrates how the Wyss Institute's recently launched Molecular Robotics initiative can impact structural biology research and developments in many laboratories," said Wyss Institute Founding Director Donald Ingber, M.D., Ph.D., who also is the Judah Folkman Professor of Vascular Biology at HMS and the Vascular Biology Program at Boston Children's Hospital, as well as Professor of Bioengineering at the Harvard John A. Paulson School of Engineering and Applied Sciences.
###

Besides Yin and Schaus, the study was authored by the Wyss Institute's Postdoctoral Fellows Sungwook Woo, Ph.D., Feng Xuan, Ph.D. and Xi Chen, Ph.D.
The APR project was funded by the Wyss Institute's Molecular Robotics Initiative, as well as grants from the National Institutes of Health, the National Science Foundation, the Office of Naval Research and support from the Jane Coffin Childs Memorial Fund and the Damon Runyon Cancer Research Foundation.
Press Contact:

Benjamin Boettner

917-913-8051

Benjamin.Boettner@wyss.harvard.edu

Multimedia Contact:

Seth Kroll

617-432-7758

seth.kroll@wyss.harvard.edu

The Wyss Institute for Biologically Inspired Engineering at Harvard University uses Nature's design principles to develop bioinspired materials and devices that will transform medicine and create a more sustainable world.
Wyss researchers are developing innovative new engineering solutions for healthcare, energy, architecture, robotics, and manufacturing that are translated into commercial products and therapies through collaborations with clinical investigators, corporate alliances, and formation of new startups.
The Wyss Institute creates transformative technological breakthroughs by engaging in high risk research, and crosses disciplinary and institutional barriers, working as an alliance that includes Harvard's Schools of Medicine, Engineering, Arts & Sciences and Design, and in partnership with Beth Israel Deaconess Medical Center, Brigham and Women's Hospital, Boston Children's Hospital, Dana-Farber Cancer Institute, Massachusetts General Hospital, the University of Massachusetts Medical School, Spaulding Rehabilitation Hospital, Boston University, Tufts University, Charit - Universittsmedizin Berlin, University of Zurich and Massachusetts Institute of Technology.
A new study published in Alcohol and Alcoholism finds that mice bred to consume high amounts of alcohol, but controlled by diet, did not necessarily develop the most severe liver injuries, suggesting that diet may pay an important role in liver injury development.
Alcoholic liver disease is a global health burden and refers to a disease spectrum ranging from hepatomegaly and simple fatty liver (hepatic steatosis), to more severe pathologies such as alcoholic steatohepatitis and hepatic cirrhosis.
In the United States about half of the population drinks alcohol and approximately 38 million people are estimated to engage in binge drinking behavior.
This study sought to compare mice bred to preferentially consume high amounts of alcohol (crossed-High Alcohol Preferring, or cHAP, mice) to other mice using a chronic-binge ethanol ingestion model to induce alcoholic liver disease.
The mice were randomized and given different diets over a four-week period.
Researchers collected tissue and serum.
The researchers discovered that the cHAP mice on a diet of alcohol and water consumed significantly more alcohol than cHAP or other mice maintained on an alcohol diet.
However, cHAP and other mice on the alcohol diet together with the artificial sugar maltodextrin had greater hepatosteatosis and overall degree of liver injury compared to mice that consumed a diet of alcohol and water together with maltodextrin.
These data suggest factors other than total amount of alcohol consumed may affect the degree of alcoholic liver disease development.
Additionally, because cHAP mice exhibit increasing ethanol consumption over time, consume ethanol in parallel with normal dietary intake, and show higher levels of daily ethanol consumption than mice maintained on the controlled diet, this model may provide an additional rodent model to study the effects of ethanol on hepatic pathology that more closely mimics human patterns of ethanol consumption in heavy drinkers.
In discussing these outcomes, the authors speculated saturated fat in the diet of the standard rodent chow used, and/or epigenetic changes during strain development, may have accounted for lack of liver injury.
This position is corroborated by studies demonstrating a protective role for saturated fats in chronic ethanol-fed rodents in which diminished inflammation and decreased micro- and macrovesicular steatosis occurs to promote hepatic fatty oxidation.
Saturated fats may also inhibit the development of alcoholic liver disease by maintaining growth of intestinal microbiota.
The findings suggest that although cHAP mice consume consistently high/sustained levels of ethanol, other factors such as disparities in specific dietary components, differences in the patterns of alcohol consumption, and timing of feeding relative to peak blood-alcohol content, alter the degree of liver injury in cHAP versus other mice.
"A critical role of the gut microbiome and fecal metabolites is becoming increasingly Appreciated," wrote Irina Kirpich and Craig McClain in an editorial accompanying the study.
Marked differences in the composition of the diets used in this study may help explain why mice consuming the highest amounts of alcohol did not develop the most severe liver injury.
Diet and microbiome may be important variables in the different outcomes observed in various experimental alcoholic liver disease models."
###

The paper "Use of a Crossed High-Alcohol Preferring (cHAP) Mouse Model with the NIAA-Model of Chronic-Binge Ethanol Intake to Study Liver Injury" is available at: https:/ / academic.
com/ alcalc/ article/ doi/ 10.
1093/ alcalc/ agx063/ 4191311/ Use-of-a-crossed-high-alcohol-preferring-cHAP

Direct correspondence to:

Iain H. McKillop

Cannon Research Center,

Carolinas Medical Center,

1542 Garden Terrace

Charlotte, NC, 28203

704-355-2843

iain.mckillop@carolinashealthcare.org

To request a copy of the study, please contact:

Daniel Luzer

daniel.luzer@oup.com

Sharing on social media?
Find Oxford Journals online at @OxfordJournals
No association found between advanced cardiac life support during transport and an increased survival for patients with out-of-hospital cardiac arrest

DES PLAINES, IL-- There is no association between prehospital advanced cardiac life support (ACLS) and survival to hospital discharge in patients suffering from out-of-hospital cardiac arrest (OHCA).
ACLS is, however, associated with an improvement in prehospital return of spontaneous circulation (ROSC), but with longer delays to hospital arrival.
These are the primary findings of a study published in Academic Emergency Medicine (AEM) a journal of the Society for Academic Emergency Medicine (SAEM).
The lead author is Alexis Cournoyer, MD, an emergency physician at the Hpital du Sacr-Coeur de Montral and a resident in the clinician-scientist program at the Universit de Montral.
He is also currently doing a PhD at the Universit de Montral in pre-hospitals resuscitation.
Dr. Cournoyer's study concludes that there is no clear advantage of the addition of prehospital ACLS to basic cardiac life support (BCLS) compared to prehospital BCLS alone, in tiered-response urban emergency medical services setting.
The study further suggests that efforts should be focused on increasing bystander CPR, decreasing emergency medical service response times, and decreasing time to defibrillation.
Dr. Cournoyer: "Prehospital ACLS does not seem to be associated with an increased survival for patients with OHCA.
When available, additional resources should be allocated to decrease delays to CPR, defibrillation, and first EMS contact.
Also, for patients eligible for E-CPR, it should be considered to put efforts into minimizing their delay before E-CPR."
The findings of the study are discussed with Dr. Cournoyer in the featured episode of Skeptics' Guide to Emergency Medicine Hot Off the Press (#SGEMHOP).
###

The Society for Academic Emergency Medicine (SAEM) is a 501(c)(3) not-for-profit organization dedicated to the improvement of care of the acutely ill and injured patient by leading the advancement of academic emergency medicine through education and research, advocacy, and professional development.
To learn more, visit saem.org.
For the first time, researchers have been able to see changes in the neural structures in specific areas of the brains of people who suffered severe abuse as children.
Difficulties associated with severe childhood abuse include increased risks of psychiatric disorders such as depression, as well as high levels of impulsivity, aggressivity, anxiety, more frequent substance abuse, and suicide.
Severe, non-random physical and/or sexual child abuse affects between 5-15 % of all children under the age of 15 in the Western world.
Researchers from the McGill Group for Suicide Studies, based at the Douglas Mental Health University Institute and McGill University's Department of Psychiatry, have just published research in the American Journal of Psychiatry that suggests that the long-lasting effects of traumatic childhood experiences, like severe abuse, may be due to an impaired structure and functioning of cells in the anterior cingulate cortex.
This is a part of the brain which plays an important role in the regulation of emotions and mood.
that suggests that the long-lasting effects of traumatic childhood experiences, like severe abuse, may be due to an impaired structure and functioning of cells in the anterior cingulate cortex.
This is a part of the brain which plays an important role in the regulation of emotions and mood.
The researchers believe that these changes may contribute to the emergence of depressive disorders and suicidal behaviour.
Crucial insulation for nerve fibres builds up during first two decades of life

For the optimal function and organization of the brain, electrical signals used by neurons may need to travel over long distances to communicate with cells in other regions.
The longer axons of this kind are generally covered by a fatty coating called myelin.
Myelin sheaths protect the axons and help them to conduct electrical signals more efficiently.
Myelin builds up progressively (in a process known as myelination) mainly during childhood, and then continue to mature until early adulthood.
Earlier studies had shown significant abnormalities in the white matter in the brains of people who had experienced child abuse.
(White matter is mostly made up of billions of myelinated nerve fibres stacked together.)
But, because these observations were made by looking at the brains of living people using MRI, it was impossible to gain a clear picture of the white matter cells and molecules that were affected.
To gain a clearer picture of the microscopic changes which occur in the brains of adults who have experienced child abuse, and thanks to the availability of brain samples from the Douglas-Bell Canada Brain Bank (where, as well as the brain matter itself there is a lot of information about the lives of their donors) the researchers were able to compare post-mortem brain samples from three different groups of adults: people who had committed suicide who suffered from depression and had a history of severe childhood abuse (27 individuals); people with depression who had committed suicide but who had no history of being abused as children (25 individuals); and brain tissue from a third group of people who had neither psychiatric illnesses nor a history of child abuse (26 people).
Impaired neural connectivity may affect the regulation of emotions

The researchers discovered that the thickness of the myelin coating of a significant proportion of the nerve fibres was reduced ONLY in the brains of those who had suffered from child abuse.
They also found underlying molecular alterations that selectively affect the cells that are responsible for myelin generation and maintenance.
Finally, they found increases in the diameters of some of the largest axons among only this group and they speculate that together, these changes may alter functional coupling between the cingulate cortex and subcortical structures such as the amygdala and nucleus accumbens (areas of the brain linked respectively to emotional regulation and to reward and satisfaction) and contribute to altered emotional processing in people who have been abused during childhood.
The researchers conclude that adversity in early life may lastingly disrupt a range of neural functions in the anterior cingulate cortex.
And while they don't yet know where in the brain and when during development, and how, at a molecular level these effects are sufficient to have an impact on the regulation of emotions and attachment, they are now planning to explore this in further research.
###

Funding was provided by the Fondation Fyssen, the Fondation Bettencourt-Schueller, the Canadian Institutes of Health Research (CIHR), the American Foundation for Suicide Prevention (AFSP), the Fondation pour la Recherche Mdicale, and the Fondation Deniker, the Fonds de Recherche du Qubec-Sant (FRQS), the Canada Research Chair Program, NARSAD Distinguished Investigator Award, the NIH, and Pfizer.
The hurricanes Harvey, Irma and Maria highlight the potential for the climate system to cause loss and damage.
"Loss and damage" is a phrase used in different ways by people who work on climate policy, negotiation and adaptation/resilience.
A new study clarifies these different perspectives which is a key issue now that the United Nations Framework Convention on Climate Change, UNFCCC, is encouraging creation and implementation of actions to address loss and damage from climate change.
Loss and Damage (L&D) has been debated at climate negotiations for decades.
In the early days of the UNFCCC, small islands states called for an international insurance pool covering residual damage associated with rising sea levels that couldn't be prevented by mitigation and adaptation efforts.
The L&D issue is complex, and sensitive, involving climate change impacts and risks and their effects on developing countries that are more vulnerable to climate change.
Despite the challenges, L&D has now entered the formal architecture of the UNFCCC.
First, in 2013 there was the establishment of the Warsaw International Mechanism (WIM) for Loss and Damage associated with Climate Change Impacts.
Then, in 2015, the Paris Agreement established a separate article on L&D and confirmed the long-term existence of the WIM.
But what does addressing L&D actually mean?
And, in particular, how might efforts to minimise and address L&D from climate change impacts differ from existing efforts under "adaptation"?
The official documents do not provide clear answers to these questions: "strategic ambiguity" which has arguably been fundamental for successful agreement between countries.
But now, researchers and practitioners are starting to ask how they can help address L&D, and many are confused about what this might involve.
Experts have begun to develop concepts and frameworks for L&D policy, but, until now, there has not been an empirical research study to analyse expert opinions.
"Given this rather confusing landscape, we hope our work will provide important clarity on the range of perspectives on L&D, in order to move the discussions forward.
Especially since the way we speak and think about these matters has implications for actions on the ground", says lead author of the study, Professor Emily Boyd at Lund University Centre for Sustainability Studies.
"As scientists, we found ourselves asking 'what kind of research might be relevant to inform L&D policy?'.
But it is hard to identify specific research gaps when some of the policy discussions are quite vague.
This led us to start asking people working on L&D what it meant to them, and we realized that we were getting a range of rather different answers."
explains co-author Dr Rachel James, from the Environmental Change Institute, University of Oxford.
"As we began exploring these different perspectives, we also realized areas of commonality such as the relevance of climate risk assessment and monitoring and evaluation and the importance of research-policy dialogue to identify and prioritize research questions" noted co-author Dr. Richard Jones from the Met Office Hadley Centre.
In the study, a number of stakeholders across science, practice and policy (such as UNFCCC negotiators, climate scientists and economists) from both industrialised and developing countries were interviewed about their viewpoint on L&D.
Four perspectives on L&D emerged:

1.
Adaptation and Mitigation perspective - where stakeholders highlight all human climate change impacts as potential L&D and have the opinion that current UNFCCC mechanisms for adaptation and mitigation are sufficient to address L&D.
Risk Management perspective - where stakeholders view discussions around L&D as an opportunity to work towards comprehensive risk management by building on existing efforts under disaster risk reduction, climate change adaptation and humanitarian work.
Limits to Adaptation perspective - this viewpoint is centred around the limits to adaptation, and how to deal with residual L&D that cannot be, or has not been, avoided through mitigation or adaptation.
Stakeholders suggest that L&D applies to impacts of any climate-related event, not just those directly linked to climate change, and have a focus on vulnerability.
Existential perspective - The viewpoint where L&D discussions represent a means to highlight the importance of addressing the inevitable harm which climate change will impose on vulnerable countries, populations, cultures and ecosystems.
There is also discussion of compensation, whether monetary or non-monetary.
"By identifying that there are different perspectives on L&D, we can move towards creating a shared platform for future research and policy work.
We found it especially interesting that there was no clear dividing line in terms of perspective variation between stakeholders from industrialised and developing countries, which could have been expected in light of the risks associated with climate change and extreme events."
Emily Boyd notes that it is the complexity surrounding the meaning and use of L&D which is important to highlight.
"Even if countries might be reluctant to acknowledge that there are different perspectives in political climate negotiations, we see it as key that policy makers are aware of these diverse viewpoints.
Otherwise, we think that it will be very hard to move forward and develop this policy space," she concludes.
Athens, Ga. - It is well-known in the medical field that the pig brain shares certain physiological and anatomical similarities with the human brain.
So similar are the two that researchers at the University of Georgia's Regenerative Bioscience Center have developed the first U.S. pig model for stroke treatments, which will provide essential preclinical data and speed the drug discovery process.
Often referred to by research teams as, "the animals most like people," pig-derived medical products have a long history of use in humans and have improved the lives of countless patients.
Pig heart valves are used to replace damaged or diseased human valves, diabetics may use insulin taken from pigs, and the blood-thinning drug heparin was first derived from a pig.
"Compared to mice, our large animal stroke model is a more rigorous test of potential therapeutics with findings that are likely more clinically relevant," said Franklin West, an associate professor in the College of Agricultural and Environmental Sciences and senior author of the paper describing the model.
Using their model, the team has shown that induced neural stem cells, or iNSCs, can replace stroke-damaged brain tissue and stimulate neuroplasticity--the brain's ability to naturally repair itself.
"This is the first time that a neural stem cell therapy has been tested in a large animal model with a brain more similar to humans," said Emily Baker, lead author of the study, who recently received her doctorate in neuroscience from UGA.
"With greater predictive capabilities, there's a better likelihood of it working in a human."
Stroke is the cause of one in every 19 U.S. and has estimated annual cost of $315 billion--56 percent higher than treatment associated costs of all cancers, according to the American Heart Association.
Despite the urgent need, almost all clinical trials of neuroprotective therapies to date have failed to translate from the laboratory to the clinic, according to the RBC research team.
Stoke therapies that worked on small animals such as mice more often than not proved ineffective in human strokes.
With the failure of so many clinical trials, the concept of neuroprotection and the inability to demonstrate a regenerative action to restore and replace brain tissue has been the subject of many discussions in both research and medical communities.
Through this model, the research team was able to more precisely establish the repair mechanism by which stem cells work in neural tissue regeneration and neuroprotection than was previously known.
"The takeaway from this work is that the injected stem cells led to cellular and tissue improvement," West said.
"Basically, if you have a stroke and you get this treatment, fewer neurons are going to die, and for stroke research that's critically important."
The findings from the study, published in Nature's Scientific Reports, suggest that there are peripherals of salvageable brain tissue that would benefit from cell-based restorative therapies after acute ischemic stroke.
Specifically, iNSCs that naturally promote brain plasticity and recovery.
In collaboration with Emory University and UGA's College of Veterinary Medicine, the team's work shows improved recovery in white matter, the "superhighways of connectivity" that connect key centers of the brain.
This change in response to restorative therapy can be monitored using MRI imaging techniques.
"From the MRI we learned of a recovery mechanism in that neural stem cell therapy improves white matter integrity," said Baker.
"We now have white matter tracts that allows for faster, more effective communication from one region in the brain to another after injury."
The research marks a major milestone and, while a long way from clinical use, it may speed stroke discoveries by providing a better, more predictive translational model.
Researchers within the RBC are already using this model as a platform for future nanotechnology applications.
"If we can replace lost brain tissue and neural systems that are basically gone from stroke, which would lead to functional improvement or functional independence like feeding yourself, getting yourself dressed and being able to move again, then we've met our long-term goals--but in the bigger picture what we've done is improve the quality of life," said West.
###

The study, "Induced Pluripotent Stem Cell-Derived Neural Stem Cell Therapy Enhances Recovery in an Ischemic Stroke Pig Model," is available online at https:/ / www.
nature.
com/ articles/ s41598-017-10406-x
New Rochelle, NY, September 25, 2017--A phase 3 study of children ages 6-12 years with attention deficit/hyperactivity disorder (ADHD) has shown that a delayed-release, long-acting formulation of the stimulant methylphenidate, when taken in the evening, led to significant improvement in ADHD symptoms and functional impairment first thing the next morning, compared to a placebo.
Children taking the delayed-release stimulant did not have to wait for a morning dose to take effect and also benefited from improved symptoms later in the afternoon and evening, according to the study results published in Journal of Child and Adolescent Psychopharmacology, a peer-reviewed journal from Mary Ann Liebert, Inc., publishers.
The article is available free on the Journal of Child and Adolescent Psychopharmacology website.
The article entitled "Efficacy and Safety of HLD200, Delayed-Release and Extended-Release Methylphenidate, in Children with Attention-Deficit/Hyperactivity Disorder" is coauthored by Steven Pliszka, MD, from The University of Texas Health Science Center at San Antonio and colleagues from Massachusetts General Hospital (Boston, MA), Westside Medical Family Practice (Clinton, UT), University of Tennessee Health Science Center (Memphis, TN), Meridien Research (Maitland and Bradenton, FL), Children's Development Center (Winter Park, FL), Ironshore Pharmaceuticals & Development (Grand Cayman, Cayman Islands), and Mount Sinai Medical Center (New York, NY), on behalf of the HLD200-108 Study Group.
The drug formulation, which consists of two layers of microbeads with an inner drug-loaded core, delays release of the active ingredient for 8-10 hours and then provides controlled extended release designed to cover the early morning into the evening.
The medication was well tolerated, with the main adverse effects of appetite suppression and insomnia being those commonly reported for other formulations of methylphenidate.
"Developing new formulations of effective medications for patients with ADHD improves the lives of children with the disorder," says Harold S. Koplewicz, MD, Editor-in-Chief of the Journal of Child and Adolescent Psychopharmacology and President of the Child Mind Institute in New York.
###

About the Journal

Journal of Child and Adolescent Psychopharmacology is an authoritative peer-reviewed journal published bimonthly in print and online.
The Journal is dedicated to child and adolescent psychiatry and behavioral pediatrics, covering clinical and biological aspects of child and adolescent psychopharmacology and developmental neurobiology.
Complete tables of content and a sample issue may be viewed on the Journal of Child and Adolescent Psychopharmacology website: http://www.
liebertpub.
com/ cap .
About the Publisher
LAWRENCE -- Minority public managers place more emphasis on both traditional values, like efficiency and effectiveness, and social equity when compared with their white counterparts, according to a new study that includes a University of Kansas researcher and two fellow KU alumni.
"Race does make a difference in how we talk about public values at the local government level, and we need more research on what that means," said Shannon Portillo, associate professor in the KU School of Public Affairs & Administration.
In their recent study published in the journal Public Administration, Portillo and her co-authors used data collected as part of the National Administrative Studies Project, which surveys senior managers and department leaders in U.S. local governments that serve a population of at least 50,000.
Her co-authors are fellow KU School of Public Affairs & Administration alumni Edmund Stazyk, of the Department of Public Administration of Policy at State University of New York at Albany, and Randall Davis, of the Department of Political Science at Southern Illinois University.
"For a long time we've talked about the idea that equity is potentially in competition with ideas of efficiency and effectiveness, but for minority managers, that wasn't the case," Portillo said.
"It is possible for managers to focus on how these public values can work in concert with each other more than conflict."
Prior research on public management had not examined whether race was a factor in how managers emphasized the traditional and social equity values.
The researchers said it was not surprising that both minority and white managers are likely to be attentive to traditional public administration values -- such as budgetary decisions being based on departmental performance -- because most public administrators likely entered the field due to the desire to help others.
The fact that minority managers place a higher value on both social equity public values and the traditional ones compared with their white counterparts might seem counterintuitive initially to the public, Portillo said.
"It may be that minority managers and managers generally think that equity won't be taken as seriously if it's sensed to be in competition with questions of efficiency and effectiveness," she said.
The minority respondents based on their own lived experience could have a more nuanced understanding of how the three types of values can be inter-related, the researchers said.
For example, managers in U.S. communities grapple with issues such as the outcomes of police stops varying by a person's race, or disparities by race in access to transportation, health care and the segregation of housing and public education.
"They connect all of these goals.
What we argue is that this may be a form of trying to legitimize their focus," Portillo said.
"These types of values really should be seen as in conversation with each other.
Right now, we really only find that for the minority public managers."
BUFFALO, N.Y. -- Scientists have used satellite data to monitor underground water reserves in California's Silicon Valley, discovering that water levels rebounded quickly after a severe drought that lasted from 2012-15.
The research points to the success of aggressive conservation measures.
It also helps to lay the groundwork for low-cost monitoring of subterranean water reserves in California and elsewhere in the world.
Underground stockpiles of water -- housed in layers of porous rock called aquifers -- are one of the world's most important sources of drinking water.
They sustain human populations in places from Silicon Valley to Beijing.
Some 2.5 billion people on Earth rely on aquifers for water, and many of these repositories are being drained more quickly than they can be refilled, according to the United Nations Educational, Scientific and Cultural Organization.
Yet, keeping tabs on these precious reserves is expensive, says Estelle Chaussard, PhD, an assistant professor of geology in UB's College of Arts and Sciences and the lead author of the new research.
"To monitor aquifers, you have to monitor water levels in as many wells as possible," she says.
"So if you have 300 wells in the area, you have to either have someone who physically goes there all the time, or instruments in each well that monitor permanently, which is very costly.
"We wanted to see if we could use a low-cost remote sensing method that doesn't require ground monitoring to understand how our aquifers are responding to a changing climate and human activity."
The research was published on Sept. 22 in the Journal of Geophysical Research.
The team included Pietro Milillo and Eric J.
Fielding from NASA's Jet Propulsion Laboratory; Roland Brgmann from the University of California, Berkeley; Daniele Perissin from Purdue University; and Brett Baker from the Santa Clara Valley Water District.
Silicon Valley's aquifer began rebounding mid-drought

During the 2012-15 drought, the Santa Clara Valley Water District employed an array of conservation measures.
These included restricting sprinkler use and asking customers to take shorter showers and convert lawns and pools into less thirsty landscapes.
The district also imported water from outside the region.
Chaussard says these actions may have helped to stave off irreversible damage to the valley's aquifer.
She explains that when groundwater levels reach a record low, the porous clay in which the reserves reside can dry up so much that the clay can't soak up water anymore.
The new study shows that this did not happen in the Santa Clara Valley.
Due to intensive water management efforts, the Santa Clara Valley aquifer actually started to rebound by late 2014, when the drought was still going strong, the research found.
Groundwater had returned to pre-drought levels by 2017.
Chaussard notes that this does not mean aquifers do not need protection: Urban, agricultural and other uses can overtax these basins even in times of abundant rainfall, and many of California's aquifers are not adequately monitored.
The recent drought heightened awareness of the importance of groundwater, with the Sustainable Groundwater Management Act of 2014 requiring local and regional authorities to manage this resource sustainably for the first time in state history after decades of neglect.
Chaussard says satellite tracking could provide an avenue for monitoring groundwater levels across California in near-real time, a key factor in preventing overuse.
Measuring tiny movements of Earth's surface

Chaussard's team used a technique called InSAR -- Interferometric Synthetic Aperture Radar -- to capture the tiny up-and-down movements of the Earth's surface that occur when water levels rise or fall underground.
"InSAR allows us to precisely measure small movements of the Earth's surface over large areas," Chaussard says.
"Scientists have used it to measure surface deformation related to volcanoes and earthquakes, we expanded its use by applying it to tracking groundwater."
She first employed InSAR to monitor aquifers in 2014, and says the method can work anywhere in the world where satellite data is available, including in developing nations with few resources for monitoring.
The technique marks an improvement over traditional methods because it allows scientists to gauge changes in water levels across a large region with great frequency.
The data in the new study came from COSMO SkyMed, a constellation of four Italian satellites that provided information for the entire Santa Clara Valley aquifer as often as once a day.
"If you're monitoring wells, even if you have lots of them, you are never going to have full coverage of what is going on," Chaussard says.
"Our method provides a more complete picture and helps fill in the gaps."
Scientists tip balance of plant metabolism to increase oil content in leaves with aim of making biofuels and related useful chemicals

UPTON, NY--Eat too much without exercising and you'll probably put on a few pounds.
As it turns out, plant leaves do something similar.
In a new study at the U.S. Department of Energy's Brookhaven National Laboratory, scientists show that retaining sugars in plant leaves can make them get fat too.
In plants, this extra fat accumulation could be a good thing.
It could help turn plants into factories for making biofuels and other useful chemicals.
But you can't just feed plants cookies and donuts to get leaves to pump out more oil.
"Plants make their own food," said John Shanklin, the Brookhaven Lab biochemist who led the research.
"They convert sunlight, water, and carbon dioxide into sugars through photosynthesis, and those sugars are usually transported out of leaves to other parts of the plant, and converted to other compounds plants need for growth and development.
Oils tend not to accumulate to high levels, except in some seeds."
To tip the balance in favor of higher oil production and accumulation in leaves--which would be more abundant and accessible than seeds for making bio-based chemicals--the scientists needed detailed knowledge of the biochemical processes that drive the metabolic pathways and the genes that control them.
As described in a paper published in the journal Plant Physiology, the Brookhaven team selectively bred plants to combine a series of traits that blocked some of the sugar transport and conversion pathways, which resulted in increased oil production and accumulation.
The scientists suspected that keeping more sugar in leaves would increase oil production in those vegetative tissues based on their earlier research.
That work identified how high sugar levels inhibit a regulatory protein that plays a key role in the breakdown of another protein that serves as a switch for turning on oil-production genes.
In the current study, the scientists found increased levels of the on-switch protein along with higher levels of oil precursors and oil.
"Combining genetic mutations that decrease the transport of sugar out of leaves and the conversion of sugars to starch increases sugar levels in leaves," Shanklin said.
"That excess sugar drives increased oil production by stabilizing the oil on-switch, and also by supplying the carbon building blocks needed to make more oil in leaves."
The scientists also bred plants that combined the sugar-increasing traits with other mutations, including one that limited the breakdown of plant lipids, which resulted in further increases in oil precursor accumulation in leaves.
"In several cases, combinations of these mutations helped tip plant metabolism to produce and store more oil than expected in leaves," Shanklin said.
"Findings from these foundational biochemical-genetic studies provide valuable insights into the relationship between sugar, oil precursors, and oil accumulation in leaves that will help inform biotechnological efforts to optimize oil accumulation in vegetative tissues of economically important plants."
###

This research was funded by the DOE Office of Science.
Brookhaven National Laboratory is supported by the Office of Science of the U.S. Department of Energy.
The Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time.
For more information, please visit science.energy.gov

Follow @BrookhavenLab on Twitter or find us on Facebook.
One of ten national laboratories overseen and primarily funded by the Office of Science of the U.S. Department of Energy (DOE), Brookhaven National Laboratory conducts research in the physical, biomedical, and environmental sciences, as well as in energy technologies and national security.
Brookhaven Lab also builds and operates major scientific facilities available to university, industry and government researchers.
Brookhaven is operated and managed for DOE's Office of Science by Brookhaven Science Associates, a limited-liability company founded by the Research Foundation for the State University of New York on behalf of Stony Brook University, the largest academic user of Laboratory facilities, and Battelle, a nonprofit applied science and technology organization.
Related Links

Scientific paper: "Sugar Potentiation of Fatty Acid and Triacylglycerol Accumulation"

Study IDs Link Between Sugar Signaling and Regulation of Oil Production in Plants

An electronic version of this news release with related graphics

Media contacts: Karen McNulty Walsh [kmcnulty@bnl.gov], (631) 344-8350, or Peter Genzer [genzer@bnl.gov], (631) 344-3174
University Park, PA -- A gene associated with the risk of schizophrenia regulates critical components of early brain development, according to a new study led by researchers from Penn State University.
The gene is involved in the translation of proteins from RNA and in the proliferation and migration of neurons in the brain.
Understanding the function of this gene -- described this month in journal Molecular Psychiatry -- could lead to more effective treatments for schizophrenia.
"A recent study identified over 100 genes associated with schizophrenia risk, but their functions are largely unknown," said Yingwei Mao, associate professor of biology at Penn State and lead author of the study.
"We investigated one of these genes that is known to be significantly associated with schizophrenia: ZNF804A.
In this study, we provide molecular evidence showing that ZNF804A could contribute to psychiatric disorders like schizophrenia."
Schizophrenia is a severe mental disorder that affects approximately 1 percent of the world's population.
Treatments tend to focus on alleviating symptoms, which include delusions and hallucinations, rather than addressing the underlying causes.
Like many human diseases, schizophrenia is complex, and no single genetic or environmental factor has been identified as the cause of the disease.
"We found that ZNF804A affects brain structure and function during early fetal development," said Mao.
"This supports the idea that changes early in neurodevelopment can produce effects that may not be triggered until adulthood.
Although schizophrenia symptoms typically appear in late adolescence or early adulthood, genetic mutations affecting early neurodevelopment could embed risk for future behavioral changes."
The research team identified genes that interact with ZNF804A, which include 9 genes involved in controlling the translation of RNA to proteins.
This suggests an influential role of ZNF804A in this important decoding process.
Using a mouse model, the team also demonstrated that two processes during neurodevelopment are regulated by the gene: proliferation -- the replication of neuronal stem cells that have the potential to become multiple different kinds of cells, including neurons -- and migration -- the movement of neurons to specific locations in the brain during development.
"ZNF804A is critical to regulating proliferation and migration," said Mao.
"Disturbances to these processes may cause neuronal stem cells to develop into different types of cells or may cause neurons to migrate to different locations in the brain, changing neuronal circuitry and potentially leading to behavioral disorders like schizophrenia."
ZNF804A also interacts with and modulates expression of other genes known to be associated with schizophrenia.
"Determining the role of ZNF804A is the first step in understanding how schizophrenia-associated genes contribute to abnormal brain development," said Mao.
"Understanding how these genes interact to contribute to the development of schizophrenia may allow us to identify the general pathway of the disease, potentially providing a better target for treatment."
###

In addition to Mao, the research team includes Yijing Zhou, Fengping Dong, Ming Li, and Long Liu at Penn State; Thomas Lanz, Veronica Reinhart, and Hualin Xi at Pfizer; and Jizhong Zou at the National Institute of Health.
This research was funded by the National Institutes of Health and supported by the Brain & Behavior Research Foundation, the American Heart Association, the National University of Defense Technology, and the Huck Institutes of the Life Sciences.
CONTACTS:

Yingwei Mao: yzm1@psu.edu, (814) 867-4739

Barbara K. Kennedy (PIO): bkk1@psu.edu, (814) 863-4682
A recent analysis of the existing research on factors associated with an individual's risk for engaging in terrorist activity highlights how little we know about these factors and the need for additional research in this area.
"It's important to have a better understanding of what distinguishes potential terrorists from individuals who pose little or no risk of becoming terrorists, whether we're talking about Middle Eastern terrorist organizations or domestic terrorists in the United States," says Sarah Desmarais, an associate professor of psychology at North Carolina State University and lead author of a paper on the work.
"When we looked into this area, we found that there is surprisingly little research in the field - and that needs to be addressed."
The researchers conducted a systematic review of research literature, dating back to 1990, related to factors associated with an individual's joining a terrorist organization or perpetrating terrorist attacks.
The in-depth review found 205 articles relevant to the subject matter, of which only 50 reported on findings related to empirical data.
Of those 50, only 24 articles included any statistical analysis.
And of those 24, only six articles compared the characteristics of terrorists to the characteristics of non-terrorists - which is essential if the work is going to offer any insight into what factors are associated with terrorist behavior.
"Basically, over the past quarter century, there have been six research articles that are useful in identifying someone who is likely to engage in terrorism," Desmarais says.
"And the quality of those six studies is variable.
"There may be classified research I'm not aware of, but this highlights the need for more research in this area," Desmarais says.
"It is, literally, an issue that can affect national security."
The paper, "The State of Scientific Knowledge Regarding Factors Associated With Terrorism," is published in the Journal of Threat Assessment and Management.
The paper was co-authored by Joseph Simons-Rudolph, a teaching assistant professor of psychology at NC State; Christine Brugh and Eileen Schilling, graduate students at NC State; and Chad Hoggan, an assistant professor of educational leadership, policy and human development at NC State.
The work was done with funding from the Laboratory for Analytic Sciences, a research partnership between NC State and the National Security Agency.
A new filter produced by Rice University scientists has proven able to remove more than 90 percent of hydrocarbons, bacteria and particulates from contaminated water produced by hydraulic fracturing (fracking) operations at shale oil and gas wells.
The work by Rice chemist Andrew Barron and his colleagues turns a ceramic membrane with microscale pores into a superhydrophilic filter that "essentially eliminates" the common problem of fouling.
The researchers determined one pass through the membrane should clean contaminated water enough for reuse at a well, significantly cutting the amount that has to be stored or transported.
The work is reported in Nature's open-access Scientific Reports.
The filters keep emulsified hydrocarbons from passing through the material's ionically charged pores, which are about one-fifth of a micron wide, small enough that other contaminants cannot pass through.
The charge attracts a thin layer of water that adheres to the entire surface of the filter to repel globules of oil and other hydrocarbons and keep it from clogging.
A hydraulically fractured well uses more than 5 million gallons of water on average, of which only 10 to 15 percent is recovered during the flowback stage.
"This makes it very important to be able to re-use this water," Barron said.
Not every type of filter reliably removes every type of contaminant, he said.
Solubilized hydrocarbon molecules slip right through microfilters designed to remove bacteria.
Natural organic matter, like sugars from guar gum used to make fracking fluids more viscous, require ultra- or nanofiltration, but those foul easily, especially from hydrocarbons that emulsify into globules.
A multistage filter that could remove all the contaminants isn't practical due to cost and the energy it would consume.
"Frac water and produced waters represent a significant challenge on a technical level," Barron said.
"If you use a membrane with pores small enough to separate, they foul, and this renders the membrane useless.
"In our case, the superhydrophilic treatment results in an increased flux (flow) of water through the membrane and inhibits any hydrophobic material - such as oil - from passing through.
The difference in solubility of the contaminants thus works to allow for separation of molecules that should in theory pass through the membrane."
Barron and his colleagues used cysteic acid to modify the surface of an alumina-based ceramic membrane, making it superhydrophilic, or extremely attracted to water.
The superhydrophilic surface has a contact angle of 5 degrees.
(A contact angle of 0 degrees would be a puddle.)
The acid covered not only the surface but also the inside of the pores, and that kept particulates from sticking to them and fouling the filter.
In tests with fracking flowback or produced water that contained guar gum, the alumna membrane showed a slow initial decrease in flux -- a measure of the flow of mass through a material -- but it stabilized for the duration of lab tests.
Untreated membranes showed a dramatic decrease within 18 hours.
The researchers theorized the initial decrease in flow through the ceramics was due to purging of air from the pores, after which the superhydrophilic pores trapped the thin layer of water that prevented fouling.
"This membrane doesn't foul, so it lasts," Barron said.
"It requires lower operating pressures, so you need a smaller pump that consumes less electricity.
And that's all better for the environment."
###

Rice alumnus Samuel Maguire-Boyle is lead author of the paper.
Co-authors are Rice alumnus Joseph Huseman; graduate student Thomas Ainscough and Darren Oatley-Radcliffe, an associate professor, at Swansea University, Wales; and Abdullah Alabdulkarem, chairman of the Mechanical Engineering Department, and Sattam Fahad Al-Mojil, an assistant professor and environmental adviser, at King Saud University, Riyadh, Saudi Arabia.
Barron is the Charles W. Duncan Jr.-Welch Professor of Chemistry and a professor of materials science and nanoengineering at Rice and the Sr Cymru Chair of Low Carbon Energy and Environment at Swansea.
The research was supported by the Welsh Government Sr Cymru Program, FLEXIS, which is partially funded by the European Regional Development Fund, and the Robert A. Welch Foundation.
Read the abstract at http://www.
nature.
com/ articles/ s41598-017-12499-w

This news release can be found online at http://news.
edu/ 2017/ 09/ 25/ filter-may-be-a-match-for-fracking-water/

Follow Rice News and Media Relations via Twitter @RiceUNews

Related materials:

Barron Research Group: http://barron.
edu/ Barron.
html

Wiess School of Natural Sciences: http://naturalsciences.
A new technique that targets drugs to specific cells could lead to improved therapies for diseases caused by an overactive immune response.
The approach could help people affected by conditions such as arthritis and inflammatory bowel diseases, where the body's own immune system mistakenly attacks healthy tissues.
Researchers focused on a group of immune cells called macrophages - some of which help the body heal after injury, while others can promote harmful inflammation.
The team at the University of Edinburgh sought to devise a new therapy to remove harmful macrophages while leaving healing cells unaffected

They coupled a drug compound to a carrier molecule that only becomes active in acidic conditions, such as those found inside harmful macrophages.
A fluorescent tag attached to the molecules enabled the team to track the cells affected by the drug.
Lab tests on human macrophages showed the treatment preferentially affected inflammatory macrophages and did not affect healing cells.
Studies with zebrafish, which share features of their immune system with people, found the treatment helped to improve the recovery of tissues after injury.
The team hopes their approach could lead to more effective therapies, with fewer side effects, for the treatment of immune-related diseases.
Their research was published in the journal ACS Central Science.
Dr Marc Vendrell, of the Medical Research Council Centre for Inflammation Research at the University of Edinburgh, who led the study, said: "This is an important step forward in the design of more precise drugs with fewer side effects.
In future studies, we want to exploit this technology to improve the treatment of diseases in which macrophages and immune cells are important."
Calculations have shown that 10 per cent of all plastic produced around the world ultimately ends up in the oceans.
As a result, a large majority of global marine debris is in fact plastic waste.
Human production of plastics is a well-known environmental concern, but few studies have studied the effects of tiny plastic particles, known as nanoplastic particles.
"Our study is the first to show that nanosized plastic particles can accumulate in fish brains", says Tommy Cedervall, a chemistry researcher at Lund University.
The Lund University researchers studied how nanoplastics may be transported through different organisms in the aquatic ecosystem, i.e.
via algae and animal plankton to larger fish.
Tiny plastic particles in the water are eaten by animal plankton, which in turn are eaten by fish.
According to Cedervall, the study includes several interesting results on how plastic of different sizes affects aquatic organisms.
Most importantly, it provides evidence that nanoplastic particles can indeed cross the blood-brain barrier in fish and thus accumulate inside fish's brain tissue.
In addition, the researchers involved in the present study have demonstrated the occurrence of behavioural disorders in fish that are affected by nanoplastics.
They eat slower and explore their surroundings less.
The researchers believe that these behavioural changes may be linked to brain damage caused by the presence of nanoplastics in the brain.
Another result of the study is that animal plankton die when exposed to nanosized plastic particles, while larger plastic particles do not affect them.
Overall, these different effects of nanoplastics may have an impact on the ecosystem as a whole.
"It is important to study how plastics affect ecosystems and that nanoplastic particles likely have a more dangerous impact on aquatic ecosystems than larger pieces of plastics", says Tommy Cedervall.
However, he does not dare to draw the conclusion that plastic nanoparticles could accumulate in other tissues in fish and thus potentially be transmitted to humans through consumption.
"No, we are not aware of any such studies and are therefore very cautious about commenting on it", says Tommy Cedervall.
The present study was conducted in collaboration between the divisions of Biochemistry and structural biology, Aquatic ecology and Center for environmental and climate research at Lund University.
Researchers have found a way to chart changes in the speed of deep-ocean currents using the most modest of materials -- mud

Researchers have found a way to chart changes in the speed of deep-ocean currents using the most modest of materials - mud.
The approach, reported in the journal Deep-Sea Research Part I, could provide scientists with a better basis for understanding the behaviour of ancient ocean currents and, in an age of mounting apprehension over climate change, could help them to judge what level of fluctuation can be considered cause for concern.
Acting like giant conveyor belts, ocean currents transport water warmed by the sun's powerful rays over the equator towards the poles.
As the water cools and releases its warmth into the atmosphere, areas in the North and South benefit from the warm air.
In turn, currents regulate temperatures along the equator by offering an escape route for some of the heat.
The speed of ocean currents is hugely variable, but scientists are increasingly concerned that man-made climate change is altering their natural flow.
If rising sea temperatures and increased levels of fresh water from melting ice caps slow down currents, this could wreak havoc on global weather systems and impede the vital role they play in counteracting the uneven distribution of solar radiation that reaches the Earth's surface.
In order to fully understand what is happening to currents today and whether it is extraordinary, researchers need to build a picture of how they have behaved over time.
Modern current meters made from steel and plastic have only been widely used to track currents far beneath the surface since the 1960s, so to get a sense of how currents naturally fluctuate over long periods, scientists rely on proxies - such as changes over time in the natural radioactivity of particles.
Now, new research led by Professor Nick McCave, Fellow at St John's College and Emeritus Professor at the Department of Earth Sciences, University of Cambridge, has found a way to use the size of mud particles deposited on the ocean floor to measure changes in the speed at which ocean currents flow, offering another means for scientists to identify patterns in ancient current speeds.
Currents pick up and carry mud particles, dropping out larger grains as they slow down.
Over time, a record of the size of particles deposited on the ocean floor is built up in layers of sediment.
For the study, McCave visited various deep sea mud deposits near the east coast of the United States, Iceland and Portugal where there have been modern current meters in operation.
From research ships the researchers sent instruments down to depths of up to four kilometres beneath the water and extracted "cores", or samples of sediment, from the ocean floor.
The average rate of sedimentation in the world's oceans is about two to three centimetres per thousand years, but in the mounds of mud McCave was investigating up to 50 centimetres is deposited per thousand years, providing the researchers with a cross section of sediment layers with a much more clearly-defined picture of how strata of mud particles correspond with periods of time.
McCave obtained the records from the current meters and examined them for an average flow speed.
Then, from the cores, he took the top two centimetres of sediment and looked for tiny particles measuring over 10 microns, where one micron is equal to one millionth of a meter.
By comparing the size of the mud grains to the data from the current meters, McCave was able to calibrate how the size of mud particles relates to the current speed.
McCave said: "While the calibration was not precise enough to say what the exact current speed was during a specific year of history, it can give an accurate measurement of how much current speed has changed between two points in time - for example between an ice age and a warm period like the present.
That's about 20,000 years.
But the variability of Atlantic current flow since the early 1800s can also be tracked and shown to be closely related to temperature changes.
"Using mud as a current meter gives us another means to look at long-term trends and could result in improved computer modelling that better incorporates deep ocean flow.
We know that ocean current speeds can vary enormously, but having data that shows patterns going further back in time than the last 50 years could tell us what level of fluctuation should set off alarm bells."
When a patient arrives in any emergency department, one of the first steps in their care process is triage, an opportunity for a care team member to identify critically ill patients and assign priority treatment levels.
"With increases in annual visits to U.S. emergency departments, declines in capacity have led to unprecedented levels of crowding and consequential delays in care," says Scott Levin, Ph.D., associate professor of emergency medicine at the Johns Hopkins University School of Medicine.
"So what emergency departments have to do is very quickly assess whether a patient is in need of real critical, time-sensitive treatment versus a patient who is safe to wait."
Across the country, nurses and physicians typically use the emergency severity index (ESI) during triage to assign a score from Level 1 for patients who are the most critically sick, to Level 5 for patients who are the least sick.
A patient's ESI level determines in which area of the emergency department that patient will be seen, places the patient in a queue and influences provider decision-making throughout the patient's care process.
"This algorithm is completely subjective," Levin says.
"Nurses and physicians make a quick assessment on whether the patient can wait solely based on their clinical judgment."
In most cases, researchers say patients are assigned to a Level 3 and not entirely differentiated.
"We thought that Level 3 patient group included a large mix of patients who are pretty sick and others who weren't, and our goal was to determine whether these patients could be sorted out," Levin says.
To help differentiate patient triage levels, Levin and a team in the Department of Emergency Medicine developed an electronic triage tool.
In a recently published paper in the Annals of Emergency Medicine, the e-triage tool showed equal or improved identification of patient outcomes compared to ESI based on a multi-site retrospective study of nearly 173,000 emergency department visits.
The study showed significant differences in patient priority levels using e-triage and ESI.
For example, out of the more than 65 percent of visits triaged to ESI Level 3, e-triage identified about 10 percent, or more than 14,000, ESI Level 3 patients who may have benefitted from being up-triaged to a more critical priority level, such as Level 1 or 2.
These patients were at least five times more likely to experience a critical outcome, such as death, admission to the ICU or emergency surgery, and two times more likely to be admitted to the hospital.
The e-triage tool was also able to increase the number of patients down-triaged to a lower priority level, such as Level 4 or 5, to help minimize low-acuity patients from waiting and overusing scarce resources.
The e-triage tool uses an algorithm to predict patient outcomes based on a systems engineering approach and advanced machine learning methods to identify relationships between predictive data and patient outcomes.
"When a patient comes in, and we collect the patient's information, the e-triage tool is comparing that patient to hundreds of other like patients to make a prediction on the patient's outcome," Levin says.
These methods are common in other industries, such as defense, transportation and finance, but rarely, if ever, are implemented in health care.
"Machine-based learning takes full advantage of electronic health records and allows a precision of outcomes not previously realizable," says Gabor Kelen, M.D., director of the Department of Emergency Medicine and professor of emergency medicine at the Johns Hopkins University School of Medicine.
"It is the wave of future health care, although some providers may be hesitant.
Decision aids that take advantage of machine-learning are also highly customizable to meet the needs of an emergency department's patient population and local health care delivery systems."
The e-triage tool is also designed to be a decision support tool to help clinicians make better care decisions about their patients.
"The theory behind this tool, and all clinical decision support tools, is that the tool paired with the clinician can make better predictions or better prognostics tasks like this than the tool alone or the clinician alone," Levin says.
Better differentiating patients' priority levels, can, in turn, help patients get the appropriate care they need.
"The ultimate objective is patients should be waiting less in the emergency department," Levin says.
"For patients at risk of having a critical care need, this technology is designed to detect them better and make sure they are seen quicker.
For patients who are less sick, e-triage should detect those patients and put them on an expedited track, so they don't need to wait as long."
E-triage is currently being used at The Johns Hopkins Hospital and Howard County General Hospital, both member hospitals of Johns Hopkins Medicine.
The tool is continuing to be prospectively evaluated with preliminary results that suggest improvement in detection of patients with critical outcomes and other measures relating to emergency department crowding, according to researchers.
###

Other researchers involved in this study include Matthew Toerper, B.S.
; Eric Hamrock, M.B.A.; Jeremiah S. Hinson, M.D., Ph.D.; Sean Barnes, Ph.D.; Heather Gardner, R.N.
; Andrea Dugas, M.D., Ph.D.; Bob Linton, M.D.
; and Tom Kirsch, M.D., M.P.H.
Funding for this study was provided by the Agency for Healthcare Research and Quality (AHRQ) (grant R21HS023641) and the National Science Foundation (NSF) (grant SBIR 1621899).
Under a license agreement between StoCastic, LLC and The Johns Hopkins University, Levin, Dugas, Kelen and Toerper are entitled to royalty distributions on technology described in this publication.
Levin is also a founder and holds equity in StoCastic, LLC.
Toerper is a paid consultant to StoCastic, LLC.
Barnes is an employee of StoCastic, LLC.
These arrangements have been reviewed and approved by The Johns Hopkins University in accordance with its conflict of interest policies.
Rats that ate junk food during pregnancy had pups that preferred the taste of fat during childhood and had altered brain circuitry into adulthood

Researchers in France found that rats who ate a junk food diet during pregnancy had heavier pups that strongly preferred the taste of fat straight after weaning.
While a balanced diet in childhood seemed to reduce the pups' desire for fat, they nevertheless showed altered brain reward circuitry into adulthood.
The Western diet is full of energy-rich foods -- from hamburgers to chocolates, we consume significant quantities of fat and sugar.
The health costs of this are well known, and conditions such as obesity and diabetes are related to overeating.
Factors underlying obesity include how we metabolize food, and our tendency to overeat and seek out energy-rich foods.
The pleasure we derive from food stems from the brain reward circuitry, and changes in these reward circuits can contribute to overeating.
Surprisingly, pregnant or breastfeeding mothers who eat significant quantities of energy-rich foods can increase their child's risk for obesity in later life.
However, scientists don't yet fully understand the mechanism behind this phenomenon.
In a study recently published in Frontiers in Endocrinology, scientists used rats to investigate the relationship between a mother's diet and their offspring's weight, relationship with food, and brain circuitry.
The research team fed rats a high fat/high sugar diet (which they called the 'Western Diet'), or a balanced diet, during pregnancy and suckling.
They monitored the mothers' pups straight after weaning, during adolescence and into early adulthood.
The pups primarily ate a balanced diet once they were weaned, but at specific times the researchers allowed some of the pups to choose between tasting a fatty or non-fatty liquid.
The liquid wasn't fatty enough to affect the pups, but allowed the team to assess their preference for fat.
Using brain tissue samples, the team also investigated gene expression and brain changes associated with the pups' reward circuitry.
While the pups from Western Diet mothers were a normal weight at birth, they gained more weight during suckling and were abnormally heavy at weaning.
This may have been caused by the Western Diet mothers producing richer milk or more milk.
When the team allowed the just-weaned pups to choose between a fatty and non-fatty liquid, pups from Western Diet mothers strongly preferred the fatty liquid compared with pups from the balanced diet mothers.
However, when the team repeated this fat preference test with adolescent pups, they found that both groups showed a similar high preference for fat -- and interestingly, the pups from Western Diet mothers gradually lost their interest in fat after a few days.
This might have been a compensatory mechanism to protect the pups from further exposure to fat.
By adulthood, both types of pups had similar strong preferences for fat.
The pups from Western Diet mothers also showed significant changes in their reward circuitry, including differences in a brain region call the hypothalamus and changes in gene expression associated with a neurotransmitter called GABA.
"Previous studies have shown that when pups from Western Diet mothers have unlimited access to junk food they maintain their preference for fatty food into adolescence," says Vincent Paill, a researcher involved in the study.
"While the pups from Western Diet mothers in our study showed extensive changes in their reward circuitry, a balanced diet in childhood seemed to protect them from an increased fat preference at adolescence."
These findings could have implications for nutrition and obesity in human children in Western countries.
The team plan to further investigate the changes in reward circuitry caused by a maternal Western diet.
"How these altered reward circuits integrate information could be different, and these pups might behave differently under stress or when they have free access to fatty food," says Paill.
A group of UK scientists, co-ordinated by the University of Southampton, has published extensive research into how industry and environmental change are affecting our seafloors, but say more work is needed to help safeguard these complex ecosystems and the benefits they provide to people for the future.
Researchers from eight institutions and organisations have worked together to examine areas of sea or ocean located on the UK continental shelf to understand the sensitivity of these systems to human activities.
The societal importance of these ecosystems extends beyond food production to include biodiversity, carbon cycling and storage, waste disposal, nutrient cycling, recreation and renewable energy.
Martin Solan, lead principal investigator and Professor in Marine Ecology at the University of Southampton, comments: "Our seafloors are teaming with life, from microscopic organisms to larger creatures such as fish and crabs.
All interact as part of a complex system which plays a vital role in maintaining the health of the seabed and the rest of food web.
"Human intervention, such as fishing, pollution and activities causing climate change are all affecting these finely balanced ecosystems.
Collectively, our research provides us with a new perspective on how the seafloor is being modified, for better or for worse - but more research is now needed to understand the longer-term consequences of such change for the wider environment and for society at large."
The research team has analysed the biodiversity, nutrient, metal and carbon cycling in areas of the seafloor around the UK subject to different environmental conditions and human use.
University of Southampton scientists focused on four main areas: the effects of climate change1, assessing the impact of bottom fishing2, understanding the importance of iron cycling across the seafloor4, and optimising areas for study3.
In relation to climate and fishing, the researchers conducted two experiments.
One took sediment communities from different areas in the Irish Sea experiencing low, medium and high levels of trawling activity and compared each sample.
Another took different types of sediment (mud, sand, sandy mud etc) from the Celtic Sea and simulated, in a laboratory, how future climatic conditions will affect important seabed processes.
The researchers found that, in a variety of complex ways, communities adjusted to their new environments, with some species thriving and some taking up new roles.
However, some species failed to adjust to the new conditions and could not survive.
In particular, the scientists found that some species were able to withstand fishing pressure, but struggled with warmer climate conditions and raised CO2 levels.
Dr Phil Williamson, from the University of East Anglia, who helped coordinate this research programme, commented: "Much of what happens in the sea is out of sight and out of mind.
This study has provided a wealth of insights into the natural recycling processes that are literally at the base of marine ecosystems, underpinning the many benefits that we obtain from the sea."
The research is part of a special issue of the scientific journal Biogeochemistry and includes contributions from the University of Southampton, the Centre for Environment, Fisheries and Aquaculture Science (CEFAS), the National Oceanography Centre (NOC), University of Portsmouth, University of Oxford, Bangor University, Plymouth Marine Laboratory and The Scottish Association for Marine Science (SAMS).
The variety of projects which the Biogeochemistry special issue brings together, were conducted on three dedicated research cruises and other expeditions around the UK.
The Shelf Sea Biogeochemistry Special Issue can be found at: https:/ / link.
springer.
com/ journal/ 10533/ 135/ 1/ page/ 1

The research was part of the Shelf Sea Biogeochemistry programme, investigating how natural and human processes interact in the seas around the UK.
###

Notes to editors

1) The papers forming the Biogeochemistry special issue are:

1Vulnerability of macronutirents to the concurrent effects of enhanced temperature and atmospheric pCO2 in representative shelf sea sediment habitats

Lead author and institute: Jasmin Godbold, University of Southampton

2Mediation of macronutrients and carbon by post-disturbance shelf sea sediment communities

Lead author: Rachel Hale, University of Southampton

3An approach for the identification of exemplar sites for scaling up targeted field observations of benthic biogeochemistry in heterogeneous environments

Lead author and institute: Charlotte Thompson, University of Southampton

4Stability of dissolved and soluble Fe(II) in shelf sediment pore waters and release to an oxic water column

Lead author and institute: Jessica Klar, University of Southampton

5Community mediation on shelf-sea benthic nitrogen cycling following bottom trawling and organic enrichment.
Lead author and institute: Marija Sciberras, Bangor University

6Benthic pH gradients across a range of shelf sea sediment types linked to sediment characteristics and seasonal variability

Lead author and institute: Briony Silburn, Centre for Environment, Fisheries and Aquaculture Science (CEFAS)

7Comparison of the ERSEM model with benthic biogeochemical measurements at two sites in the Celtic Sea and including a representation of advective pore water flow

Lead Author and institute: John Aldridge CEFAS

8Predicting the standing stock of organic carbon in surface sediments of the North-West European continental shelf

Lead author and institute: Markus Diesing, CEFAS

9Seasonal benthic nitrogen cycling in a temperate shelf sea; the Celtic Sea

Lead author and institution: Vas Kitidis, Plymouth Marine Laboratory (PML)

10Oxygen dynamics in shelf seas sediments incorporating seasonal variability

Lead author and institute: Natalie Hicks, Scottish Association for Marine Science (SAMS)

2) For further images or interviews please contact Peter Franklin, Media Relations, University of Southampton.
Tel: 023 80 59 5457 Email: p.franklin@southampton.ac.uk

3) For more on partner institutions and organisations:

Bangor University

https:/ / www.
bangor.
uk/

Centre for Environment, Fisheries and Aquaculture Science (CEFAS)

https:/ / www.
cefas.
co. uk/

Plymouth Marine Laboratory

http://www.
uk/

Scottish Association for Marine Science (SAMS)

http://www.
uk/

The National Oceanography Centre Southampton

https:/ / noc.
uk/

The University of Portsmouth

http://www.
uk/

The University of Oxford

http://www.
uk/

4) The University of Southampton drives original thinking, turns knowledge into action and impact, and creates solutions to the world's challenges.
We are among the top one per cent of institutions globally.
Our academics are leaders in their fields, forging links with high-profile international businesses and organisations, and inspiring a 24,000 strong community of exceptional students, from over 135 countries worldwide.
Through our high-quality education, the University helps students on a journey of discovery to realise their potential and join our global network of over 200,000 alumni.
http://www.
southampton.
Cancer vaccines may need to better target T cells that can hold up to the long fight against cancer, scientists report.
AUGUSTA, Ga. (Sept. 25, 2017) - Cancer vaccines may need to better target T cells that can hold up to the long fight against cancer, scientists report.
Studies of two T cell types that are equally activated by alpha-fetoprotein, a well-established antigen made by liver cancer, show that while one starts off with a bang, the other endures as the more powerful tumor fighter.
"You poke one and it runs.
The other you must give a big push," says Dr. Yukai He, immunologist at the Georgia Cancer Center and Department of Medicine at the Medical College of Georgia at Augusta University.
He, a Georgia Research Alliance Distinguished Investigator, is corresponding author of the study in the journal Cancer Immunology Research.
T cells are the frontline of the immune system.
Most typically, dendritic cells, the most powerful antigen presenters, show a suspicious item, like alpha-fetoprotein in liver cancer, to receptors on T cells, which starts an education process that prompts the immune system to attack.
While the high sensitivity and strong initial response to alpha-fetoprotein shown by the T cell Tet???
sound good, the scientists also found the cell quickly became exhausted from the prolonged immersion in the antigen, and even committed suicide.
"Basically with a tumor it's like jumping into a pool of antigen.
If you are very sensitive to the antigen, you are going to make yourself overreactive and exhausted," he says.
On the other hand, it took more antigen to get the attention of Tet??
?, but this T cell stayed on target, generating a stronger, longer antitumor effect.
Tet???
also contained more stem-like memory T cells, which basically meant they could perpetuate themselves.
"It's a self-replicating army," says He, and only about 1-2 percent of T cells have this ability.
In their animal studies, the team also found that the receptors on Tet???
had weaker signals, which is probably why it required more antigen to get its attention, the scientists write.
But it's also apparently what helped Tet???
avoid the excessive signaling, exhaustion and death of its colleague Tet???
in the long battle against a tumor.
That kind of vigorous response and persistent stimulation followed by burnout and death of T cells has been noted in chronic infections, they write.
Since many acute infections, like the influenza virus, are short-lived compared to a tumor, Tet???
's approach appears effective in those situations, but not so great in chronic infections like HIV, or tumors, He says.
There is still much to learn about exactly why the two T-cell types respond so differently, whether it's how their receptors respond initially, internal signaling differences in the two, both or something else, He notes.
But He suspects that we all have both T-cell types in our repertoire.
One of his many goals is to develop a vaccine that would increase our level of Tet???
and/or Tet???
-like cells or at least get Tet???
and other similar cells to function more like the persistent Tet???.
"We have to find a way in order for cancer vaccines to succeed," he says.
Vaccines are under study for a variety of cancers including breast, brain, lung and pancreatic, according to the American Society of Clinical Oncology.
Two cancer prevention vaccines are already on the market, for human papillomavirus, the major cause of cervical cancer, and for hepatitis b, a cause of liver cancer; as well as one treatment vaccine for metastatic prostate cancer, according to the National Cancer Institute.
Problems with treatment vaccines include the fact that cancer suppresses the immune system while vaccines are trying to bolster and target it; and sick and/or older patients typically already have generally weaker immune systems.
Also, as He says, cancer cells are derived from the individual's own cells, which is why cancer cells are often good at avoiding even the natural immune response.
"Sometimes the mutation is so subtle, that the immune system does not see it as a threat," he says.
"Our current study points out that our immune system needs to be trained just right so that the immune fighters can persist in the malicious tumor environment and win the war," He says.
He's research team members postdoctoral fellow Dr. Sha Wu and research assistant Wei Zhu, are co-first authors.
The NCI funded the research.
While most cancer incidence rates are declining, liver cancer incidence has more than tripled in the United States since 1980 and liver cancer death rates have increased by almost 3 percent per year since 2000, according to the American Cancer Society.
###

Alpha-fetoprotein is made by the yolk sac and the liver during human development, and levels are measured during pregnancy as an indicator of birth defects.
Researchers from Instituto de Medicina Molecular (iMM) Lisboa have found that the host's susceptibility to develop malaria depends on his or her metabolic state, which can be easily manipulated through external stimuli such as dietary patterns.
The progression and development of an infectious disease is directly dependent not only on the characteristics of the causing infectious agent but also on the genetic characteristics of the host, which also dictate the efficiency of the infection.
During the last years scientific discoveries have suggested that external factors independent of the host-parasite dichotomy, such as eating habits, can impact in the establishment, progression and endpoint of infections.
The team, led by Maria Mota, manipulated the diet fed to lab mice for very short periods of time and evaluated the level of infection caused by the malaria parasite.
The results, now published in the prestigious journal Nature Microbiology, show that an increase in the levels of pro-oxidants caused by dietary shifts leads to a 90% reduction in parasite load during the hepatic phase of the infection and consequently lowers the severity of the disease.
The mechanism used by the host to eliminate the malaria parasite, now revealed in this study, may contribute to explain how certain genetic alterations associate to high levels of oxidative stress, such as sickle-cell anaemia or beta thalassemia, have been selected in the population due to their protective effect against malaria.
###

iMM Lisboa

The Instituto de Medicina Molecular in Lisbon (Portugal) is a private non-profit research institute that offers a vibrant scientific environment where world-class ingenious scientists with an ambitious research portfolio are supported by state-of-the-art technology, aiming to maximize creativity towards discoveries without boundaries.
In spite of iMM's young age (created in 2002), several iMM findings, spanning from basic to translational research, are already being applied to improve human health.
imm.medicina.ulisboa.pt/en/
The CNIC research team has devised a novel image analysis tool that allows groups of pluripotent cells to be tracked according to the level of Myc in each cell

A sophisticated system of 'social control' operating between neighboring cells allows embryos to protect the purity of their pluripotent cell population, which is able to generate all body tissues.
This system works through the elimination of cells that begin to differentiate prematurely, in a process mediated through 'cell competition' based on the expression levels of the gene Myc.
This control system is important for pluripotency during the development of mammalian embryos, and is described for the first time in Developmental Cell by researchers at the Centro Nacional de Investigaciones Cardiovasculares (CNIC).
When mammalian embryos contain no more than a few dozen cells, some of these cells have the capacity to differentiate into all the cell types of the organism.
This capacity is known as pluripotency, and study leader Dr. Miguel Torres explained that at these early stages the embryo must conserve and expand the pluripotent cell population over a period of days in order to produce several thousand pluripotent cells, enough to generate a new individual organism.
During this process, cells progress from a naive state (more pluripotent) to a primed state, in which they have a greater propensity to differentiate.
Until now, scientists have had no way to explain why this expansion in the numbers of pluripotent cells is accompanied by the spontaneous death of large numbers of these same cells.
What was known is that neighboring pluripotent cells compare their levels of a factor called Myc, which controls cells' ability to grow and proliferate.
Cells with less Myc than their neighbors die through a process known as cell competition; however, it was unknown why the pluripotent cell population in the embryo contains cells with different levels of Myc.
For the new study, a multidisciplinary team at the CNIC used pluripotent cells in which the levels of Myc are revealed by a fluorescent signal.
The team also developed a novel image analysis tool that allowed them to film groups of live pluripotent cells.
By tracking the level of Myc in these populations, the scientists were able to observe which cells become 'losers' in cell competition and die, and which ones become 'winners' and survive.
The fluorescent signal also allowed the team to isolate cells with high or low Myc levels and study their characteristics.
As Dr. Torres explained, "the study shows that the level of Myc reveals the differentiation state of the cell; cells with more Myc are more naive, in other words more pluripotent, whereas those with less Myc are more primed, or more differentiated."
What this discovery implies is that the pluripotent stem cell population is not pure, due to the appearance of primed cells with lower levels of Myc.
Interestingly, when the cells divide, the daughter cells show a very strong tendency to conserve the Myc level and the differentiation state of the mother cell.
So when the research team blocked cell competition, they found that differentiated cells accumulated in the pluripotent stem cell population.
In other words, the pluripotent stem cell population became less pure when primed cells could not be eliminated through cell competition.
The scientists also made the following fascinating discovery.
Once the embryo has accumulated enough pluripotent cells to generate a new individual, these cells need to start differentiating.
The research team found that at this stage entire cell populations reduce their levels of Myc simultaneously, thus avoiding death by cell competition and initiating a coordinated process of differentiation.
The conclusions reached by first author Covadonga Daz-Daz and colleagues establish that cell competition based on differential Myc levels among neighboring cells acts as 'social control' system, through which embryos protect the purity of the pluripotent stem cell population by eliminating cells that begin to differentiate prematurely.
According to the researchers, these results also reveal the yin and yang action of genes such as Myc.
"The ability of Myc to regulate cell growth and proliferation is essential for embryonic development, but this same capacity means that excessive Myc expression in the cells of adults is one of the main causes of cancer."
Open-ended laboratory tests for cyclists could help athletes train better

Scientists at the University of Kent's School of Sport and Exercise Sciences have discovered that cyclists can perform better when they do not have to pace their efforts.
Using 17 experienced male cyclists in a series of tests, they compared open-ended Time-To-Exhaustion (TTE) trials that are often used in laboratories with race-like Time-Trials to measure endurance performance.
All of the cyclists were blinded to elapsed time, power output, cadence and heart rate.
In an article published in the Journal of Sports Sciences Professor Louis Passfield and Dr Sarah Coakley say that they were surprised to discover that, despite being experienced, the cyclists could not pace their effort effectively in the Time-Trial without.
They expect that their findings will help to better understand the role of feedback in how people choose when to stop exercising because they imply we are not good at keeping track of time when we exercise hard.
For the study the cyclists were told to maintain their target power for as long as possible and were given verbal encouragement to continue as long as possible.
Their performance was compared with separate Time-Trials where they had to "race" for a set period of time.
They were not told during any of the trials how much time had passed.
The cyclists performed better in the open-ended TTE than in the trials where they knew beforehand how long they had to "race".
This finding has implications for how athletes plan their training as these types of tests are frequently used for this purpose, as well as to predict their race performances.
Previously TTE tests were considered by many as less useful but the research by Professor Passfield and Dr Coakley indicates TTE should still be regarded as a useful measure of performance in the laboratory.
###

Cycling Performance is superior for time-to-exhaustion versus time-trial in endurance laboratory tests - Journal of Sports Sciences - Sarah L Coakley and Louis Passfield.
In order to get rid of unpleasant competitors, some bacteria use a sophisticated weapon - a nanosized speargun.
Researchers at the University of Basel's Biozentrum have now gained new insights into the construction, mode of action and recycling of this weapon.
As they report in the journal Nature Microbiology, the speargun drills a hole into the neighboring cells in only a few thousandths of a second and injects a cocktail of toxins.
Millions of tiny microbes on leaves, stones or our skin jostle for space.
And almost everywhere they have to compete for resources and nutrients.
In the course of evolution, some bacteria have therefore developed a weapon to inject a toxic cocktail into competitors and rivals in their neighborhood, thus eliminating them.
Among experts, this weapon resembling a speargun is also known as the type VI secretion system (T6SS).
Two years ago, Prof. Marek Basler was able to elucidate the atomic structure of the speargun in the "post-firing" state.
In the current study, which was carried out in cooperation with various research groups and technology platforms at the Biozentrum, his team has now solved the structure of the "ready to fire" speargun.
Based on these findings, the researchers have been able to model how this T6SS speargun works.
Structure of nanosized speargun changes during firing

The speargun is composed of various components, including a sheath and a spear with a sharp tip.
The sheath consists of over 200 connected cogwheel-like protein rings that are assembled around the inner rigid spear.
When T6SS fires, the sheath rapidly contracts and pushes the toxic spear out of the cell, which can then penetrate into neighboring cells where it releases deadly toxins.
"So far, there have only been assumptions as to how the structure of the T6SS sheath changes during contraction," says Basler.
"Using cryo-electron microscopy available at C-CINA, we have now obtained an image of the spear and the extended sheath in atomic resolution."
By comparing the structures of the extended and contracted states, the researchers were able to model how the T6SS works in detail.
"During the sheath contraction, ring after ring turns and gets closer to the previous ring, while the ring diameter expands and thus releases the spear," explains Basler.
"This combination of sheath shrinking and turning results in drilling a hole into the target cells.
Within less than two milliseconds, the T6SS sheath contracts to half of its length and at the same time the toxic spear spirals out like a screw.
Therefore, the bacteria have an extremely powerful drill."
Only contracted T6SS sheaths are disassembled

Furthermore, the researchers also addressed another question.
After firing T6SS, bacteria re-use the individual components of the sheath to assemble a new speargun.
"For a long time, it was not clear why only the contracted, but not the extended sheath is disassembled," says Basler.
"Now, we could see that a certain protein domain is exposed on the surface of the sheath during contraction and can be recognized by a specific protein responsible for dismantling the sheath.
In the extended sheath state, this domain is hidden and the T6SS sheath is therefore protected from disassembly."
The bacterial speargun will continue to be the subject of future research.
"One of our projects is dedicated to the question of how the T6SS is embedded in the bacterial cell envelope.
As the speargun is fired with such a high force, it must be firmly anchored, otherwise firing would not work properly or could be also fatal for the weapon-carrying bacteria themselves."
A study published in the journal Nature Neuroscience, from Nature publishing group, describes the identification of a novel molecular pathway that can constitute a therapeutic target for cognitive defects in Parkinson's disease.
The study coordinated by Portuguese researchers from the Instituto de Medicina Molecular (iMM Lisboa), the University Medical Center Goettingen, Germany and CEDOC - Nova Medical School Lisbon, along with other colleagues from Germany, showed that abnormal forms of Parkinson's disease (PD)-associated protein alpha-synuclein interact with the prion protein (PrP), triggering a cascade of events that culminates in neuronal dysfunction, causing cognitive defects that are reminiscent of those in PD.
"This is the follow up of a previous study initiated in my laboratory, where we found that particular forms of the protein alpha-synuclein cause dysfunction of neuronal circuits involved in memory formation.
We did not know how this was happening, and in this new study we have detailed the molecular mechanisms involved, which suggests we now have new targets for therapeutic intervention" - explains Tiago Outeiro, a former Group leader of iMM now in Germany and at CEDOC, who coordinated the study together with Lusa Lopes, a Group Leader at iMM.
Using pharmacology and genetics, the team has now defined a series of molecular events that explains the memory defects observed in animals that model some important aspects of PD.
Lusa Lopes adds: "We used a mouse model of PD in which human alpha-synuclein is produced and found that by blocking this interaction with PrP using a caffeine analogue, reverted the abnormal neuronal activity and memory deficits.
This study links nicely with our previous work on Alzheimer's disease, further suggesting that molecules like caffeine may indeed have potential benefits against memory deficits upon neurodegeneration".
Parkinson's disease is a devastating disorder affecting millions of people worldwide.
Current therapies are only symptomatic, and treat only some of the motor symptoms of the disease.
"We now know that PD is much more than just a motor disease, and there is a great demand for novel therapies, especially those capable of modulating disease progression or, ideally, capable of preventing the onset of the disease" - explains Tiago Outeiro.
"We are very excited with the findings of our collaboration, and this study demonstrates that when we pull together our complementary expertise we can make important discoveries that can impact the lives of the millions of people (patients and families) affected by these terrible disorders" - concludes Lusa Lopes.
###

Original Publication: Diana G. Ferreira, Mariana Temido-Ferreira, Hugo Vicente Miranda, Vnia L. Batalha, Joana E. Coelho, va M. Szeg, Ins Marques-Morgado, Sandra H. Vaz, Jeong Seop Rhee, Matthias Schmitz, Inga Zerr, Lusa V. Lopes* and Tiago F. Outeiro* (2017) -Synuclein interacts with PrPC to induce cognitive impairment through mGluR5 and NMDAR2B.
Nature Neuroscience doi: 10.1038/nn.4648

About Tiago Fleming Outeiro

Tiago Fleming Outeiro is full Professor and Director of the Department of Experimental Neurodegeneration at the University Medical Center Goettingen.
Tiago graduated in Biochemistry at the University of Porto in 1998.
He did his PhD thesis at the Whitehead Institute for Biomedical research - MIT, in the USA, and then was a Postdoctoral Research Fellow in the Department of Neurology of the Massachusetts General Hospital - Harvard Medical School where he focused on the study of Neurodegenerative disorders such as Parkinson's and Alzheimer's disease.
Tiago directed the Cell and Molecular Neuroscience Unit at IMM, Lisbon, from 2007 to 2014, and has been Full Professor and the Director of the Department of Experimental Neurodegeneration at the University Medical Center Goettingen, in Germany since October 2010.
He is also currently leading the Cell and Molecular Neuroscience Group at CEDOC, Nova Medical School, in Lisbon.
Tiago is a world leader in the field of Parkinson's and Alzheimer's disease, and has authored >160 research articles in the field of Parkinson's and Alzheimer's disease.
He participates in various international collaborative projects with the aim of identifying the molecular basis of neurodegenerative disorders such as Alzheimer's and Parkinson's disease.
He has received multiple awards and grants in Germany, from the European Union, and from other leading international funding agencies.
About Lusa Lopes

Lusa V. Lopes is a Group leader@iMM Lisboa, Portugal since 2013 https:/ / imm.
medicina.
ulisboa.
pt/ en/ investigacao/ labs/ lopes-lab/ .
Lusa graduated in Lisbon in 1998 in Biochemistry and then pursued a PhD in Neurosciences being trained in the University of Cambridge, UK and at the Karolinska Institute, Sweden, followed by a postdoc at Nestl Research Center in Lausanne, Switzerland.
Her work focuses on understanding the mechanisms inducing the "early-aging" of the cognitive function.
The team has provided evidence for an important contribution of adenosine receptors in pathophysiological context, and their impact in noxious brain conditions such as stress, aging and neurodegeneration.
Her team provided crucial evidence of a possible glucocorticoid-adenosine link in Alzheimer's disease (with D. Blum) following previous groundwork suggesting circadian disorders as a trigger for accelerated cognitive loss.
About iMM Lisboa

Instituto de Medicina Molecular (iMM Lisboa) is a reference biomedical research centre in Portugal, having acquired the special status of Associate Laboratory of the Portuguese Ministry of Science and Technology.
iMM's mission is to promote basic, translational and clinical biomedical research, with the aim of understanding the mechanisms of disease and developing new therapeutic approaches.
For more information please visit the iMM Lisboa's website: http://www.
medicina.
ulisboa.
pt

About CEDOC
Leaves are fundamental light-capturing organs of plants.
The vast majority of higher plants utilize leaves as their solar panels to harvest solar energy.
A common feature of leaves is their flat blades.
Scientists from the Institute of Genetics and Developmental Biology in Beijing discovered that the classical phytohormone auxin enables leaf blade expansion and leaf flattening.
The flattening of leaves to form broad blades is an important adaptation that maximizes photosynthesis.
However, the molecular mechanism underlying this process remains unclear.
A new research led by JIAO Yuling from the Institute of Genetics and Developmental Biology of the Chinese Academy of Sciences (CAS), shows that spatial auxin signaling defines the expression of two redundant genes WOX1 and PRS, which enables leaf blade expansion and flattening.
Following their previous report on the auxin regulation of leaf polarity patterning, the researchers further found that auxin and auxin response factors (ARFs) have limited overlaps, which refines auxin signaling in the middle domain of leaf primordium.
Furthermore, they found that MP/ARF5, an ARF activator directly activates the expression of WOX1 and PRS, which promote the marginal meristem and enable leaf flattening.
On the other hand, ARF repressors expressed in the abaxial (ventral) domain inhibit WOX1 and PRS expression.
"The new findings in this work explain how adaxial-abaxial (dorsal-ventral) polarity patterns the mediolateral axis and subsequent lateral expansion of leaves", said Dr. JIAO Yuling.
He also mentioned that other recent research of their group described auxin regulation of leaf development at the biomechanical level.
"Finding how leaves get flattened will be necessary to maintain and enhance yield in cultivated plants and crops" said JIAO.
This study entitled "Spatial auxin signaling controls leaf flattening in Arabidopsis" has been published online in Current Biology.
###

This study was supported by the National Basic Research Program of China (973 Program), the National Natural Science Foundation of China, and the State Key Laboratory of Plant Genomics.
Once you've bought your first organic milk in the supermarket, you are highly likely to continue buying organic milk.
With time, you are also likely to increase the number of different types of organic food products on your shopping list.
This is the result of new research from Aarhus BSS in which researchers have monitored the daily shopping habits of almost 10,000 households over a period of 20 months and have subsequently analysed the huge amount of data.
The organic escalator

"In connection with organic consumption, there has previously been talk of an 'organic staircase' in the sense that consumers are generally buying certain organic products before others.
But our research shows that in fact we're dealing with an escalator where the upwards movement is taking place automatically.
Once you've purchased your first organic product, you're not likely to stop.
You'll continue and over time, you'll increase your organic shopping list.
And you'll even be following a rather predictable consumption pattern," says Professor John Thgersen, the Department of Management at Aarhus BSS.
He conducted the study together with Professor in Economics Hans Jrn Juhl and PhD student Morten H.J.
Fenger from the Department of Economics and Business Economics.
The study has recently been published in the internationally recognised Journal of Consumer Research.
In Denmark, milk represents the typical entry into organic consumption.
But milk is not the organic product with the greatest market share.
Oatmeal is.
However, in terms of volume, milk is by far the main organic product.
Fixed pattern

Once you have started buying organic dairy products, you have paved the way for adding more organic products to your shopping list.
The typical consumption pattern is that you go from dairy products to vegetables, eggs, baking ingredients until you are consistently buying organic products.
"What's interesting is that something is making the organic consumers stick to their guns.
Something is making them stand fast.
Our study doesn't tell us anything about why this is the case, but if we include our knowledge from previous research in the area, we're able to make an educated guess," says John Thgersen, who studies and teaches consumer behaviour.
According to the behavioural scientist, buying organic products is connected to our perception of ourselves as moral human beings.
Once we have established that connection, there is no way back.
Moral values

Previous studies have thus shown that sustainability and environmental protection are some of the positive aspects that most consumers associate with organic products.
"It becomes a way in which we define ourselves.
As a result, we build an identity around the notion of buying organic products, and we're highly unlikely to suddenly change our moral values," says John Thgersen.
Within behavioural research, this phenomenon is known as a behavioural "spillover" effect, which stands in contrast to the well-known phenomenon of "moral licensing".
The latter deals with how we as people allow ourselves to become less moral, once we have performed one moral act.
"We're not questioning that moral licensing exists, but our study shows that in practise, it doesn't apply to our ethical or pro-environmental behaviour.
Once a person has decided to do good, he or she will actually be even more likely to continue to do good," says John Thgersen.
Speeding up the escalator

The Danish supermarket giant Coop Danmark provided the data for the Aarhus BSS researchers and Head of CSR Thomas Roland is thrilled with the results.
Here, the results can be used to target the marketing of organic products to specific customer segments.
This is a way of speeding up the organic escalator.
"In reality, we can use a number of different staircases or escalators.
Based on the researchers' analyses, we have been able to establish a number of organic consumer segments.
We conducted a test in which we exposed each of the segments to specific marketing messages.
This has increased the organic consumption significantly in these customer groups," explains Thomas Roland.
He sees great opportunities in analysing customers' actual purchasing patterns rather than relying on more traditional consumer analyses that are often based on attitude measurement.
"This analysis provides us with a mapping of the level and segments that consumers belong to right now.
This is achieved by taking point of departure in the actual shopping patterns and then targeting our marketing accordingly.
It could also be interesting to explore whether the same staircases and escalators apply to unbranded products e.g.
local high-quality products," says Thomas Roland.
SAN DIEGO, September 25, 2017 -- A new survey finds breast cancer patients' actual radiation therapy experiences largely exceeded their expectations.
The survey, which addressed the fears and misconceptions regarding radiation therapy for breast cancer, found that more than three-fourths of the breast cancer patients surveyed found their experiences with radiation therapy, including overall and specific long-term and short-term side effects, to be less "scary" than anticipated, according to research presented today at the 59th Annual Meeting of the American Society for Radiation Oncology (ASTRO).
"Radiation oncologists know firsthand that our patients come in with fears and sometimes misconceptions.
Unlike many other treatments and fields of medicine, it is very hard to imagine what radiation therapy is like," said Narek Shaverdian, MD, lead author of the study and a radiation oncology resident at the David Geffen School of Medicine at the University of California, Los Angeles (UCLA).
"Still, it is surprising to find that upwards of 90 percent of women surveyed agree that if future patients knew the reality of the radiation therapy experience, they would be less afraid of treatment.
"Advances in radiation therapy technologies over the past several decades and the increased use of hypofractionation--where radiation is given in larger doses across fewer sessions--have afforded patients more convenient treatment options, as well as lower toxicity rates in many situations," said Dr. Shaverdian.
"Our study shows that women who received modern breast radiation therapy overwhelmingly found the treatment experience far better than expected.
The negative stories out there are frightening and pervasive, but they generally are not reflective of the actual experience," said Susan McCloskey, MD, MSHS, assistant professor at the David Geffen School of Medicine at UCLA, Director of the Breast Service at UCLA Radiation Oncology and senior author of the study.
Surveys were sent to all patients who received treatment for breast cancer at a UCLA-affiliated multidisciplinary breast cancer clinic between 2012 and 2016.
Eligible patients had six or more months of follow-up and were without tumor recurrence.
Sixty-five percent of these 502 patients returned surveys, and study findings are based on these 327 responses.
The median age of survey respondents was 59 years (range 28-89 years).
Patients represented various disease stages; 18 percent had stage 0 breast cancer; 38 percent stage I; 34 percent stage II; and 9 percent stage III.
Eighty two percent underwent breast conserving surgery, 13% had axillary dissection, 37% received chemotherapy, and 70% received endocrine therapy.
All patients received radiation therapy (RT), delivered as either standard whole-breast RT with or without regional nodal coverage, hypofractionated whole-breast RT, post-mastectomy RT, or partial breast RT.
Patients completed the survey a median of 31 months (range 6-61 months) after completing radiation therapy.
Survey questions assessed fears and beliefs about breast cancer treatment and side effects, as well as how the actual experience compared to initial expectations.
Specifically, patients were asked if the treatment experience, short-term side effects and long-term side effects were as expected, worse than expected or better than expected.
Nine in ten patients (90%) found the actual experience of breast radiation therapy to be "less scary" than anticipated.
Overall short-term and long-term side effects of radiation were better than expected or as expected for 83 percent and 84 percent of respondents, respectively.
Patients also reported that side effects were less severe than or as expected for short-term breast pain (75%), skin changes (61%) and fatigue (78%), as well as for long-term appearance changes (85%), breast pain (79%), breast size changes (73%) and breast textural changes (70%).
More than two-thirds (68%) of these breast cancer patients reported that they had little to no prior knowledge of radiation therapy at the time of their diagnosis, yet nearly half (47%) also shared that they had previously read or heard "frightening" stories of serious side effects from radiation therapy.
Nearly all women surveyed (94%) responded that they were initially fearful of receiving radiation therapy.
The most common initial fears related to radiation therapy were concerns about damage to internal organs (40%), skin burning (24%) and becoming radioactive (7%).
Very few patients found confirmation for these negative stories during treatment, however; among 327 respondents, eight women (3%) found the negative stories they previously read about radiation therapy to be true and six women (2%) found the negative stories they heard from family and friends to be true.
The trend of finding negative stories to be largely untrue was even more pronounced among patients who underwent breast conservation therapy.
Nine in ten survey respondents agreed that "After treatment, I now realize that radiation therapy is not as bad as they say it is," and/or that "If future patients knew the 'real truth' about radiation therapy, they would be less scared about treatment."
"We hope that these data, which reflect the voices of past breast cancer patients, can help to counsel future patients and their physicians on the actualities of the modern breast radiation therapy experience," said Dr. Shaverdian.
"Patients who have received this treatment provide the most credible account of its actual impact, and their accounts show that outdated, negative stereotypes of breast radiation are almost universally found to be untrue."
###

The abstract, "The patient's perspective on breast radiation therapy: initial fears and expectations versus reality," will be presented in detail during a news briefing and an oral abstract session at ASTRO's 59th Annual Meeting in San Diego (full details below).
To schedule an interview with Dr. Shaverdian and/or outside experts in breast cancer, contact ASTRO's media relations team on-site at the San Diego Convention Center, September 24 through 27, by phone at 703-286-1600 or by email at press@astro.org.
ATTRIBUTION TO THE AMERICAN SOCIETY OF RADIATION ONCOLOGY (ASTRO) ANNUAL MEETING REQUESTED IN ALL COVERAGE.
This news release contains additional and/or updated information from the study author(s).
Full original abstract and author disclosures available from press@astro.org or at http://www.
astro.
org/ annualmeeting .
Study Presentation Details

Scientific Session: Monday, September 25, 7:45 - 9:00 a.m. Pacific time, San Diego Convention Center, room 5B

News Briefing: Tuesday, September 26, 1:00 - 2:00 p.m. Pacific time, San Diego Convention Center, room 24C, webcast: http://www.
do/ astro17-3

Resources on Breast Cancer and Radiation Therapy

Videos: Radiation Therapy for Breast Cancer (Spanish version), An Introduction to Radiation Therapy (Spanish version)

Digital brochure: Radiation Therapy for Breast Cancer (Spanish version)

Additional brochures, videos and information on radiation therapy from ASTRO's patient site, RTAnswers.org

ASTRO's clinical practice statements and guidelines

ABOUT ASTRO'S ANNUAL MEETING

ASTRO's 59th Annual Meeting, the world's largest scientific meeting in radiation oncology, will be held September 24-27, 2017, at the San Diego Convention Center.
The 2017 Annual Meeting is expected to attract more than 11,000 attendees from across the globe, including oncologists from all disciplines and members of the entire radiation oncology team.
More than 2,800 abstracts sharing results from clinical trials and other research studies will be presented in conjunction with educational sessions and keynote addresses that underscore the meeting's theme, "The Healing Art and Science of Radiation Oncology."
Led by ASTRO President Brian Kavanagh, MD, MPH, FASTRO, the 2017 meeting will feature keynote addresses from Richard D. Zane, MD, FAAEM, Chief Innovation Officer for the University of Colorado Health System; Lucy Kalanithi, MD, FACP, widow of Paul Kalanithi, MD, the best-selling author of "When Breath Becomes Air," with Heather Wakelee, MD, Paul's oncologist; and Vinay K. Prasad, MD, MPH, an assistant professor of medicine at the Oregon Health & Science University.
During the four-day meeting, more than 200 exhibitors will demonstrate cutting-edge technology and medical device innovations for radiation oncology.
Visit us online for more information about ASTRO's 59th Annual Meeting or press opportunities at the meeting.
ABOUT ASTRO
Controlling electronic current is essential to modern electronics, as data and signals are transferred by streams of electrons which are controlled at high speed.
Demands on transmission speeds are also increasing as technology develops.
Scientists from the Chair of Laser Physics and the Chair of Applied Physics at Friedrich-Alexander-Universitt Erlangen-Nrnberg (FAU) have succeeded in switching on a current with a desired direction in graphene using a single laser pulse within a femtosecond - a femtosecond corresponds to the millionth part of a billionth of a second.
This is more than a thousand times faster compared to the most efficient transistors today.
Graphene is up to the job

In gases, insulating materials and semiconductors, scientists have already shown that it is possible to steer electrons with light waves and thus, in principle, to control current.
However, this concept has not yet been applied to metals as light cannot usually penetrate the material to control the electrons inside.
To avoid this effect, physicists in the working groups of Prof. Dr. Peter Hommelhoff and Prof. Dr. Heiko Weber used graphene - a semimetal consisting of only a single layer of carbon atoms.
Even though graphene is an excellent conductor, it is thin enough to let some light penetrate the material and move the electrons.
For their experiments, the scientists fired extremely short laser pulses with specially engineered waveforms onto graphene.
When these light waves hit the graphene, the electrons inside were hurled in one direction, like a whiplash.
'Under intense optical fields, a current was generated within a fraction of an optical cycle - a half femtosecond.
It was surprising that despite these enormous forces, quantum mechanics still plays a key role,' explains Dr. Takuya Higuchi from the Chair of Laser Physics, the first author of the publication.
Two paths to the same destination

The researchers discovered that the current generation process in the graphene follows complicated quantum mechanics.
The electrons travel from their initial state to the excited state by two paths rather than one - similar to a forked road leading to the same destination.
Like a wave, the electrons can split at the fork and flow on both roads simultaneously.
Depending on the relative phase between the split electron waves, when they meet again, the current can be very large, or not present at all.
'This is like a water wave.
Imagine a wave breaks against a building wall and flows to the left and the right of the building at the same time.
At the end of the building, both parts meet again.
If the partial waves meet at their peak, a very large wave results and current flows.
If one wave is at its peak, the other at its lowest point, the two cancel one another out, and there is no current,' explains Prof. Dr. Peter Hommelhoff from the Chair of Laser Physics.
'We can use the light waves to regulate how the electrons move and how much electricity is generated.'
Will we see electronics controlled by light frequency in the future?
The results are another important step in bringing electronics and optics together.
In the future, the method could open a door for realizing ultrafast electronics operating at optical frequencies.
###

The scientists have published their results, supported by the European Research Council (Consolidator Grant NearFieldAtto) and SFB 953 'Synthetic Carbon Allotropes', in the journal Nature: doi: 10.1038/nature23900
We already know that climate change has a hold on the earth's surface processes, such as erosion and fluctuations in sea levels... but do surface processes in turn have an influence on volcanic activity?
This was the question raised by geologists from the University of Geneva (UNIGE, Switzerland) working in partnership with the University of Orlans, University Pierre and Marie Curie in Paris and the ICTJA-CSIC Institute in Barcelona.
The researchers analysed volcanic data from the Messinian salinity crisis in the Mediterranean Sea, when the Strait of Gibraltar was blocked and the Mediterranean temporarily isolated from the Atlantic.
After observing a sharp rise in volcanic activity during this period, and testing various scenarios, the geologists concluded that the increase in magmatic activity could only be explained by the almost total drying out of the Mediterranean.
These results, which you can read all about in the journal Nature Geoscience, reveal the influence of surface processes -- largely controlled by climate -- on volcanic activity.
It is known that the Strait of Gibraltar was shut on a temporary basis during the Messinian Era (more precisely, from 5.96 to 5.33 million years ago) and that the Mediterranean Sea was isolated from the Atlantic.
In fact, as far back as the 1970s scientists found layers of salt several hundred metres thick on the seabed.
The only explanation for their formation is that there was no or very limited connection between the Mediterranean and the Atlantic.
The scientists also discovered huge underwater canyons dating back to the same period, hollowed out by rivers running over land that is now submerged, suggesting that the sea level was much lower at the time.
This also points to the massive drying up of the Mediterranean with enormous geographical and climatic disruption across the entire basin.
This hypothesis, however, continues to be a source of debate.
Nevertheless, a team of UNIGE-led geologists has provided new evidence of the Mediterranean's drying up and the forcing of surface processes on magmatic activity.
Deep magmatic activity varies according to the earth's surface pressure

"We understand that what happens at the Earth's surface, such as a sudden sea level lowering, causes the pressure to change at depth and has an effect on magma production," says Pietro Sternai, researcher in the Department of Earth Sciences in UNIGE's Science Faculty.
Given that the salinity crisis was capable of generating these changes in pressure, the geologists -- working on the hypothesis that the Mediterranean dried out -- studied the changes in volcanic activity during this period.
When a volcano erupts, the magma cools on the Earth's surface and the minerals crystallise.
Based on these silent witnesses of volcanic activity, the scientists were able to establish that there were 13 eruptions around the Mediterranean between 5.9 and 5.3 million years ago.
This is over twice the average activity, which is around 4.5 eruptions over a longer time length encompassing the salinity crisis.
Why is the figure so high?
"The single logical explanation", suggests Sternai, "is the hypothesis that the sea dried out, since this is the only event powerful enough to alter the Earth's pressure and magmatic production over the entire Mediterranean."
Numerical simulation of the Mediterranean drawdown

The geologists used numerical models to test the hypothesis that the Mediterranean dried up: they reproduced the history of the charging and discharging of the weight of water and sediment in the Mediterranean as it was drying out.
Then they calculated the changes in pressure at depth and the impact on magma production.
Two scenarios were examined: the first factored in the salinity crisis with drastic lowering of the sea level, whereas the second excluded the drawdown.
"The simulations showed that the only way to account for the proven increase in volcanic activity was that the level (and thus the weight) of the Mediterranean Sea dropped by about two kilometres," explains Sternai, before adding: "I leave it to you to imagine what the landscape looked like."
Climate and volcanism

In addition to providing further evidence of the drying out of the Mediterranean, the research also demonstrates the impact of climate change on the deep Earth.
Climate change influences magmatic production, in particular via the effects on erosion and hydrology, which modify the pressure exerted at the Earth's surface on the deep layers.
Although we have been aware of the impact of volcanism on the climate for quite some time, the results presented in the study have disclosed that the opposite is also possible.
"This pioneering work opens up new perspectives for interdisciplinary studies about the coupling between the solid Earth and the fluid Earth, and -- for example -- involving volcanologists, geomorphologists and climatologists," concludes Sternai.
Behavioral scientists and psychologists use the term "metacognition" to describe our ability to access, report and regulate our own mental states: "thinking about thinking", "knowing about knowing" "being aware about being aware", are all higher-order cognitive skills that fit this category.
Specifically, metacognition enables the brain to compute a degree of confidence when we perceive events from the external world, such as a sound, light, or touch.
The accuracy of confidence estimates is crucial in daily life, for instance when hearing a baby crying, or smelling a gas leak.
Confidence estimates also need to combine input from multiple senses simultaneously, for instance when buying a violin based on how it sounds, feels, and looks.
From a neuroscience point of view, the way metacognition operates in different senses, and for combination of senses is still a mystery: Does metacognition use the same rules for visual, auditory, or tactile stimuli, or does it use different components of each of sensory domains?
The first of these two ideas - i.e.
the "common rules" - is known as "supramodality" and it has proven controversial among neuroscientists.
Settling the matter

A series of experiments by Olaf Blanke's lab at EPFL now provide evidence in favor of supramodality.
The study, led by researcher Nathan Faivre, tested human volunteers using three different types of experimental techniques: behavioral psychophysics, computational modeling, and electrophysiological recordings.
The behavioral part of the study found that participants with high metacognitive performance for one sense (e.g.
vision) were likely to perform well in other senses (e.g.
audition or touch).
"In other words," explains Faivre, "those of us who are good at knowing what they see are also good at knowing what they hear and what they touch."
The computational modeling indicated that the confidence estimates we build when seeing an image or hearing a sound can be efficiently compared to one another.
This implies that they share the same format.
Finally, the electrophysiological recordings revealed similar characteristics when the volunteers reported confidence in their responses to audio or audiovisual stimuli.
This suggests that visual and audiovisual metacognition is based on similar neural mechanisms.
"These results make a strong case in favor of the supramodality hypothesis," says Faivre.
"They show that there is a common currency for confidence in different sensory domains - in other words, that confidence in a signal is encoded with the same format in the brain no matter where the signal comes from.
This gives metacognition a central status, whereby the monitoring of perceptual processes occurs through a common neural mechanism."
The study is an important step towards a mechanistic understanding of human metacognition.
It tells us something about how we perceive the world and become aware of our surroundings, and can potentially lead to ways of treating several neurological and psychiatric disorders where metacognition is impaired.
###

Contributing institutions

CNRS

Max Plank Institute for Human development

Humboldt-Universitt zu Berlin

Bernstein Center for Computational Neuroscience Berlin

Universidad de Buenos Aires

University Medical Center Hamburg-Eppendorf

University Hospital Geneva

Funding

Bertarelli Foundation,

Swiss National Science Foundation

European Science Foundation

Reference

Nathan Faivre, Elisa Filevich, Guillermo Solovey, Simone Kuhn, Olaf Blanke.
Behavioural, modeling, and electrophysiological evidence for domain-generality in human metacognition.
Journal of Neuroscience 15 September 2017, 0322-17.
DOI: 10.1523/JNEUROSCI.0322-17.2017
A recent study, affiliated with UNIST has presented a new way to advance the click chemistry.
This is expected to be used in various areas, such as the synthetic chemistry of new drugs, development of functional high-molecules, and bio-imaging.
In the study, the research team has introduced a new synthetic method to obtain a novel triazole structure, used for the production of drugs and high molecules.
This breakthrough has gotten much attention as it is capable of producing triazoles using water as a solvent at room temperature, instead of using high-temperature organic solvent.
This study has been led by Professor Sung You Hong in the School of Energy and Chemical Engineering at UNIST in collaboration with Dr. Gonalo J. L. Bernardes' group at University of Cambridge, Professor Sebyung Kang in the School of Life Sciences, as well as Professor Jung-Min Kee, Professor Jan-Uwe Rohde and Professor Wonyoung Choe in the School of Natural Science at UNIST.
The name triazole refers to any of the heterocyclic compounds with molecular fomula C2H3N3 that contain five atoms within the ring structure.
The presence of three nitrogen hetero-atoms in five-membered ring systems defines the class of triazole.
One of these is the 1,4-substituted 1,2,3-triazole compounds, a new synthetic method developed by Karl Barry Sharpless, the 2001 Nobel Prize Laureate in Chemistry and has been widely applied in various fields, such as pharmacology, biology, and materials science.
The Huisgen 1,3-dipolar cycloaddition is a chemical reaction to access 1,2,3-triazoles, yet it suffers from the low regioselectivity.
The Sharpless group demonstrated an elegant copper-catalyzed method to yield 1,4-disubstituted 1,2,3-triazoles in high yields under the mild conditions.
The copper-catalyzed azide-alkyne cycloaddition (CuAAC) has become the key example of click chemistry and has been widely utilized in medicinal chemistry, biochemistry, polymer chemistry, and materials science.
In 2005, the ruthenium-based chemistry to access complementary 1,5-disubstituted 1,2,3-triazoles was reported.
Yet, the RuAAC is often sensitive to air and moisture and it requires elevated temperatures.
In the study, the research team reported nickel-catalyzed cycloaddition reaction to afford 1,5-disubstituted 1,2,3-triazoles.
The reactions proceed in water and air at room temperature.
Authors employed nickelocene precatalyst and Xantphos ligand to react the building units, an organic azide and an alkyne.
"The NiAAC reaction is performed under the mild reaction conditions.
Therefore this approach can be applied in various research fields including chemical biology and materials science," says Woo Gyum Kim in the Combined M.S/Ph.D.
of Energy and Chemical Engineering, the first author of the study.
###

This study has been supported by the National Research Foundation of Korea (NRF), IT R&D program of MOTIE/KEIT, and UNIST fund.
This NiAAC reaction was published in the August issue of the Journal of the American Chemical Society.
###

Journal Reference

Woo Gyum Kim, et al., "Nickel-Catalyzed Azide-Alkyne Cycloaddition To Access 1,5-Disubstituted 1,2,3-Triazoles in Air and Water," JACS (2017).
JooHyeon Heo

Public Relations Team

Ulsan National Institute of Science and Technology (UNIST)

E. joohyeonheo@unist.ac.kr | T. 052 217 1223 | M. 010 3880 6622

Off: Main Administration Bldg.
201, Room 407
A synthetic DNA-targeting molecule could pave the way for tissue regeneration.
Stem cells can be triggered to change into heart muscle cells by a new method involving synthetic molecules.
The method overcomes challenges facing current approaches and can be fine-tuned to prompt the formation of a variety of cell types.
Human induced pluripotent stem cells (hiPSCs) are generated from adult cells and can be programmed to change into any cell type in the body.
The cell type conversion is controlled by coordinated regulation of signalling cues and genes.
Molecules that switch ON and OFF these diverse signals involved in organ development have been used to control the fate of hiPSCs.
But molecules that can directly switch OFF the desired signalling genes have not been found.
Currently available protocols involve the introduction of foreign genetic material, which could be risky to patients.
Junichi Taniguchi and Ganesh Pandian Namasivayam at Kyoto University's Institute for Integrated Cell-Material Sciences (iCeMS) in Japan constructed a synthetic molecule that can recognize and bind with a specific DNA sequence involved in the differentiation of hiPSCs into mesoderm, an intermediary cell type that can be stimulated into changing into heart muscle cells.
When the synthetic molecule, called PIP-S2, binds to its target DNA sequence, it prevents a protein, called SOX2, from binding to the same site.
SOX2 is highly expressed in hiPSCs and is responsible for keeping them in their 'pluripotent' state, meaning it stops them from converting into other cell types.
In the study, PIP-S2 bound to DNA, leading to the conversion of hiPSCs to mesoderm.
The team then added to the hiPSC cell culture another signalling inhibitor molecule that is a known driver for heart muscle cell formation.
Heart muscle cells demonstrating the ability to contract and retract were formed within a total period of 12 days.
"To our knowledge, this work reports the first DNA-binding synthetic molecule capable of guiding the differentiation of hiPSCs into a particular cell lineage," writes Hiroshi Sugiyama, the principal investigator of the study published in the journal Nucleic Acids Research.
This strategy could be used to design additional synthetic molecules that target various DNA sequences, inducing hiPSCs to develop into different cell types, the researchers conclude.
###

The paper "A synthetic DNA-binding inhibitor of SOX2 guides human induced pluripotent stem cells to differentiate into mesoderm" appeared on July 31, 2017 in Nucleic Acids Research, with doi: 10.1093/nar/gkx693
Researchers at Tohoku University have developed an innovative method for fabricating semitransparent and flexible solar cells with atomically thin 2D materials.
The new technology improves power conversion efficiency of up to 0.7% - this is the highest value for solar cells made from transparent 2D sheet materials.
Transparent or semi-transparent solar cells with excellent mechanical flexibility have attracted much attention as next-generation smart solar cells.
They can be used in various applications such as on the surfaces of windows, front display panels of personal computers and cell phones, and human skin.
But issues remain with regards to improving their power conversion efficiency, optical transparency, flexibility, stability and scalability.
Led by Associate Professor T. Kato, the team showed easy and scalable fabrication of semitransparent and flexible solar cells using transition metal dichalcogenides (TMDs) - an atomically thin 2D material.
Using a Schottky-type configuration, power conversion efficiency can be increased up to 0.7%, which is the highest value reported with few-layered TMDs.
Clear power generation was also observed for a device fabricated on a large transparent and flexible substrate.
"Since our device structure, Schottky-type solar cell, is very simple, the TMDs-based Schottky-type solar cell possesses good properties for scalability, which is one of the most important elements for use in practical applications."
says Kato.
"The transparent and semi-transparent solar cell can be used in a variety of ways.
This new type of solar cell is likely to have impact on the technologies we use in daily life in the near future."
###

Details of this study were published online on September 20 in Scientific Reports.
Scientists from Nanyang Technological University, Singapore (NTU Singapore) have developed a new gel patch prototype that could speed up the healing of a skin wound while minimising the formation of scars.
The team unveiled the patch today as a proof-of-concept.
When fully developed, this healing patch could be a boon for diabetic patients, who suffer from hard-to-heal skin lesions and for patients undergoing surgery.
The new patch is unlike other single-purpose patches in the market, which either reduce the scarring or improve healing, but not both.
The NTU research team led jointly by Associate Professor Andrew Tan and Assistant Professor Cleo Choong, recently published their findings in Scientific Reports, a peer-reviewed journal under the Nature Publishing Group.
In the published paper, the team found that a protein known as Angiopoietin-like 4 (ANGPTL4) reduces inflammation in the early phase of wound healing in mice models.
Later, it helps in the formation of new blood vessels and cell growth, and at the final phase, reduces scarring.
The team developed their patch enriched with ANGPTL4 to control the amount of scarring.
NTU already has a patent on the use of Angiopoietin-like 4 (ANGPTL4) in wound healing, which can be licensed to pharmaceutical companies.
Assoc Prof Andrew Tan, the corresponding author of the paper, said scarring happens when excessive collagen produced by the body is assembled in one direction.
"To reduce scars, all we had to do was to find a 'tuning knob' that controls the amount of collagen produced, instead of turning it off completely which is what typical anti-scarring medicine does, and which could interfere with the healing process," explained Assoc Prof Tan from NTU's School of Biological Sciences.
In their experiments involving mice with diabetic wounds, a wound was shown to heal stronger and thrice as fast with the application of ANGPTL4.
Asst Prof Cleo Choong, a materials scientist from the School of Materials Science and Engineering said the active ingredient ANGPTL4 can be harvested from discarded fatty tissues from patients in hospitals.
"The easy extraction of ANGPTL4 also could mean that in future, a surgeon can use the patient's fat and turn it into a healing agent on the spot, to promote faster recovery of the patient's wounds after an operation," she explained.
"In addition, we have developed ways to package ANGPTL4 into easy-to-use formulations such as gel patches, topical creams and injectable microcapsules.
This will make it easy for doctors and even patients to use in future, should the product be made available to the market," said Asst Prof Choong.
How the patch works

Typical anti-scarring medication targets an "on-off switch" for collagen production: a pathway in the body called TGFbeta-Smad3.
When this pathway is turned off, collagen production stops and there will be no scarring as a result.
However, turning off collagen production could be counter-productive for wound repair, since collagen is needed for various processes in skin repair and regeneration.
To control the amount of collagen produced and its assembly, Assoc Prof Tan and Asst Prof Choong studied what causes scar collagen production to go into overdrive.
They found that a protein called Scleraxis was found to be working with the TGFbeta-Smad3 pathway whenever scars were produced.
Scleraxis plays an important role in the formation of tendons, which are composed of parallel arrays of collagen closely packed together and similar in structure to scar tissue.
The NTU team found that ANGPTL4 produces molecules that interferes with Scleraxis, thus reducing the scar collagen production.
Also, the ANGPTL4 protein could be useful for other fibrotic diseases such as keloids.
A keloid is a type of raised scar that is larger than the wound that caused the scar, and has no known treatment or prevention.
Already there are other research studies, using extracts of ANGPTL4 protein from placenta and adipose (commonly known as fat) tissue, demonstrating accelerated healing in wounds.
The NTU research project took two years and was done in collaboration with Dr Marcus Wong, previously a Senior Consultant at Tan Tock Seng Hospital's Plastic Surgery Service.
Dr Wong supplied the NTU researchers with adipose tissue samples and provided advice on clinical application.
Assoc Prof Tan, who also has a joint appointment at NTU's Lee Kong Chian School of Medicine, is one of the leading experts on ANGPTL4 and holds several patents regarding its use in the treatment of infectious diseases, cancer and skin conditions.
Research interests in his group include skin wound repair and novel drug targets in tumour metastasis.
An interdisciplinary material scientist, Asst Prof Choong's research usually involves biomedical applications.
She is now developing new biomaterials which support cell regrowth, nano-encapsulation for targeted release of drugs and new drug candidates for skin and aging applications.
The research was funded by the Skin Research Institute of Singapore (SRIS), a collaboration between the Agency for Science, Technology and Research (A*STAR), National Skin Centre (NSC) and NTU.
Moving forward, the joint research team will further refine the gel formulation to improve its efficacy, before conducting further lab trials and eventually moving into clinical trials.
###

Media contact:

Lester Kok

Manager

Corporate Communications Office

Nanyang Technological University, Singapore

Email: lesterkok@ntu.edu.sg

About Nanyang Technological University, Singapore

A research-intensive public university, Nanyang Technological University, Singapore (NTU Singapore) has 33,500 undergraduate and postgraduate students in the colleges of Engineering, Business, Science, Humanities, Arts, & Social Sciences, and its Interdisciplinary Graduate School.
It also has a medical school, the Lee Kong Chian School of Medicine, set up jointly with Imperial College London.
NTU is also home to world-class autonomous institutes - the National Institute of Education, S Rajaratnam School of International Studies, Earth Observatory of Singapore, and Singapore Centre for Environmental Life Sciences Engineering - and various leading research centres such as the Nanyang Environment & Water Research Institute (NEWRI), Energy Research Institute @ NTU (ERI@N) and the Institute on Asian Consumer Insight (ACI).
Ranked 11th in the world, NTU has also been ranked the world's top young university for the last four years running.
The University's main campus has been named one of the Top 15 Most Beautiful in the World.
NTU also has a campus in Novena, Singapore's medical district.
A new two-dimensional film, made of polymers and nanoparticles and developed by researchers at the Department of Energy's Lawrence Berkeley National Laboratory (Berkeley Lab), can direct two different non-mixing liquids into a variety of exotic architectures.
This finding could lead to soft robotics, liquid circuitry, shape-shifting fluids, and a host of new materials that use soft, rather than solid, substances.
The study, reported today in the journal Nature Nanotechnology, presents the newest entry in a class of substances known as bicontinuous jammed emulsion gels, or bijels, which hold promise as a malleable liquid that can support catalytic reactions, electrical conductivity, and energy conversion.
Bijels are typically made of immiscible, or non-mixing, liquids.
People who shake their bottle of vinaigrette before pouring the dressing on their salad are familiar with such liquids.
As soon as the shaking stops, the liquids start to separate again, with the lower density liquid - often oil - rising to the top.
Trapping, or jamming, particles where these immiscible liquids meet can prevent the liquids from completely separating, stabilizing the substance into a bijel.
What makes bijels remarkable is that, rather than just making the spherical droplets that we normally see when we try to mix oil and water, the particles at the interface shape the liquids into complex networks of interconnected fluid channels.
Bijels are notoriously difficult to make, however, involving exact temperatures at precisely timed stages.
In addition, the liquid channels are normally more than 5 micrometers across, making them too large to be useful in energy conversion and catalysis.
"Bijels have long been of interest as next-generation materials for energy applications and chemical synthesis," said study lead author Caili Huang.
"The problem has been making enough of them, and with features of the right size.
In this work, we crack that problem."
Huang started the work as a graduate student with Thomas Russell, the study's principal investigator, at Berkeley Lab's Materials Sciences Division, and he continued the project as a postdoctoral researcher at DOE's Oak Ridge National Laboratory.
Creating a new bijel recipe

The method described in this new study simplifies the bijel process by first using specially coated particles about 10-20 nanometers in diameter.
The smaller-sized particles line the liquid interfaces much more quickly than the ones used in traditional bijels, making the smaller channels that are highly valued for applications.
"We've basically taken liquids like oil and water and given them a structure, and it's a structure that can be changed," said Russell, a visiting faculty scientist at Berkeley Lab.
"If the nanoparticles are responsive to electrical, magnetic, or mechanical stimuli, the bijels can become reconfigurable and re-shaped on demand by an external field."
The researchers were able to prepare new bijels from a variety of common organic, water-insoluble solvents, such as toluene, that had ligands dissolved in it, and deionized water, which contained the nanoparticles.
To ensure thorough mixing of the liquids, they subjected the emulsion to a vortex spinning at 3,200 revolutions per minute.
"This extreme shaking creates a whole bunch of new places where these particles and polymers can meet each other," said study co-author Joe Forth, a postdoctoral fellow at Berkeley Lab's Materials Sciences Division.
"You're synthesizing a lot of this material, which is in effect a thin, 2-D coating of the liquid surfaces in the system."
The liquids remained a bijel even after one week, a sign of the system's stability.
Russell, who is also a professor of polymer science and engineering at the University of Massachusetts-Amherst, added that these shape-shifting characteristics would be valuable in microreactors, microfluidic devices, and soft actuators.
Nanoparticle supersoap

Nanoparticles had not been seriously considered in bijels before because their small size made them hard to trap in the liquid interface.
To resolve that problem, the researchers coated nano-sized particles with carboxylic acids and put them in water.
They then took polymers with an added amine group - a derivative of ammonia - and dissolved them in the toluene.
This configuration took advantage of the amine group's affinity to water, a characteristic that is comparable to surfactants, like soap.
Their nanoparticle "supersoap" was designed so that the nanoparticles join ligands, forming an octopus-like shape with a polar head and nonpolar legs that get jammed at the interface, the researchers said.
"Bijels are really a new material, and also excitingly weird in that they are kinetically arrested in these unusual configurations," said study co-author Brett Helms, a staff scientist at Berkeley Lab's Molecular Foundry.
"The discovery that you can make these bijels with simple ingredients is a surprise.
We all have access to oils and water and nanocrystals, allowing broad tunability in bijel properties.
This platform also allows us to experiment with new ways to control their shape and function since they are both responsive and reconfigurable."
The nanoparticles were made of silica, but the researchers noted that in previous studies they used graphene and carbon nanotubes to form nanoparticle surfactants.
"The key is that the nanoparticles can be made of many materials," said Russell.
"The most important thing is what's on the surface."
###

Study co-authors from Oak Ridge National Laboratory are Weiyu Wang, Kunlun Hong, and Gregory Smith.
This work is supported by the DOE Office of Science.
The Molecular Foundry at Berkeley Lab and the Center for Nanophase Materials Sciences at Oak Ridge National Laboratory are DOE Office of Science User Facilities.
Lawrence Berkeley National Laboratory addresses the world's most urgent scientific challenges by advancing sustainable energy, protecting human health, creating new materials, and revealing the origin and fate of the universe.
Founded in 1931, Berkeley Lab's scientific expertise has been recognized with 13 Nobel Prizes.
The University of California manages Berkeley Lab for the U.S. Department of Energy's Office of Science.
For more, visit http://www.
DOE's Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time.
For more information, please visit science.energy.gov.
More than 5 million Americans today are affected by Alzheimer's disease (AD).
If nothing is done to stop this upward trajectory, there will be more than 16 million people with AD in the United States and more than 60 million people with AD worldwide by 2050.
In the past 25 years, only five symptomatic medications for AD have met their primary clinical endpoints in Phase III clinical trials and successfully come to market; of these, four are still available.
There is increasing evidence that multiple medical conditions increase the risk of neurodegeneration and subsequent development of dementia.
It also is becoming clear that a majority of those risk factors acts in amyloid- and tau-independent ways.
Since 2003, every symptom- and disease-modifying agent has failed in Phase II or III trials because of challenges with safety or efficacy, including trials testing the amyloid hypothesis, anti-inflammatory agents, and early-phase anti-tau therapies.
With disease-modifying treatment trials unsuccessful at the present time and only medications to treat symptoms available, what now?
Thinking "out-of-the-box," a leading neuroscientist at Florida Atlantic University has developed an innovative program in the Comprehensive Center for Brain Health at FAU called the "Dementia Prevention Initiative" (DPI), which abandons generalized methods used to research and treat AD.
His secret weapon: a novel "N-of-1 design" that individualizes medicine down to a single patient.
Instead of conducting a conventional trial of 100 people all getting the same treatment, he has switched it around and is conducting 100 single trials personalized to the individual.
His youngest patient is 61 and the oldest is 86.
"Because Alzheimer's disease is heterogeneous in terms of risk factors, age of onset, presentation, progression, and pathology burden, designing a study to treat individuals as a homogenous population requires thousands of patients who have to be followed for years and even decades.
This approach is very costly and burdensome on clinicians and patients," said James E. Galvin, M.D., M.P.H., associate dean for clinical research in FAU's Charles E. Schmidt College of Medicine, a world-renowned neuroscientist, a leading international expert on AD and Lewy Body Dementia (LBD), and founder of the DPI.
The DPI is a two-year clinical trial and Galvin is developing a best-practice model of personalized care that looks at each individual as the sole unit of observation.
The idea is to treat neurodegenerative diseases as a disorder that develops over a lifetime and individualize ways to build a better brain as we age.
The ultimate goal is to prevent dementia from happening in the first place.
Galvin's approach follows a form of personalized treatment similarly used in cancer and delivers an individualized prevention plan, tailored to each patient's risk profile based on their genetic traits, biomarkers (blood, imaging, and electrophysiology), socio-demographics, lifestyle choices, and co-existent medical conditions.
This approach specifically targets the heterogeneity of AD by identifying person-specific risk factors and applying a customized intervention directed against this risk profile.
Galvin anticipates that this method will provide more rapid information on whether personalized prevention plans can improve person-centered outcomes.
"While we know that a well-balanced, healthy lifestyle may be the cornerstone of disease prevention and brain health, each risk factor such as vascular, lifestyle choices, psychosocial behavior may both act independently and potentiate the effects of each other.
Therefore, a prevention initiative needs to be multimodal and tailored to address individual risks," said Galvin.
Although the single greatest risk factor for AD is age, AD is not inevitable.
It is estimated that at age 85 there is a 42 percent risk of developing AD, which means that 58 percent of older adults do not develop dementia, even if amyloid can be detected in the brain.
The reasons are unknown, but may be explained in part by a host of modifiable and non-modifiable risk factors.
Up to 30 percent of AD cases may be preventable through modification of risk factors and behavioral changes to mitigate the effect of those risk factors that can't be modified.
"We know what's good for the heart is good for the brain and we are changing people's blood profiles, controlling blood sugars, reducing inflammation, lowering blood pressure, and changing lipids and cholesterol," said Galvin.
"Our patients say that they are in better overall health, their moods have improved and they are more physically fit than before."
Even if these precision approaches alone are not successful in preventing AD, Galvin believes that they may greatly improve the likelihood of amyloid- or tau-specific therapies reaching their endpoints by reducing comorbidities.
"Prevention of Alzheimer's Disease: Lessons Learned and Applied," was recently published in the Journal of the American Geriatrics Society.
Nationally, if the onset of AD and related disorders is delayed by five years, 25 years later there would be approximately 5.7 million fewer cases, collective family savings would approach $87 billion, and societal savings would approach $367 billion.
###

About the Charles E. Schmidt College of Medicine:

FAU's Charles E. Schmidt College of Medicine is one of approximately 147 accredited medical schools in the U.S.
The college was launched in 2010, when the Florida Board of Governors made a landmark decision authorizing FAU to award the M.D.
degree.
After receiving approval from the Florida legislature and the governor, it became the 134th allopathic medical school in North America.
With more than 70 full and part-time faculty and more than 1,300 affiliate faculty, the college matriculates 64 medical students each year and has been nationally recognized for its innovative curriculum.
To further FAU's commitment to increase much needed medical residency positions in Palm Beach County and to ensure that the region will continue to have an adequate and well-trained physician workforce, the FAU Charles E. Schmidt College of Medicine Consortium for Graduate Medical Education (GME) was formed in fall 2011 with five leading hospitals in Palm Beach County.
In June 2014, FAU's College of Medicine welcomed its inaugural class of 36 residents in its first University-sponsored residency in internal medicine.
For more information, visit http://www.
About Florida Atlantic University:
(Boston) - Ischemia is a serious medical condition in which the flow of blood and delivery of oxygen to tissues is restricted, thus resulting in pain, weakness, and more seriously, tissue and organ damage.
Ischemia in muscle tissue, most commonly as a result of atherosclerosis, leads to life-threatening diseases like coronary artery disease and stroke, but also to chronic peripheral artery disease (PAD).
PAD symptoms can vary from discomfort and difficulty walking to debilitating pain and even limb amputations from irreversible muscle damage.
Treatment methods include blood-thinning medications and angioplasty or, in more severe cases, bypass surgery.
However, a promising alternative to surgical intervention involves stimulating angiogenesis, or growth of new blood vessels, in order to increase blood flow at the ischemic site via delivery of angiogenic growth factors like Vascular Endothelial Growth Factor (VEGF) and Insulin-like Growth Factor-1 (IGF).
Injection of growth factors systemically, or directly into the blood vessels, is a straightforward approach to deliver recombinant growth factors into the body.
This method, however, is not very safe and reliable as higher doses of factors are required for an effect, there is no specificity to targeting to the ischemic site, and proteins are not protected from degradation.
To overcome these limitations, researchers are increasingly turning to biomaterial-based delivery of growth factors.
One widely-used biopolymer found in brown seaweed is alginate.
In its hydrogel, or water-swollen form, it is biocompatible, stable, cost-effective, non-toxic and easily crosslinks, or gelates in the presence of calcium.
Doctors use alginates extensively for wound dressing and healing.
Scientists have utilized them to build 2D and 3D matrices for cellular culture, and when imbued with growth factors and stem cells released as the gel dissolves, alginate hydrogels promote growth and regeneration in a variety of tissues.
A team at the Wyss Institute for Biologically Inspired Engineering led by David Mooney Ph.D., a Founding Core Faculty of the Wyss and the Robert P. Pinkas Family Professor of Bioengineering at Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS), engineered an alginate hydrogel infused with VEGF and IGF in order to promote vascularization in the ischemic hind limbs of aged mice and young rabbits and increase perfusion, or blood flow in the damaged tissue.
This work was done in collaboration with the University of Michigan under Paul Grossman, where the rabbit studies were performed.
The findings are reported in the Journal of Vascular Research.
"Alginate hydrogels allow for gentle and protective encapsulation of therapeutic proteins and other biologically active molecules and cells," said Mooney.
"With minimally invasive injections we can deliver a combination of these active molecules while controlling degradation rates, and our tests in different ischemia animal models are very promising," he explained.
To validate their gel-based delivery method, the Wyss team addressed a number of key questions.
The first question concerns age.
Previous animal work has been carried out in young mice that are not the age equivalent of older PAD patients and individuals enrolled in therapeutic angiogenesis clinical trials.
A 17-month old mouse is roughly the equivalent of a 70-year old human; therefore 20-month old mice were selected for the aged cohort of this study.
Another critical question is that of scalability.
Mice are 3000 times smaller than humans and the protein dose must be appropriately scaled according to body mass.
The team utilized young rabbits to assess the treatment and dose efficacy for a larger animal than the mouse.
And to more accurately model the effect of their method on chronic artery disease, the researchers tested delayed treatment with growth factors.
The Wyss team began by ligating the major hind limb arteries of mice in order to generate ischemia models, and then injected the hydrogel containing VEGF and IGF directly into the ischemic site of the hind limb muscles.
"We found that blood flow at the ischemic site in aged mice increased by 2-fold compared to the gel control without factors, or gel-less injection of VEGF and IGF.
Additionally, these animals also showed more muscle strength and a higher number of small blood vessels at the injury site" said Alexander Stafford, a staff scientist at the Wyss and a leading author of this study.
Younger mice also greatly benefited from the treatment as the blood flow in their muscles reached levels comparable with the pre-ischemic state.
The scientists also tested the stability of the new blood vessels over time and found that perfusion continues even at 3 months after treatment.
The findings in the young rabbit cohort mirrored those in mice as gel-delivery of IGF and VEGF promoted perfusion and growth of smaller blood vessels stemming from arteries adjacent to the ischemic site.
Local delivery of gel-based angiogenic factors also enabled restoration of blood flow using a smaller and safer amount of growth factors as rabbits are 100-times larger than mice, but only required a 6.5-fold increase of the mouse dose.
The Wyss researchers also compared perfusion recovery between rabbits treated immediately or 30 days post-ischemia, which models chronic ischemia.
They found that the delayed treatment works just as well to increase perfusion to levels comparable with the immediately treated cohort.
A larger number of capillaries, or small blood vessels, per area were also observed compared to the controls.
This study establishes the efficacy and safety of localized alginate-based delivery of angiogenic recombinant growth factor proteins to promote vascularization and treat arterial blockage in older and larger animals.
Mooney's team plans to use these findings as groundwork for future clinical trials aimed at restoring perfusion to ischemic human tissues, such as the muscle damage caused by PAD.
Additionally, the team plans to test the use of alginate hydrogel-based delivery in regrowth of damaged peripheral nerves.
"As we focus on developing new technologies that can bring positive impact on the world, we are very excited to see this demonstration that biomaterial-mediated growth factor delivery can restore perfusion of blood vessels in both mice and rabbits, as it brings us one step closer to treating disease in human patients," said Donald Ingber, M.D., Ph.D., Founding Director of the Wyss Institute, the Judah Folkman Professor of Vascular Biology at Harvard Medical School (HMS) and the Vascular Biology Program at Boston Children's Hospital, as well as Professor of Bioengineering at the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS).
###

Other authors contributing to this study are Erin Anderson, Eduardo Silva, Yibai Hao, Kathleen Martinick, Sarah Vermillion, Elisabeth Doherty, Lin Wang, Edward Doherty and Paul Grossman.
This research was supported by NIH R01 grant HL069957, NIH R01 grant DE013349 and the Wyss Institute for Biologically Inspired Engineering.
MULTIMEDIA AVAILABLE

PRESS CONTACT

Wyss Institute for Biologically Inspired Engineering at Harvard University

Benjamin Boettner, benjamin.boettner@wyss.harvard.edu, +1 617-432-8232 MULTIMEDIA CONTACT

Wyss Institute for Biologically Inspired Engineering at Harvard University

Seth Kroll, seth.kroll@wyss.harvard.edu, +1 617-432-7758

The Wyss Institute for Biologically Inspired Engineering at Harvard University

The Wyss Institute uses Nature's design principles to develop bioinspired materials and devices that will transform medicine and create a more sustainable world.
Wyss researchers are developing innovative new engineering solutions for healthcare, energy, architecture, robotics, and manufacturing that are translated into commercial products and therapies through collaborations with clinical investigators, corporate alliances, and formation of new startups.
The Wyss Institute creates transformative technological breakthroughs by engaging in high risk research, and crosses disciplinary and institutional barriers, working as an alliance that includes Harvard's Schools of Medicine, Engineering, Arts & Sciences and Design, and in partnership with Beth Israel Deaconess Medical Center, Brigham and Women's Hospital, Boston Children's Hospital, Dana-Farber Cancer Institute, Massachusetts General Hospital, the University of Massachusetts Medical School, Spaulding Rehabilitation Hospital, Boston University, Tufts University, Charit - Universittsmedizin Berlin, University of Zurich and Massachusetts Institute of Technology.
ANN ARBOR--If you're one of those lucky individuals with high motivation and who actively pursues personal growth goals, thank your family and friends who support you.
People who view their relationships as supportive may confidently strive for growth, new University of Michigan research shows.
U-M researchers used data from samples from the United States and Japan to determine if personal growth is an outcome of an individual's traits or the positive relationships they have with others.
In Study 1, about 200 participants were randomly assigned to one of three relationship conditions: supportive, nonsupportive and neutral.
In the two main conditions, some had to consider a person in their life with whom they felt comfortable (or not) and did not worry (did worry) about being abandoned by them.
The neutral group had to consider an acquaintance for whom they did not have strong feelings.
Participants read a hypothetical scenario in which they had to choose between a higher-paying job with high familiarity (Company A) or a lower-paying job that required learning that would help their long-term career development (Company B).
Among those in the supportive relationship condition, 65 percent selected Company B, whereas 40 percent of those in the nonsupportive condition chose the same company.
Fifty percent of the neutral group picked Company B.
Participants who thought about a supportive person were more willing to choose a job that promoted personal growth, even at lower pay, in part because they had more self-confidence, the study indicated.
Studies 2 and 3 analyzed people's perceptions of the support received from family and friends to determine personal growth tendencies in two cultures.
Using data from the Survey of Midlife Development in the United States, more than 3,800 participants in Study 2 rated the support received from family and friends.
The questions included: "How much does your family (do your friends) really care about you?"
and "How much can you open up to them if you need to talk about your worries?"
They also rated their willingness to develop their potential and grow as a person, as well as self-confidence.
People who reported their relationships to be supportive had a greater willingness to grow personally and felt more self-confident, the study showed.
The results were similar in the data from the Survey of Midlife Development in Japan, which sampled about 1,000 people.
"The more supportive people judged their relationships to be, the higher their personal growth tendencies, even in a culture that puts more emphasis on the collective rather than the individual," said David Lee, the study's lead author who obtained his doctorate in psychology at U-M.
Overall, the findings support the "I-through-We" perspective, which means the social tendency to connect with others, and the individual tendency to strive and grow as individuals, are not mutually exclusive and may augment and magnify each other.
"In other words, relationships do not necessarily conflict with but help sustain one's personal growth," said Oscar Ybarra, U-M professor of psychology and of management and organizations.
The findings thus address both the importance of distinguishing yourself from others by fulfilling personal goals, but also being a good group member by fulfilling social obligations and cultivating supportive relationships.
"Building positive social connections with others should put people in a good position to receive social support that is instrumental to personal growth, as well as allowing people to strike a balance between two fundamental values: to strive and connect," said Lee, who is now a postdoctoral fellow at Ohio State University.
###

The study's other authors are Richard Gonzalez, the Amos N. Tversky Collegiate Professor of Psychology and Statistics, and Phoebe Ellsworth, the Frank Murphy Distinguished Professor of Law and Psychology.
Having one child with autism is a well-known risk factor for having another one with the same disorder, but whether and how a sibling's gender influences this risk has remained largely unknown.
Now new research led by scientists at Harvard Medical School has for the first time successfully quantified the likelihood that a family who has one child with autism would have another one with the same disorder based on the siblings' gender.
Overall, the results, published Sept. 25 in JAMA Pediatrics, reveal that having an older female child diagnosed with autism spelled elevated risk for younger siblings and that the risk was highest among younger male siblings.
They also affirm past research findings that having one child with autism or an autism spectrum disorder (ASD) portends higher risk for subsequent children, that the disorder is somewhat rare--slightly more than 1.2 percent of children in the study were affected-- and that boys have a notably higher overall risk than girls.
The findings can arm physicians and genetic counselors with information useful in counseling families and clarifying the risk for younger siblings in families who already have one child with autism.
"Our results give us a fair degree of confidence to gauge the risk of autism recurrence in families affected by it based on a child's gender," said study first author Nathan Palmer, instructor in biomedical informatics at Harvard Medical School.
"It is important to be able to provide worried parents who have one child with the condition some sense of what they can expect with their next child.
That information is critical given how much better we've become at screening for the disease earlier and earlier in life."
Such knowledge, the researchers added, could be particularly important in light of physicians' growing ability to detect autism's manifestations early in a child's life and intervene promptly.
"This study is a powerful example of how big data can illuminate patterns and give us insights that allow us to empower parents and pediatricians to implement anticipatory and far more precise medicine," said study senior author Isaac Kohane, head of the Department of Biomedical Informatics at Harvard Medical School.
The newly published results stem from the largest study of its kind.
Researchers analyzed de-identified health insurance records of more than 1.5 million U.S. families with two children between the ages of 4 and 18, tracking patterns of recurrence among siblings over a year or longer.
Of the more than 3.1 million children in the study, some 39,000, or about 1.2 percent--2 percent of boys and 0.5 percent of girls--received a diagnosis of autism or an ASD.
The results confirm previous research showing that, overall, boys have a higher risk of autism and related disorders than girls.
The results, however, also reveal a curious pattern of recurrence based on gender: Siblings born after a female child with autism or a related disorder had a higher risk than siblings born after a male child with autism.
Male children were, overall, more susceptible to autism than females.
In other words, boys with older female siblings with autism had the highest risk for autism themselves, while female siblings with older brothers with autism had the lowest risk.
For every 100 boys with an older female sibling with autism, 17 received a diagnosis of autism or a related disorder.
Male children with older male siblings with ASD had a 13 percent risk of an ASD diagnosis, followed by younger female siblings with older male siblings with ASD (7.6 percent).
The lowest risk--4 percent--was observed among younger female siblings who had an older brother with autism or an ASD.
The investigators caution that families should keep the risk in perspective because autism and related disorders remain relatively rare, affecting roughly 1 percent of the general population.
"Even for the group at highest risk--males with an older female sibling with autism--the odds are still about five to one that the child will be unaffected," Palmer said.
"What we have provided here is context for families who already have children with autism or another similar disorder and need a clearer perspective on recurrence risk."
The results, the researchers said, underscore the notion that autism and related disorders likely arise from the complex interplay between genes and environment and, for reasons yet to be understood, these conditions disproportionately affect more males than females even within families.
The stark gender variance, however, hints at a possible role of inherent biological sex differences that may precipitate the development of such disorders under the right environmental conditions, the research team said.
Autism-spectrum disorders are neurodevelopmental conditions that typically emerge in the first few years.
They are marked by a range of brain problems, impaired social interactions and compromised communication skills.
The Centers for Disease Control and Prevention estimate that autism spectrum disorders affect 1 in 68 children in the United States, with males having four times greater risk than females--an observation also borne out in the new study.
Yet exactly what portion of these diagnoses are strictly rooted in genetic mutation and how many are influenced by environmental factors has long mystified scientists.
While some forms of autism arise from a single genetic mutation, most cases appear to be the result of a complex interplay between genes and environment.
###

Other investigators involved in the study included Andrew Beam, Denis Agniel, Alal Eran, and Arjun Manrai, of Harvard Medical School; Kenneth Mandl of Boston Children's Hospital; Stanley Nelson of the University of California-Los Angeles; Claire Spettell and Kathe Fox of Aetna, Inc.; and Gregory Steinberg.
The work was supported in part by Aetna Life Insurance Company.
Harvard Medical School

Harvard Medical School has more than 11,000 faculty working in 10 academic departments located at the School's Boston campus or in hospital-based clinical departments at 15 Harvard-affiliated teaching hospitals and research institutes: Beth Israel Deaconess Medical Center, Boston Children's Hospital, Brigham and Women's Hospital, Cambridge Health Alliance, Dana-Farber Cancer Institute, Harvard Pilgrim Health Care Institute, Hebrew SeniorLife, Joslin Diabetes Center, Judge Baker Children's Center, Massachusetts Eye and Ear/Schepens Eye Research Institute, Massachusetts General Hospital, McLean Hospital, Mount Auburn Hospital, Spaulding Rehabilitation Network and VA Boston Healthcare System.
Brazil and other areas hardest hit by the Zika virus - which can cause babies to be born with abnormally small heads - are also home to dengue virus, which is spread by the same mosquito species.
A new study led by researchers at Washington University School of Medicine in St. Louis shows that an antibody that protects against dengue virus is also effective against Zika in mice.
Antibodies remain in the bloodstream for weeks, so one or a few doses of an antibody-based drug given over the course of a woman's pregnancy potentially could protect her fetus from Zika, with the added benefit of protecting her from both Zika and dengue disease, the researchers said.
Dengue causes high fever, severe headaches, and joint and muscle pain in children and adults but does not directly harm fetuses.
"We found that this antibody not only neutralizes the dengue virus but, in mice, protects both adults and fetuses from Zika disease," said Michael S. Diamond, MD, PhD, the Herbert S. Gasser Professor of Medicine and the study's senior author.
The study is published Sept. 25 in Nature Immunology.
Since dengue and Zika are related viruses, the researchers reasoned that an antibody that prevents dengue disease may do the same for Zika.
Diamond and graduate student Estefania Fernandez collaborated with Gavin Screaton, MD, DPhil, of Imperial College London, who had generated a panel of human anti-dengue antibodies years before.
The scientists infected nonpregnant adult mice with Zika virus and then administered one of the anti-dengue antibodies one, three or five days after infection.
For comparison, another group of mice was infected with Zika virus and then given a placebo.
Within three weeks of infection, more than 80 percent of the untreated mice had died, whereas all of the mice that received the anti-dengue antibody within three days of infection were still alive, and 40 percent of those that received the antibody five days after infection survived.
To find out whether the antibody also could protect fetuses from infection, the researchers infected female mice on the sixth day of their pregnancies with Zika virus and then administered a dose of antibody or a placebo one or three days later.
On the 13th day of gestation, the amount of Zika's genetic material was 600,000 times lower in the placentas and 4,900 times lower in the fetal heads from the pregnant mice that were treated one day after infection, compared with mice that received the placebo.
However, administering the antibody three days after infection was less effective: It reduced the amount of viral genetic material in the fetal heads nineteenfold and in the placentas twenty-threefold.
These findings suggest that for the antibody to effectively protect fetuses from Zika infection, it must be administered soon after infection.
Such a goal may be unrealistic clinically because women rarely know when they get infected.
However, giving women the antibody as soon as they know they are pregnant could provide them with a ready-made defense against the virus should they encounter it.
Antibody-based drugs have been used for decades to provide temporary protection against infectious diseases such as rabies when there is no time to vaccinate or, as in the case of Zika, when there is no vaccine available.
The key to using this antibody as a preventive drug would be to make sure that antibody levels in a woman's bloodstream stay high enough to protect her fetus for the duration of her pregnancy.
Diamond and colleagues are working on identifying how much antibody a pregnant woman would need to ensure that her fetus is protected from Zika.
They also are exploring ways to extend the antibody's half-life in the blood, to reduce the number of times it would need to be administered.
Having anti-dengue antibodies circulating in the bloodstream for months on end poses a risk, though, because antibodies that protect against one strain of dengue virus sometimes worsen symptoms if a person is infected by another dengue strain.
To avoid the possibility of accidentally aggravating an already very painful disease, the researchers mutated the antibody in four spots, making it impossible for the antibody to exacerbate dengue disease.
"We mutated the antibody so that it could not cause antibody enhancement of dengue infection, and it was still protective," said Diamond, who is also a professor of pathology and immunology, and of molecular microbiology.
"So now we have a version of the antibody that would be therapeutic against both viruses and safe for use in a dengue-endemic area, because it is unable to worsen disease."
A team of researchers from the UK and Russia have successfully demonstrated that a type of 'magic dust' which combines light and matter can be used to solve complex problems and could eventually surpass the capabilities of even the most powerful supercomputers.
The researchers, from Cambridge, Southampton and Cardiff Universities in the UK and the Skolkovo Institute of Science and Technology in Russia, have used quantum particles known as polaritons - which are half light and half matter - to act as a type of 'beacon' showing the way to the simplest solution to complex problems.
This entirely new design could form the basis of a new type of computer that can solve problems that are currently unsolvable, in diverse fields such as biology, finance or space travel.
The results are reported in the journal Nature Materials.
Our technological progress -- from modelling protein folding and behaviour of financial markets to devising new materials and sending fully automated missions into deep space -- depends on our ability to find the optimal solution of a mathematical formulation of a problem: the absolute minimum number of steps that it takes to solve that problem.
The search for an optimal solution is analogous to looking for the lowest point in a mountainous terrain with many valleys, trenches, and drops.
A hiker may go downhill and think that they have reached the lowest point of the entire landscape, but there may be a deeper drop just behind the next mountain.
Such a search may seem daunting in natural terrain, but imagine its complexity in high-dimensional space.
"This is exactly the problem to tackle when the objective function to minimise represents a real-life problem with many unknowns, parameters, and constraints," said Professor Natalia Berloff of Cambridge's Department of Applied Mathematics and Theoretical Physics and the Skolkovo Institute of Science and Technology, and the paper's first author.
Modern supercomputers can only deal with a small subset of such problems when the dimension of the function to be minimised is small or when the underlying structure of the problem allows it to find the optimal solution quickly even for a function of large dimensionality.
Even a hypothetical quantum computer, if realised, offers at best the quadratic speed-up for the "brute-force" search for the global minimum.
Berloff and her colleagues approached the problem from an unexpected angle: What if instead of moving along the mountainous terrain in search of the lowest point, one fills the landscape with a magical dust that only shines at the deepest level, becoming an easily detectible marker of the solution?
"A few years ago our purely theoretical proposal on how to do this was rejected by three scientific journals," said Berloff.
"One referee said, 'Who would be crazy enough to try to implement this?!'
So we had to do it ourselves, and now we've proved our proposal with experimental data."
Their 'magic dust' polaritons are created by shining a laser at stacked layers of selected atoms such as gallium, arsenic, indium, and aluminium.
The electrons in these layers absorb and emit light of a specific colour.
Polaritons are ten thousand times lighter than electrons and may achieve sufficient densities to form a new state of matter known as a Bose-Einstein condensate, where the quantum phases of polaritons synchronise and create a single macroscopic quantum object that can be detected through photoluminescence measurements.
The next question the researchers had to address was how to create a potential landscape that corresponds to the function to be minimised and to force polaritons to condense at its lowest point.
To do this, the group focused on a particular type of optimisation problem, but a type that is general enough so that any other hard problem can be related to it, namely minimisation of the XY model which is one of the most fundamental models of statistical mechanics.
The authors have shown that they can create polaritons at vertices of an arbitrary graph: as polaritons condense, the quantum phases of polaritons arrange themselves in a configuration that correspond to the absolute minimum of the objective function.
"We are just at the beginning of exploring the potential of polariton graphs for solving complex problems," said co-author Professor Pavlos Lagoudakis, Head of the Hybrid Photonics Lab at the University of Southampton and the Skolkovo Institute of Science and Technology, where the experiments were performed.
"We are currently scaling up our device to hundreds of nodes, while testing its fundamental computational power.
The ultimate goal is a microchip quantum simulator operating at ambient conditions."
Scientists have discovered that antibodies taken from patients infected with Dengue virus are effective in treating Zika infection in rodents.
The team, led by researchers at Imperial College London and Washington University in St Louis, found that giving Zika-infected mice the antibodies was enough to treat the early stages of infection, and even protected unborn pups in pregnant animals.
According to the researchers, if the findings can be replicated in humans, the discovery could potentially lead to a single therapy to protect against both viral threats.
The findings build on previous work by the group, which looked at blood samples from patients in South East Asia infected with Dengue virus, discovering they produced antibodies which were highly effective against the virus.
These Y-shaped proteins, which recognise the virus and stick to its surface, single out Dengue for destruction by the body's immune cells.
Last year, the group discovered that this same class of antibodies also recognised a close cousin of Dengue, the Zika virus.
Now, in a study published in the journal Nature Immunology, they have shown for the first time that these antibodies are effective in treating Zika infection in mice.
Dengue is found in multiple regions around the world, including the Pacific, South America, Africa and South East Asia.
The virus, of which there are four types, is spread by mosquitoes and can lead to Dengue haemorrhagic fever, a complication which can lead to shock and eventually death if untreated.
Zika is from the same family of viruses (called Flaviviridae) as Dengue, occurs in many of the same regions and is transmitted by the same species of mosquito.
Recent Zika outbreaks in South and Central America have been associated with deformities in newborns as well as Guillain-Barr syndrome, a rare but debilitating condition affecting the nerves.
Professor Gavin Screaton, Dean of the Faculty of Medicine at Imperial and senior author, said: "This paper shows for the first time that antibodies we had previously found to be effective against Dengue potently protect against Zika virus in mice and can treat the early stages of infection."
In trials, animals infected with Zika were treated with the antibodies in the first five days of infection and monitored for 21 days.
The researchers found that treatment with antibodies reduced deaths and weight loss in the rodents when compared to a control group.
They found that overall, the treatment was able to reduce the damage caused by the virus, Furthermore, tests in pregnant mice infected with Zika reduced the deaths of unborn pups to just one in 10, compared to a 90 per cent loss seen in infected control animals.
Further tests revealed that the antibody treatment was associated with a drastic reduction in the amount of virus detected in the brain and testes, sites known to be targeted by Zika.
Earlier studies have shown that being previously infected with Dengue has the potential to cause a more severe reaction to Zika, with the virus using the body's own immune cells as a Trojan horse.
In a phenomenon called antibody-dependent enhancement (ADE), antibodies stick to the Zika virus and tag it for destruction, but the immune cells which destroy the virus are unable to finish the job, and become overrun with the virus.
However, the latest study - which focused on a subgroup of these immune proteins, called anti-EDE1 antibodies - found that modifying the antibodies slightly could overcome the problem, without losing their effectiveness.
The researchers explain that while their latest findings highlight a tantalising potential for a dual-Zika-Dengue vaccine, the findings need to be replicated in primates, which will be able to give a clearer idea of potential effects in humans and whether it could move towards clinical trials in humans.
The group is now exploring how to develop a vaccine based on the target of the antibodies, pairs of interlocking proteins on the surface of the Dengue virus.
The hope is that by introducing these protein pairs into the blood stream, the body's immune cells will recognise them as a virus and start to make antibodies to counter them, leaving the host better prepared against future attack from Dengue and Zika.
Professor Screaton added: "This group of antibodies is unique in being able to target Dengue and Zika.
The next step is to see whether they are effective in larger animal models, and potentially even humans."
The outbreak of Zika in 2015-16 made headlines around the world following a sharp increase in associated birth deformities, which were seen in countries including Brazil, Colombia and Mexico with some cases reported in the US.
The wave of infection, and the associated cases of microcephaly seen in infants, seems to have passed, with a sharp drop off in cases being reported.
"The threat from Zika is clearly not as great as it was, but nobody knows when it's going to come back," said Professor Screaton.
"It might be a number of years between now and the emergence of another big epidemic."
###

The research was supported by the Wellcome Trust, with ongoing collaboration with the Pasteur Institute in Paris.
'Human antibodies to the dengue virus E-dimer epitope have therapeutic activity against Zika virus infection' by Fernandez, E. et al.
is published in Nature Immunology.
Our visual attention is drawn to parts of a scene that have meaning, rather than to those that are salient or "stick out," according to new research from the Center for Mind and Brain at the University of California, Davis.
The findings, published Sept. 25 in the journal Nature Human Behavior, overturn the widely-held model of visual attention.
"A lot of people will have to rethink things," said Professor John Henderson, who led the research.
"The saliency hypothesis really is the dominant view."
Our eyes we perceive a wide field of view in front of us, but we only focus our attention on a small part of this field.
How do we decide where to direct our attention, without thinking about it?
The dominant theory in attention studies is "visual salience," Henderson said.
Salience means things that "stick out" from the background, like colorful berries on a background of leaves or a brightly lit object in a room.
Saliency is relatively easy to measure.
You can map the amount of saliency in different areas of a picture by measuring relative contrast or brightness, for example.
Henderson called this the "magpie theory" our attention is drawn to bright and shiny objects.
"It becomes obvious, though, that it can't be right," he said, otherwise we would constantly be distracted.
Making a Map of Meaning

Henderson and postdoctoral researcher Taylor Hayes set out to test whether attention is guided instead by how "meaningful" we find an area within our view.
They first had to construct "meaning maps" of test scenes, where different parts of the scene had different levels of meaning to an observer.
To make their meaning map, Henderson and Hayes took images of scenes, broke them up into overlapping circular tiles, and submitted the individual tiles to the online crowdsourcing service Mechanical Turk, asking users to rate the tiles for meaning.
By tallying the votes of Mechanical Turk users they were able to assign levels of meaning to different areas of an image and create a meaning map comparable to a saliency map of the same scene.
Next, they tracked the eye movements of volunteers as they looked at the scene.
Those eyetracks gave them a map of what parts of the image attracted the most attention.
This "attention map" was closer to the meaning map than the salience map, Henderson said.
In Search of Meaning

Henderson and Hayes don't yet have firm data on what makes part of a scene meaningful, although they have some ideas.
For example, a cluttered table or shelf attracted more attention than a highly salient splash of sunlight on a wall.
With further work, they hope to develop a "taxonomy of meaning," Henderson said.
Although the research is aimed at a fundamental understanding of how visual attention works, there could be some near-term applications, Henderson said, for example in developing automated visual systems that allow computers to scan security footage or to automatically identify or caption images online.
###

The work was supported by the National Science Foundation.
Stem rust, named for the blackening pustules that infect plant stems, caused devastating crop epidemics and famine for centuries before being tamed by fungicides and resistance genes.
Since the turn of the century, however, aggressive new strains have emerged -- such as 'Ug99', first detected in Uganda in 1999 -- that infect widely grown varieties of wheat.
These diseases threaten to disperse trillions of pathogenic fungal spores on winds across countries and continents.
The current global economic loss from wheat stem rust is approximately U.S. $1 billion a year.
The fear is that these airborne and highly virulent strains could spread from known sites to some of the world's most important 'breadbasket' regions, such as the Punjab in South Asia, where these strains have not yet been detected.
Now, a team of scientists of the University of Cambridge, the UK Met Office and CIMMYT (International Maize and Wheat Improvement Centre) have adapted modelling systems previously used to forecast, ash dispersal from erupting volcanoes and radiation from nuclear accidents (NAME), to predict when and how Ug99 and other such strains are most likely to spread.
The research, published today in the journal Nature Plants, quantifies for the first time the circumstances -- routes, timings and outbreak sizes -- under which dangerous strains of stem rust pose a threat from long-distance dispersal out of East Africa to the large wheat-producing areas in India and Pakistan.
The results highlight the role of Yemen as a potential 'stepping stone' for the transmission of the disease between continents.
The key scenario for disease spread is from Yemen directly to Pakistan or India.
In case of a large outbreak in Eastern Yemen results indicate a 30% chance for transmission to occur.
Another important scenario for wheat rust spread is from Yemen through Middle Eastern countries, in particular Iran, to Central and South Asia.
If Iran were to suffer a moderate outbreak of Ug99 -- more than 1000 hectares -- then spores would likely spread to Afghanistan, and from there potentially further to the northern plains of Pakistan and India.
However, transmission along this route is restricted to a relatively short time-window in March and April, before wheat is typically harvested in South Asia.
"New races of wheat rust are threatening wheat worldwide, and we need to know which areas are at risk," said senior author Prof Chris Gilligan, from Cambridge's Department of Plant Sciences.
"From our work, we now believe that if we start to see Ug99 or other new wheat rust strains take hold in Yemen in early spring then action must be taken immediately to mitigate the risk of further spread."
However, the modelling work also offers some encouraging news: the airborne transmission of the disease from East African countries directly to South Asia is highly unlikely, with transmission events possible only on less than one day a year.
The scientific team used field disease surveys from the International Maize and Wheat Improvement Centre and weather data from the UK Met Office as key input for the modelling framework.
"This research has allowed us to obtain the first quantitative estimates of long-term airborne spore transmission frequencies for different outbreak scenarios.
We compiled risk assessments for pathogen dispersal from key disease locations to important wheat-producing countries.
These assessments can effectively inform surveillance and control strategies," said Cambridge's Marcel Meyer, the study's first author.
The team say their work, including 3-D spore dispersal animations and a catalogue of dispersal trends (indicating likely directions, frequencies, pathogen loads), provides new ways to raise awareness, communicate risks, and inform agricultural stakeholders.
Their modelling framework can be applied as a tool to analyse risks in case new disease strains should be uncovered in other geographic areas.
This has already helped in estimating dispersal risks from sites of other diseases in Europe and Siberia.
In ongoing work the team is developing an Early Warning System forecasting rust risk in Ethiopia, East Africa's largest wheat producing country.
"The combined expertise from plant sciences and atmospheric dispersion sciences has delivered ground breaking tools that highlight the risks, and support the management of the devastating potential of these diseases," said Dr Matthew Hort, co-author from the UK's Met Office.
CLEVELAND--A new magnetic resonance imaging (MRI) contrast agent being tested by researchers at Case Western Reserve University not only pinpoints breast cancers at early stages but differentiates between aggressive and slow-growing types.
"Doing both will help doctors find the right treatment," said Zheng-Rong Lu, the M. Frank Rudy and Margaret Dormiter Rudy Professor of Biomedical Engineering at Case Western Reserve and leader of the research.
"There's no such technology available now that we know of."
The gadolinium-based agent is also more efficient and safer than traditional agents, requiring a gadolinium dose 20-times smaller, easily flushing from the body and leaving no accumulation in tissues, the researchers found in tests with mouse models.
At the low dosage, the agent lights up cancer biomarkers during scans, overcoming the low sensitivity of MRI's for imaging the markers.
The research was published today (Sept. 25) in Nature Communications.
To make the agent, Lu and colleagues at Case Western Reserve combined commercially available tri-gadolinium nitride metallofullerene (Gd3N@C80), a highly efficient contrast agent, with a peptide labeled ZD2, which was developed in Lu's lab.
Compared to the gadolinium used in traditional agents, Gd3N@C80's "structure is different--the gadolinium ions are encaged in a hollow molecule of fullerene that looks like a soccer ball," Lu said.
"The cage prevents direct contact between the gadolinium and tissue, and the gadolinium will not be released, which prevents any kind of interaction with tissue."
"But the key technology for our targeted contrast agent is the peptide attached," Lu said.
The lab applies ZD2 to the surface of the soccer ball.
The peptide specifically targets the cancer protein extradomain-B fibronectin (EDB-FN).
EDB-FN, which is associated with tumor invasion, metastasis and drug resistance, is highly expressed in the matrix around cancerous cells in many aggressive forms of human cancers.
In testing on six mouse models, MRI's detected breast cancers in all cases.
But the signal created by the accumulation of contrast molecules on three aggressive triple-negative breast cancers (MDA-MB-231, Hs578T and BT549) were significantly brighter.
Because slow-moving ER-positive breast cancers (MCF-7, ZR-75-1 and T47D) produce less EDB-FN, fewer molecules attached.
While detectible, the signal was muted.
Coauthors of the study are biomedical engineering PhD students Zheng Han and Xiaohui Wu, research assistant Sarah Roelle and undergraduate student Chuheng Chen; and William Schiemann, the Goodman-Blum Professor of Cancer Research at the Case Comprehensive Cancer Center.
Lu's lab is now investigating ways to reduce the cost of producing the agent to make it more attractive for clinical use.
###

Case Western Reserve University is one of the country's leading private research institutions.
Located in Cleveland, we offer a unique combination of forward-thinking educational opportunities in an inspiring cultural setting.
Our leading-edge faculty engage in teaching and research in a collaborative, hands-on environment.
Our nationally recognized programs include arts and sciences, dental medicine, engineering, law, management, medicine, nursing and social work.
About 5,100 undergraduate and 6,200 graduate students comprise our student body.
Visit case.edu to see how Case Western Reserve thinks beyond the possible.
The world's botanic gardens contain at least 30% of all known plant species, including 41% of all those classed as 'threatened', according to the most comprehensive analysis to date of diversity in 'ex-situ' collections: those plants conserved outside natural habitats.
The study, published today in the journal Nature Plants, found that the global network of botanic gardens conserves living plants representing almost two-thirds of plant 'genera' (the classification above species) and over 90% of plant families.
However, researchers from the University of Cambridge discovered a significant imbalance between temperate and tropical regions.
The vast majority of all plants species grown ex-situ are held in the northern hemisphere.
Consequently, some 60% of temperate plant species were represented in botanic gardens but only 25% of tropical species, despite the fact that the majority of plant species are tropical.
For the study, researchers analysed datasets compiled by Botanic Gardens Conservation International (BGCI).
They cross-referenced the working list of known plant species - currently sitting at 350,699 - with the species records of a third of botanic gardens on the planet, some 1,116 institutions.
They say this provides a "minimum estimate" for the plant diversity held in botanic gardens.
However, while gardens hold approaching half all threatened species, just 10% of overall storage capacity is dedicated to such plants.
The researchers argue that botanic gardens are of "critical importance to plant conservation", and internationally coordinated efforts are needed to house even more species at risk of extinction - particularly those from tropical climates.
"The global network of botanic gardens is our best hope for saving some of the world's most endangered plants," said senior author Dr Samuel Brockington, a researcher at Cambridge's Department of Plant Sciences as well as a curator at the University's own Botanic Garden.
"Currently, an estimated one fifth of plant diversity is under threat, yet there is no technical reason why any plant species should become extinct.
Botanic gardens protect an astonishing amount of plant diversity in cultivation, but we need to respond directly to the extinction crisis.
"If we do not conserve our plant diversity, humanity will struggle to solve the global challenges of food and fuel security, environmental degradation, and climate change."
The plants not currently grown in botanic gardens are often more interesting than those that are, say the researchers.
Hydrostachys polymorpha, for example, an African aquatic plant that only grows in fast flowing streams and waterfalls, or the tiny parasitic plant Pilostyles thurberi - only a few millimetres long, it lives completely within the stem tissue of desert shrubs.
Species from the most ancient plant lineages, termed 'non-vascular' plants, are currently almost undocumented in botanic gardens - with as few as 5% of all species stored in the global network.
These include plants such as the liverworts and mosses.
"Non-vascular species are the living representatives of the first plants to colonise the land," said Brockington.
"Within these plants are captured key moments in the early evolutionary history of life on Earth, and they are essential for understanding the evolution of plants."
From the spectacular new Gardens by the Bay in Singapore to London's own legendary Royal Botanic Gardens at Kew, the world's botanic gardens collectively host 500 million visitors a year.
"As a professional community, botanic gardens conserve and manage a far greater array of plant diversity than any other sector.
However, we still have much to do."
said Dr Paul Smith, study co-author and Secretary General of BGCI.
"This study is extremely important because it will enable us to target our efforts much more effectively, and work together to ensure that plant species don't needlessly become extinct," Smith said.
Subnanometer noble-metal clusters have been studied previously for these possibilities in catalytic applications.
For example, it is known that platinum is used as a catalyst for a fuel cell, which is conductive and exhibits magnetism.
Platinum is indispensable as a material for the next-generation energy grid, but it is "rare metal" whose reserves are limited.
In order to make effective use of resources, it is essential to improve the atomic level accuracy of subnanometer metal clusters and increase the amount of synthesis.
While several different synthesis methods have been developed from multi-nuclear metal complexes or stable ligand-protected "magic number" clusters, the precursors were not scalable and the exact preservation of atomicity was never provided.
This may have been due, in part, to the high metal-to-ligand binding energy that requires extremely high calcination temperature, which results in the aggregation of the clusters.
Scientists at Tokyo Tech investigated platinum-thiolates because they may lead to the formation of bare metal clusters through reductive metal-sulfur bond cleavage.
These platinum-thiolates have been shown to form tiara-like complexes that are stable enough for isolation.
Takane Imaoka, Kimihisa Yamamoto and colleagues studied the monodispersed subnanometer platinum clusters, and their catalytic activity for preparative-scale reactions through non-destructive conversion from platinum thiolate complexes to platinum tiara-like clusters.
Prior to this study, only one example of a platinum thiolate tiara-like complex with full chemical identification has been published.
As a basis for this study, they began the investigation with basic reactions between PtCl4 and n-octanethiol.
Results of these basic reactions suggested that platinum thiolates initially undergo a linear chain growth, followed by entropically favorable formation of smaller cyclic compounds.
Based on these results, they optimized the synthesis protocol in two steps that resulted in a crude product with platinum tiara-like complexes of various ring sizes.
The platinum tiara-like complexes where further isolated into their pure clusters based on ring sizes through size exclusion chromatography based on preparative recycling HPLC.
To affirm the potential of these complexes to be used as precursors for atom-precise platinum clusters, the thermostability and catalytic activity of the complexes were investigated.
These scientists discovered that the tiara-like platinum thiolate complexes were suitable as precursors for the synthesis of monodispersed zero-valent platinum clusters with a specific atomicity: a reaction that previous chemical methods could not realize.
This is the first method to demonstrate a preparative-scale reaction using atom precise clusters with a single digit atomicity, which is expected to be useful in producing high-performance catalysts.
Further investigation on the stability and support effect is necessary for the complete understanding of the catalyst system by clusters of such a small size.
Given how proud we are of our big brains, it's ironic that we haven't yet figured out why we have them.
One idea, called the cognitive buffer hypothesis, is that the evolution of large brains is driven by the adaptive benefits of being able to mount quick, flexible behavioral responses to frequent or unexpected environmental change.
It is difficult to test this idea on people because there is only one living species in the genus Homo.
Birds, according to Carlos Botero, assistant professor of biology in Arts & Sciences at Washington University in St. Louis, are another matter.
There are many species, they have a range of brain sizes and they live everywhere.
In many ways, they are the ideal group for testing this hypothesis.
As a young scholar, Botero was able to show how mockingbirds that live in variable habitats have more elaborate songs.
Since song complexity is a proxy for learning ability, this finding seemed to support the cognitive buffer hypothesis.
But, after a while, he began to think about alternative explanations for his results.
The hypothesis requires that big brains improve survival, but Botero's study didn't show this.
And it didn't settle a crucial timing issue: Did large brains evolve in variable habitats, or did they evolve elsewhere and then make it easier to colonize harsh environments?
However, the mockingbird study didn't look back in time.
So together with Trevor Fristoe, postdoctoral associate in biology at Washington University and Canadian biologist Andrew Iwaniuk of the University of Lethbridge, Botero decided to tease out the assumptions behind the cognitive buffer hypothesis and test each of them separately.
Their study, published Sept. 25 in Nature Ecology and Evolution, showed that large brains weren't more likely to evolve in variable compared to stable habitats, so that part of the hypothesis wasn't supported.
But it also showed that brainier birds were better able to colonize seasonal, unpredictable places.
So birds with big brains were able to move into a broader range of environments.
"The findings were pretty surprising," Fristoe said.
"In the first part of the study, we showed that a big brain really does give birds a survival advantage in variable environments.
So the mechanism works.
But that made it all the more puzzling when the second part of the study showed that big brains often evolved in stable -- not in variable -- habitats."
Botero is the first to acknowledge that brain size is an imperfect measure of cognition, a term that itself has many definitions.
What the scientists looked at was not absolute brain size, but the difference between brain size and the statistically predicted brain size for the bird's body size.
"An ostrich seems to have a huge brain, but relative to its body size, it's really not that impressive," Botero said.
"A raven is not much larger than a chicken, but its brain is proportionally much more massive.
"The correlation between relative brain size and cognitive ability is better for birds than for mammals," Botero said.
"Although relative brain size is a noisy metric, it's still one of the better ways we have to measure brain-related differences among species at large taxonomic scales.
"This whole field is fraught with caveats."
Botero and Fristoe first tested the assumption that a bigger brain gave birds a survival advantage by analyzing the data gathered by the Breeding Bird Survey, a huge database of bird sightings that is used to monitor populations of North American birds.
Each year since 1966, volunteer birders have followed pre-established routes during peak breeding season, stopping for three minutes at designated points to count all the birds they can hear or see.
"We went through all the data for North America, all of the species for which we knew brain size, and came up with a metric for population stability, adjusting for other factors that can affect stability such as clutch size and whether or not the bird is migratory," Botero said.
They characterized environmental conditions over the same period with data from ecoClimate, an open database of climatic simulations, and data from NASA Earth Observations.
"We showed that species with big brains maintain stable populations in environments where the temperature, precipitation or productivity change a lot, and species with smaller brains cope less well," Botero said.
"So the mechanism people were proposing really does seem to work," he said.
"Big brains do improve survival when environmental conditions change frequently and unexpectedly."
The scientists were now ready to tackle the main issue.
"The cognitive buffer hypothesis asserts brains became bigger because species were being exposed to more variable environments," Botero said.
"It makes sense, but is it true?"
For the hypothesis to be true, the variable conditions had to happen first, and that meant the scientists had to devise some way to reconstruct the characteristics of birds and environments which vanished long ago.
To do this, they looked for evolutionary correlations between transitions in brain size and the temperature and precipitation variability of species-specific habitats in a global phylogeny of birds (a diagram that represents the order in which species are thought to have evolved from a common ancestor).
"We found that big brains are equally likely to evolve in places that had variable conditions and places that had stable conditions.
We don't see any difference between the two," Botero said.
(See sidebars for examples.)
"But we found that variable environments are more likely to be colonized by species that already had big brains," he said.
"That explains why, when we go out today, we find an association between big brains and variable environments.
And probably why his earlier study found the best singers among mockingbirds lived in variable habitats.
So we now know a big brain helped species like the common raven to expan into the variety of habitats where they live today, but we still don't know why ravens and even humans evolved big brains in the first place.
Botero and Fristoe are thinking about it.
Molecules made in labs often have a downside that nature mysteriously avoids.
The problem is that many molecules are chiral, which means they have an asymmetrical structure.
A consequence of chirality is that when we synthesize a chiral molecule we also often make its doppelganger, a mirror image of the intended molecule.
The two may look similar but, like the right and left hand, they aren't interchangeable.
Depending on the handedness, the molecule limonene smells like oranges or turpentine, ibuprofen can be four times more potent and thalidomide either treats morning sickness or leads to severe birth defects.
"Approximately 50 percent of drugs and 30 percent of agrichemicals are chiral, which means they can be left- or right-handed.
Of those, more than 90 percent are sold as mixtures of both handed molecules because it's so hard to separate them," said Jennifer Dionne, associate professor of materials science and engineering at Stanford University.
The normal chemical methods of separating molecules - to keep the good version and weed out the bad - are expensive, time-intensive or inefficient.
Dionne's lab has now shown one approach that holds promise for separating chiral molecules.
It involves a nanostructured filter that, when illuminated with a laser, attracts one handed specimen while repelling its mirror image.
The team published this technique in the Sept. 25 issue of Nature Nanotechnology.
A light handshake

Focused light can change the momentum of an object.
This effect has been used to create incredible tools, called optical tweezers, which allow scientists to manipulate particles with highly focused beams of light.
(It was his work with optical tweezers that earned Steven Chu, professor of physics at Stanford and professor of molecular and cellular physiology in the Stanford School of Medicine, the Nobel Prize in Physics in 1997.)
Although the idea of tweezing apart chiral forms has seemed appealing, many of the molecules we want to target are too small to be pulled apart by optical forces directly.
Yang Zhao, a postdoctoral fellow in the Dionne lab, overcame that weakness by creating a nanostructure that allows circularly polarized light to interact more strongly with small specimens.
The light path in the nanostructure maps a spiral in one direction but not the other.
Once the chiral light has passed through this path, it interacts with molecules that complement its shape and pulls those downward.
The researchers tested their prototype by measuring the forces exerted on chiral specimens.
They built a tool called a chiral optical force microscope, which combines the optical tweezers with an atomic force microscope (AFM), a tool capable of resolving the chemical structure of a single molecule.
A chiral AFM tip served as the chiral specimen and, at the same time, mapped out the forces specific to the handedness of the tip.
They showed that the optical forces produced by their tweezers are strong enough to separate certain chiral molecules.
Building the optical filter

The team has not yet tested the tweezers on actual chiral molecules, but Zhao has begun quantifying the forces they are able to apply to DNA and certain proteins.
These chiral molecules have a specific handedness in nature but can be either handedness if produced in a lab.
The next step will be assembling their tweezers into a sort of filter that can separate two forms of a drug or other molecules.
"We will put many of these nanostructures on a microfluidic chip where a drug of interest can be introduced," said Zhao.
"If it works as we want it to, we should be able to have the drug separated upon illumination."
In addition to sorting drugs to make them safer or more effective, the researchers think their tweezers could be put to other uses, such as monitoring the folding or unfolding of a protein or enabling light-mediated synthesis of chiral chemicals.
###

Stanford co-authors of this paper include Brian Baum, Justin A. Briggs, Alice Lay, Olivia Alexandra Reyes-Becerra and Amr A.E.
Saleh, who is also of Cairo University in Giza, Egypt.
Additional co-author Marie Anne van de Haar is at FOM Institute AMOLF in Amsterdam, Netherlands.
Dionne is also a member of Stanford Bio-X, an affiliate of the Precourt Institute for Energy and a member of the Stanford Neurosciences Institute.
This research was funded by the Gordon and Betty Moore Foundation, the National Science Foundation, the Air Force Office of Scientific Research and a European Research Council grant.
Children who watched a PG-rated movie clip containing guns played with a disabled real gun longer and pulled the trigger more often than children who saw the same movie not containing guns, according to the results of a randomized experiment published in a new article by JAMA Pediatrics.
Many U.S. households with guns do not secure the weapons and children in the United States are more likely to die by unintentional gun shootings than children in other developed countries.
Lots of factors influence children's interest in guns.
The experiment by Brad J. Bushman, Ph.D., of Ohio State University, Columbus, and Kelly P. Dillon, Ph.D., of Wittenberg University, Springfield, Ohio, and formerly of Ohio State University, focused on movie characters with guns.
The experiment included 104 children (52 pairs of siblings, cousins, step-siblings or friends) between the ages of 8 and 12.
Each pair was randomly selected to watch a 20-minute edited version of the PG-rated films "The Rocketeer" or "National Treasure" that did or did not contain guns.
Scenes in the movies showing guns were edited out for the no-gun version but the action and narrative of the film were not altered.
After watching the movie, the children were taken to a different room with a cabinet full of toys and they were told they could play with any of the toys and games in the room.
One drawer of the cabinet contained a real 0.38-caliber handgun that had been modified so it could not fire, although the gun's hammer and trigger were still functional.
The children had 20 minutes to play in the room together with the door closed.
Of the 52 pairs of children, 43 pairs (82.7 percent) found the gun in the cabinet drawer; 14 pairs (26.9 percent) gave the gun to a research assistant or told them about it; and 22 pairs (42.3 percent) had one or both children handle the gun.
The type of movie clip (containing guns or not containing guns) did not influence whether children found the gun or handled it, according to the results.
However, the median number of trigger pulls among children who saw the movie containing guns was 2.8 trigger pulls compared with 0.01 trigger pulls among children who saw the movie not containing guns.
Also, the median number of seconds spent holding the gun among children who saw a movie containing guns was 53.1 seconds compared with 11.1 seconds among children who saw the movie not containing guns, according to the results.
Analyses suggest children who saw the movie containing guns also played more aggressively and sometimes fired the gun at people.
The study notes limitations, including only one modified handgun was available for play and a stationary hidden camera could record the entire room but not all the actions of all the participants.
"The present experiment aimed to understand the connection between exposure to gun violence in the media and interest in and playing with guns in the real world.
We believe that these data are a compelling start to the conversation on the various factors that can increase children's interest in guns and violence," the article concludes.
###

For more details and to read the full article, please visit the For The Media website.
(doi:10.1001/jamapediatrics.2017.2229)

Editor's Note: Please see the article for additional information, including other authors, author contributions and affiliations, financial disclosures, funding and support, etc.
Enrollment in the Supplemental Nutrition Assistance Program (SNAP), the nation's largest program aimed at alleviating food insecurity, was associated with reduced health care spending by low-income adults in the United States over a two-year period, according to a new study published by JAMA Internal Medicine.
SNAP serves approximately 1 in 7 Americans and provides a monthly benefit to supplement household budgets to buy food.
SNAP eligibility is set at the federal level but enrollment policies vary by state.
Policymakers and clinicians are interested in whether social programs such as SNAP can offer benefits to the health care sector.
Seth A. Berkowitz, M.D., M.P.H., of the Massachusetts General Hospital and Harvard Medical School, Boston, and coauthors used survey data to explore the relationship between SNAP program participation and health care costs, while accounting for factors that may influence the likelihood of participating in SNAP.
The study included 4,447 adults with income below 200 percent of the federal poverty threshold who participated in the 2011 National Health Interview Survey and the 2012-2013 Medical Expenditure Panel Survey.
Of the 4,447 participants, 1,889 were SNAP participants and 2,558 were not.
SNAP program participation was associated with about $1,400 in lower subsequent health care costs per year per person for low-income adults, according to the results.
The study acknowledges limitations and notes questions remain unanswered, including the development of a deeper understanding of the mechanism by which SNAP and other food security assistance programs could lead to changes in health and health care expenditures.
"The results of this study have several policy indications.
Prioritizing ways to make it easier for eligible Americans to enroll in SNAP is likely to be a feasible way to help reduce health care costs.
... As an entitlement program, SNAP benefits are paid for by the federal government, while Medicaid, which would likely see some of the savings if health care costs are reduced, is paid for jointly by states and the federal government.
Although this study focused on health care expenditures, SNAP is a food insecurity and nutrition program, not a health care program.
SNAP's purpose is not to reduce health care expenditures, and we are of the opinion that its funding justification does not depend on affecting health care costs," the article concludes.
###

For more details and to read the full study, please visit the For The Media website.
(doi:10.1001/jamainternmed.2017.4841)

Editor's Note: The study includes funding/support disclosures.
Please see the article for conflict of interesting and funding/support disclosures.
For more information, including other authors, author contributions and affiliations, financial disclosures, funding and support, etc.
A study led by a Massachusetts General Hospital (MGH) investigator suggests that participation in the Supplemental Nutritional Assistance Program (SNAP, formerly known as the Food Stamp Program) may reduce health care costs for recipients.
In their paper published in JAMA Internal Medicine, the researchers describe finding, after controlling for factors known to be common among SNAP participants, that annual health care costs for recipients were around $1,400 less than for low-income individuals not participant in SNAP.
"These savings are significant, especially because SNAP is not designed as a health care program," says Seth Berkowitz, MD, MPH, of the MGH Division of General Internal Medicine, who led the study.
"Prior studies have found that food insecurity is associated with higher health care costs, and our findings indicate that directly addressing social determinants of health such as food insecurity could be an important way to lower costs."
SNAP provides a monthly benefit that low-income participants can use to purchase certain food products.
The program serves approximately one in seven Americans, and while eligibility standards are set by the Federal government, enrollment policies are set by individual states.
As Berkowitz notes, studies have proven that SNAP participation can reduce food insecurity, defined as the inability to purchase nutritious food on a regular basis due to cost.
Several studies have found that low-income individuals have more health problems and, as a result, higher health care costs.
Food insecurity could contribute to that association in several ways - including poor dietary quality, contributing to conditions such as obesity and type 2 diabetes; the need to choose between purchasing food or medications; and financial stress that draws attention away from chronic disease management.
Prior to this study, it was unclear whether participating in SNAP could reduce health costs.
To address that question, the researchers analyzed data from two surveys: the 2011 National Health Interview Survey (NHIS) of the National Center for Health Statistics and the 2012-13 Medical Expenditure Panel Survey (MEPS) of the Agency for Healthcare Research and Quality.
MEPS surveyed a subset of 2011 NHIS respondents regarding their health care expenditures over the two-year period, with data provided by respondents verified by their clinicians and third-party payers.
Data used for the current study reflected 4,447 MEPS respondents who were over age 18, had family incomes below 200 percent of the federal poverty level and, on the 2011 NHIS, answered whether or not they had received SNAP benefits at any time during the previous year.
While unadjusted results suggested little difference in annual health care costs between those who did and did not receive SNAP benefits in 2011, Berkowitz notes several confounding factors are likely to explain this.
Compared with individuals who are eligible for but do not participate in the program, SNAP participants are known to have more health problems, more severe health problems, are more likely to be disabled and to have incomes even lower than nonparticipating eligible individuals.
Controlling for those factors revealed that SNAP participation could reduce annual health care costs by $1,400 per person.
"While our study was not a randomized experiment, which means it is possible that some factor we were not able to account for could explain these results, the evidence that SNAP participation can lower health care costs appears strong," he says.
"Receiving SNAP benefits could make it easier to follow recommended diets to manage chronic illness, free up resources that would other be spent on food for other disease management activities, and reduce stress over concerns such as where one's next meal is coming from."
The researchers note that states may benefit from making it easier to enroll in SNAP, since it is funded by the Federal government, while Medicaid costs are shared with the states.
"Combining easier enrollment with proven health education programs - such as the SNAP-ED program that provides nutritional teaching - could further extend the health benefits of SNAP participation," says Berkowitz, who is an assistant professor of Medicine at Harvard Medical School.
###

Co-authors of the JAMA Internal Medicine paper are James Meigs, MD, MPH, MGH General Internal Medicine; Hilary Seligman, MD, MAS, University of California, San Francisco; and Joseph Rigdon, PhD, and Sanjay Basu, MD, PhD, Stanford University.
The study was funded by a grant from the University of Kentucky Center for Poverty Research through U.S. Department of Agriculture agreement 58-5000-3-0066; and by the National Institute for Diabetes, Digestive and Kidney Diseases, the National Heart, Lung and Blood Institute, and the National Institute on Minority Health and Health Disparities.
DURHAM, N.C. -- A study by Chinese and U.S. scientists finds that while populations of the iconic giant panda have increased recently, the species' habitat still covers less area and is more fragmented than when it was first listed as an endangered species in 1988.
The study, published Sept. 25 in the peer-reviewed journal Nature Ecology and Evolution, used geospatial technologies and remote sensing data to map recent land-use changes and the development of roads within the panda's habitat.
"The International Union for Conservation of Nature (IUCN) has recently changed the status of the giant panda from 'endangered' to the less threatened 'vulnerable,' " said Stuart L. Pimm, Doris Duke Professor of Conservation Ecology at Duke University's Nicholas School of the Environment.
"This was based on the increasing numbers, which are a very encouraging sign, of course."
"But what my colleagues and I wanted to know was how the panda's habitat has changed over the last four decades, because the extent and connectivity of a species' habitat is also a major factor in determining its risk of extinction," Pimm said.
The team, led by Zhuyan Ouyang and Weihua Xu of the Research Center for Eco-Environmental Sciences at the Chinese Academy of Sciences, used satellite imagery to examine changes across the panda's entire geographic range from 1976 to 2013.
"We found complex changes," Xu said.
"Habitat decreased nearly 5 percent from 1976 to 2001, but has increased since.
However, the average size of the habitat patches decreased by 23 percent from 1976 to 2001.
It has increased only slightly since."
Study co-author Jianguo Liu of Michigan State University, who began studying the human and natural forces driving habitat loss in the panda's geographic range in 1996, noted that some of the changes that have occurred in the region are encouraging.
"Banning commercial logging in natural forests, establishing nature reserves and helping residents in the reserve change behaviors that damaged habitat has been beneficial," said Liu, who published Pandas and People (Oxford University Press) last year with four other authors of the new study.
"But conservation is a dynamic process with humans and nature in a constant push and pull to survive and thrive, so new solutions always are in demand."
Other changes, though highly beneficial to the region's human population, present challenges from a conservation standpoint.
"The most obvious changes in this region since Professor Liu and his colleague Professor Zhiyun Ouyang first visited it together in 2001 have been the increase and improvement in roads and other infrastructure," Pimm said.
"These have been the major factor in fragmenting the habitat.
There was nearly three times the density of roads in 2013 than in 1976."
"We suggest several solutions," Ouyang concluded.
"One of the most important will be to establish protected corridors through which pandas can move to prevent their isolation into small and unsustainable populations."
###

Funding came from the National Natural Science Foundation of China (grant #41671534); the Ministry of Science and Technology through the National Key Programme of Research and Development Project (grant #2016YFC0503200); the State Forestry Administration; the Worldwide Fund for Nature; and the U.S. National Science Foundation (grant #1340812).
CITATION: "Reassessing the Conservation Status of the Giant Panda Using Remote Sensing," by Weihua Xu, Andrs Via, Lingqiao Kong, Stuart L. Pimm, Jingjing Zhang, Wu Yang, Yi Xiao, Lu Zhang, Xiaodong Chen, Jianguo Liu and Zhiyun Ouyang.
Nature Ecology and Evolution, Sept. 18, 2017.
DOI: 10.1038/s41559-017-0317-1
Scientists demonstrate, and then rescue, muscle and nerve defects caused by removing brains of day-old frog embryos; reveal earliest known example of brain-body interface

MEDFORD/SOMERVILLE, Mass.
(Sept. 25, 2017) -- The brain plays an active and essential role much earlier than previously thought, according to new research from Tufts University scientists which shows that long before movement or other behaviors occur, the brain of an embryonic frog influences muscle and nerve development and protects the embryo from agents that cause developmental defects.
Remarkably, the brain performs these functions while it is itself still developing, marking the earliest known events of the brain-body interface.
In addition to identifying these essential instructive functions for the first time, the Tufts researchers successfully rescued defects caused by lack of a brain by using widely available, human-approved drugs.
The discoveries, reported in Nature Communications on Sept. 25, could expand understanding of human cognition and neuroplasticity and lead to better ways to address birth defects, treat injuries and regenerate or bioengineer complex organs.
Frogs are a widely used model in biomedical research because they share many basic biological mechanisms and processes with humans.
"Everyone knows that the brain guides behavior, but these data suggest that we need to revise our view of the brain as quiescent prior to an animal's independent activity.
Our research shows that the brain is engaged long before that, before it's even fully built.
What is particularly promising on the therapeutic side is that we were able to reverse developmental defects that result in the absence of a brain by applying relatively simple bioelectric and neurotransmitter manipulations," said the paper's corresponding author, Michael Levin, Ph.D., Vannevar Bush professor of biology and director of the Tufts Center for Regenerative and Developmental Biology and the AAllen Discovery Center at Tufts.
The Allen Discovery Center at Tufts focuses on reading and writing the morphogenetic code that orchestrates how cells communicate to create and repair complex anatomical shapes and includes researchers from Tufts, Harvard, Princeton, the University of Chicago and Tel Aviv University.
Rescuing "birth" defects

To examine the role of the brain during early development, the researchers removed the brains of Xenopus laevis frog embryos 27-1/2 hours after the eggs were fertilized, long before independent embryonic activity occurs.
Brainless embryos showed problems in three main areas.
Most obvious was abnormal development of the muscles and the peripheral nervous system.
Collagen density diminished, and muscle fibers were shorter and lacked the characteristic chevron patterning found in normal embryos.
Peripheral nerves also grew ectopically and chaotically throughout the trunk, revealing that even regions far away from the brain depend on its presence and activity for normal embryogenesis.
In addition, when exposed to chemicals that do not cause birth defects in normal embryos, embryos without brains developed severe deformities, such as bent spinal cords and tails.
These results demonstrated that the normal brain provides a protective effect against exposure to influences that without the brain's activity would act as potent teratogens.
Importantly, the researchers were able to rescue many of these defects by administering scopolamine, a drug used to regulate human neural function, or injecting messenger RNA encoding the HCN2 ion channel, which modulates bioelectric signals in many contexts and animals, including humans.
"Our data suggest that the brain exercises these functions using electrical and chemical channels that communicate locally and at a distance.
Such distributed communications means we may be able to repair damage in a difficult-to-reach site by providing therapies to more easily-accessible tissues.
Being able to treat one part of the body and see results in another part is particularly valuable in specialties like neuroregeneration," said the paper's first author, neuroscientist Celia Herrera-Rincon, Ph.D., a postdoctoral researcher in the Levin laboratory.
Future research will focus on decoding the specific information being sent through the newly identified communication channels from the brain, identifying other body structures that require brain presence, exploring relevance in other species, and honing the ability to provide brain-like signals in other contexts to improve complex patterning and tissue repair.
Levin is particularly fascinated by the question of how the brain, or any structure, can deliver information while it's still being built and whether other organs have similarly special roles.
"The brain and body form a feedback loop; the brain is being constructed by the embryo's patterning activities even as it itself is contributing instructive guidance to those processes -- a delicate balance between structure and function.
Explaining this could lead to understanding how brains keep memories during massive remodeling and regeneration.
We might one day be able to regenerate portions of the brain while the memories were still intact," he said.
"We have already found that the brain performs important functions at this stage of development, and my guess is this is only the tip of the iceberg."
###

In addition to Herrera-Rincon and Levin, paper authors are Vaibhav P. Pai, Ph.D., and Joan M. Lemire, Ph.D., research associates in the Levin laboratory, and Tufts undergraduate Kristine M. Moran, a rising senior who is pursuing a B.A.
in community health.
Work was supported by the Allen Discovery Center program through The Paul G. Allen Frontiers Group, the W. M. Keck Foundation, the G. Harold and Leila Y. Mathers Charitable Foundation, and the National Institute of Arthritis and Musculoskeletal and Skin Diseases of the National Institutes of Health (AR055993, AR061988).
Herrera-Rincon, C., Pai, V., Moran, K., Lemire, J., Levin, M., "The brain is required for normal muscle and nerve patterning during early Xenopus development," Nature Communications, Sept. 25, 2017, doi: 10.1038/s41467-017-00597-2.
This content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or other funders.
About Tufts University

Tufts University, located on campuses in Boston, Medford/Somerville and Grafton, Massachusetts, and in Talloires, France, is recognized among the premier research universities in the United States.
Tufts enjoys a global reputation for academic excellence and for the preparation of students as leaders in a wide range of professions.
A growing number of innovative teaching and research initiatives span all Tufts campuses, and collaboration among the faculty and students in the undergraduate, graduate and professional programs across the university's schools is widely encouraged.
A new study conducted in a cancer center in a state with legalized medicinal and recreational marijuana found that approximately one-quarter of surveyed patients used marijuana in the past year, mostly for physical and psychological symptoms.
Published early online in CANCER, a peer-reviewed journal of the American Cancer Society, the study also revealed that legalization also revealed that legalization increased the likelihood for use among patients.
Eight states and the District of Columbia have legalized recreational marijuana, and over half the states in the U.S. have passed laws allowing for medical marijuana in some form.
As availability and acceptance of marijuana use continue to increase, many cancer patients will have greater access to marijuana during their cancer treatment.
Marijuana is purported to alleviate symptoms related to cancer treatment, but patterns of use among cancer patients are not well known.
To investigate, Steven Pergam, MD, MPH, of the Fred Hutchinson Cancer Research Center and his colleagues surveyed 926 patients at the Seattle Cancer Center Alliance.
The team found that most patients had a strong interest in learning about marijuana during treatment and 74 percent wanted information from cancer care providers.
Sixty-six percent had used marijuana in the past, 24 percent used in the last year, and 21 percent used in the last month.
Most current users smoked or consumed marijuana primarily for physical symptoms (such as pain and nausea) or psychological reasons (such as coping with stress, depression, and insomnia).
The study reports that random analysis of patient urine samples showed that 14 percent had evidence of recent cannabis use, similar to the 18 percent of users who reported use within the past week.
Although nearly all respondents wanted more information directly from their doctors, most reported that they were more likely to get information from sources outside of the healthcare system.
"Cancer patients desire but are not receiving information from their cancer doctors about marijuana use during their treatment, so many of them are seeking information from alternate non-scientific sources," said Dr. Pergam.
He stressed that marijuana may be dangerous for some cancer patients or lead to unwanted side effects.
"We hope that this study helps to open up the door for more studies aimed at evaluating the risks and benefits of marijuana in this population.
This is important, because if we do not educate our patients about marijuana, they will continue to get their information elsewhere."
###

Additional Information

Full Citation: "Cannabis Use among Patients at a Comprehensive Cancer Center in a State with Legalized Medicinal and Recreational Use."
Steven A. Pergam, Maresa Woodfield, Christine M. Lee, Guang-Shing Cheng, Kelsey Baker, Sara R. Marquis, and Jesse R. Fann.
CANCER; Published Online: September 25, 2017 (DOI: 10.1002/cncr.30879).
URL Upon Publication: http://doi.
wiley.
com/ 10.
1002/ cncr.
30879

Author Contact: Claire Hudson, of the Communications & Marketing Office at the Fred Hutchinson Cancer Research Center, at crhudson@fredhutch.org or +1 (206) 667-7365.
About the Journal

CANCER is a peer-reviewed publication of the American Cancer Society integrating scientific information from worldwide sources for all oncologic specialties.
The objective of CANCER is to provide an interdisciplinary forum for the exchange of information among oncologic disciplines concerned with the etiology, course, and treatment of human cancer.
CANCER is published on behalf of the American Cancer Society by Wiley and can be accessed online at http://wileyonlinelibrary.
com/ journal/ cancer .
Follow us on Twitter @JournalCancer and Facebook https:/ / www.
facebook.
com/ ACSJournals

About Wiley
Organ donation in Ontario increased 57% since 2006 when the province introduced a Canadian policy that allows donation of organs after circulatory functions cease, called circulatory determination of death (DCD), according to a new study published in CMAJ (Canadian Medical Association Journal) http://www.
ca/ lookup/ doi/ 10.
1503/ cmaj.
161043

Before 2006, deceased organ donation traditionally occurred after neurologic determination of death (NDD), commonly known as brain death, when a person was declared dead after the complete and irreversible loss of all brain functions.
Because of a lack of organ donors and other factors, waiting lists for donations of lungs, kidneys, livers and hearts are long and recipients often die while waiting, depending on the type of organ.
"The most important development in efforts to expand the donor pool has involved donation after [DCD]," writes Dr. Vivek Rao, Multiorgan Transplant Unit, Toronto General Hospital and the University of Toronto, Toronto, Ontario, with coauthors.
Researchers compared data from the pre-DCD period (2002/03 to 2005/06), early DCD period (2006/07 to 2009/10) and recent DCD period (2010/11 to 2013/14) to understand trends in organ donation after introducing the policy.
"Donation after circulatory determination of death has had a positive effect in Ontario in terms of both overall number of donors and transplant activity," state the authors.
"Donation after NDD does not appear to have been adversely affected.
Although there are disparities among organ groups, we foresee that an active DCD program will continue to have a positive effect for all solid-organ transplant recipients," they conclude.
In a related commentary http://www.
ca/ lookup/ doi/ 10.
1503/ cmaj.
170988 , Dr. Sam Shemie of Montreal Children's Hospital and McGill University Health Centre & Research Institute, and medical advisor for Deceased Organ Donation, Canadian Blood Services, says that the research paper shows Ontario has seen a rise in numbers of transplants in the province over a 12-year period that is almost entirely attributable to DCD.
This important finding is instructive for the rest of the country.
Substantial variation in organ donation rates between provinces still exists and can be explained largely by variable degree of DCD implementation.
However, according to Dr. Shemie, the 2015 rate of 18.2 donors per million people falls far below the number of potential donors, estimated at 40-89 donors per million.
Canada must continue to focus on increasing organ donation and preventing death and disability for potential transplant candidates.
The research study was conducted by researchers from Toronto General Hospital, University Health Network, St. Michael's Hospital, the University of Toronto, Trillium Gift of Life Network, Toronto, Ontario; and Children's Hospital of Eastern Ontario, Ottawa, Ontario;

###
Using a computational technique that accounts for how genes interact, scientists revealed genes that may be related to autism spectrum disorder

The identification of genes related to autism spectrum disorder (ASD) could help to better understand the disorder and develop new treatments.
While scientists have found many genetic differences in different people with ASD, these often show little overlap and don't appear to be related.
Using a new technique that accounts for how genes interact, Italian researchers have identified new networks of related genes that may be involved in ASD - including genes that are related to cancer.
Autism spectrum disorder encompasses a range of neurodevelopmental disorders, and includes conditions like autism and Asperger's syndrome.
ASD symptoms vary significantly, but typically start within the first three years of life and include repetitive behaviors, and difficulty communicating and socializing.
Autism spectrum disorder has a significant genetic component, and scientists have found thousands of genetic differences between some people with ASD and those without.
Researchers suspect that many of these genetic anomalies can predispose people to the disorder, or directly contribute to it.
However, ASD is complex, and researchers have only begun to unravel its genetic basis.
One problem is that many studies have found completely different genetic anomalies in different people with ASD, which show little overlap and don't appear to be related.
This makes it difficult to find common hallmarks that could provide better clues to understanding and treating the disorder.
While this genetic variability demonstrates that ASD is incredibly complex, a new technique could help to cast more light on the situation.
When a cell in your body expresses a gene, it produces a specific protein.
These proteins interact to form complex signaling pathways that can have wide-reaching effects.
Different genes could affect the same pathway, meaning that the different genetic anomalies found in people with ASD could potentially all be affecting similar pathways.
By accounting for how genes interact, rather than just looking at individual genes, scientists might potentially spot biological hallmarks of ASD they might otherwise miss.
In a study recently published in Frontiers in Genetics, researchers in Italy used a new computational technique to do just that.
Searching public databases, the team investigated genes that previous studies have associated with ASD.
They used another database listing interactions between different proteins, to narrow down the list of genes and account for interactions between them.
Using a computational technique called network diffusion, the team identified networks of genes that are interrelated through their connection to the ASD genes in the databases.
They also investigated if the genes were involved in any known signaling pathways.
So, what do these genes do?
Some of the ASD genes in the networks are involved in brain function and how neurons develop and transmit information.
Others are involved in conditions that tend to occur alongside ASD, such as psychiatric disorders and epilepsy, and interestingly, some of the genes are also involved in cancer.
The team also identified genes that had not been previously linked to ASD, but are heavily involved in numerous protein interactions and signaling pathways.
Using their data, the team constructed complex pathway maps that could provide clues about ASD and potential treatments.
For example, many of the genes in the new networks are related to cancer, suggesting that certain cancer treatments that target these genes might also be useful to treat ASD.
The team's computational technique can also be used to learn about other conditions.
"The computational method we have proposed can be applied to other data-sets to predict new genes involved in other conditions," says Alessandra Mezzelani, a researcher involved in the study.
"We hope that global gene databases will continue to grow, allowing scientist to share and reuse these types of data, and we will update our model as more ASD risk genes are discovered."
says Ettore Mosca, who was also involved.
SAN DIEGO, September 24, 2017 - Combining radiation therapy with chemotherapy for patients with limited metastatic non-small cell lung cancer (NSCLC) may curb disease progression dramatically when compared to NSCLC patients who only receive chemotherapy, according to a new randomized phase II clinical trial reported today at the 59th Annual Meeting of the American Society for Radiation Oncology (ASTRO).
Progression-free survival in the trial escalated from 3.5 months to 9.7 months with the addition of radiation therapy delivered to all the metastatic sites of lung cancer as well as the primary disease site.
Treatment-related side effects were similar for the two treatment approaches.
Lung cancer claims the most cancer-specific deaths of any tumor type.
Few existing treatments offer durable survival benefits for patients whose NSCLC has spread past the lungs, due in part to the aggressive nature of lung cancer and its propensity to progress, even following treatment.
Research on metastatic colorectal cancer and sarcoma, however, suggests a potential benefit from adding local therapy--treatment directed specifically at the tumor cells--to the standard approach of systemic therapy.
In these studies, adding radiation or/and surgery bolstered the ability of systemic therapies, such as chemotherapy, to control disease and improve survival in patients with few metastases.
"Even in the era of immunotherapy, there are not large numbers of metastatic NSCLC patients with durable responses to systemic therapy.
In our trial, however, the addition of radiation therapy directed at each of the cancerous areas greatly improved how patients responded to subsequent rounds of chemotherapy," said Puneeth Iyengar, MD, PhD, lead author of the study and an assistant professor of radiation oncology at the University of Texas Southwestern Medical Center in Dallas.
"This finding suggests that local treatments, including radiation, could work in concert with chemotherapy to prolong the amount of time before recurrence occurs in patients with limited sites of metastatic NSCLC."
The study was a randomized phase II trial testing whether the addition of local treatment, in the form of consolidative radiation therapy, to the standard treatment of systemic therapy improved progression-free survival for patients with limited metastatic NSCLC.
Eligible patients included those with stage IV disease, spread to six or fewer sites including the primary tumor site, and who responded at least partially to first-line/induction chemotherapy.
Patients were randomly assigned to receive either maintenance chemotherapy alone (15 patients) or a combination of stereotactic ablative radiotherapy (SAbR)--also known as stereotactic body radiation therapy or SBRT--to all sites of disease followed by maintenance chemotherapy (14 patients).
Radiation to metastases was offered as a single fraction (to 21-27 Gray (Gy)), three fractions (to 26.5-33 Gy) or five fractions (to 30-37.5 Gy) of SAbR (regimens were biologically equivalent).
Radiation to the primary disease site was delivered via SAbR where possible, or through 15 fractions of hypofractionated radiation therapy if the primary tumor was too central or involved mediastinal nodes.
Maintenance chemotherapy was left to the discretion of the treating medical oncologists and consisted of pemetrexed, docetaxel, erlotinib or gemcitabine.
Twenty-nine patients were accrued between April 2014 and July 2016.
The median patient age was 70 years (range 51-79 years) for the patients receiving maintenance chemotherapy only and 63.5 years (range 51-78) for the patients receiving SAbR to metastases followed by maintenance chemotherapy.
Most patients were male (69%).
Eighty-six percent of all patients had tumors with non-squamous histologies.
Thirty-one lesions were treated with radiation in the 14 patients that received local therapy.
The median follow-up for this report was 9.6 months (range 2.4-30.2 months).
Patient accrual was stopped ahead of schedule after an unplanned interim analysis found substantially improved survival rates in the arm receiving local therapy, matching similar findings in a parallel trial.
The interim analysis found a median progression-free survival rate of 9.7 months with consolidative radiation therapy followed by chemotherapy, versus 3.5 months for maintenance chemotherapy alone (p = 0.01; Hazard Ratio (HR) = 0.304, 95% CI 0.113-0.815).
Survival rates were estimated using the Kaplan-Meier method and compared using the log-rank test and Cox proportional hazard models.
Specifically, rates of local control and delay in distant metastases also favored the approach incorporating radiation with systemic therapy.
In the arm with consolidative local therapy, there were no recurrences in original sites of gross disease versus seven failures in original sites of gross disease in the arm receiving only maintenance therapy.
At the time of analysis, 10 of the 15 patients receiving maintenance chemotherapy-only had progressed, compared with four of the 14 patients also receiving radiation.
None of the recurrences among the latter patients were in areas treated directly with radiation therapy.
Treatment-related side effects were similar between the two treatment arms, indicating that the addition of local therapy was well-tolerated by patients.
There were no grade 5 toxicities attributable to study treatment.
On the maintenance chemotherapy-only arm, there were two grade 3 and one grade 4 toxicities.
On the SAbR plus maintenance chemotherapy arm, there was one grade 4 toxicity.
"These findings verify that progression-free survival for limited metastatic disease really is no different than it is for widely metastatic disease, suggesting that local therapy could play an important future role in survival outcomes," said Dr. Iyengar.
"Moreover, the addition of consolidative radiation did not increase toxicity, which allowed patients to continue on to additional systemic therapy that is important to controlling aggressive metastatic disease."
Next steps for this research include a larger, randomized phase III trial to test progression-free survival, as well as overall survival.
While results indicate a clear benefit of adding local therapy for the management of limited metastatic NSCLC, Dr. Iyengar stressed the need for confirmation in a larger prospective trial.
"There is a significant possibility that local therapy, such as consolidative radiation, may become an important part of the management of limited metastatic NSCLC patients, but this validation must take place in randomized phase III studies.
Interested patients should seek more information about the ongoing NRG LU 002 and SARON trials."
###

The abstract, "Consolidative radiotherapy for limited metastatic non-small cell lung cancer (NSCLC): A randomized phase II trial," will be presented in detail during a news briefing and the Plenary Session at ASTRO's 59th Annual Meeting in San Diego (full details below).
The study is also available beginning today in JAMA Oncology.
To schedule an interview with Dr. Iyengar and/or outside experts in lung cancer, contact ASTRO's media relations team on-site at the San Diego Convention Center September 24 through 27, by phone at 703-286-1600 or by email at press@astro.org.
ATTRIBUTION TO THE AMERICAN SOCIETY OF RADIATION ONCOLOGY (ASTRO) ANNUAL MEETING REQUESTED IN ALL COVERAGE.
This news release contains additional and/or updated information from the study author(s).
Full original abstract and author disclosures available from press@astro.org or at http://www.
astro.
org/ annualmeeting .
Study Presentation Details

News Briefing: Sunday, September 24, 1:00 - 2:00 p.m. Pacific time, San Diego Convention Center, room 24C, webcast: http://www.
do/ astro17-1

Scientific Session: Plenary, Monday, September 25, 2:15 - 3:45 p.m. Pacific time, San Diego Convention Center, Ballroom 20

Digital brochure: Radiation Therapy for Lung Cancer (Spanish version)

Videos: Radiation Therapy for Lung Cancer (Spanish version), An Introduction to Radiation Therapy (Spanish version)

ASTRO's clinical practice statements and guidelines

Additional brochures, videos and information on radiation therapy from ASTRO's patient site, RTAnswers.org

ABOUT ASTRO'S ANNUAL MEETING

ASTRO's 59th Annual Meeting, the world's largest scientific meeting in radiation oncology, will be held September 24-27, 2017, at the San Diego Convention Center.
The 2017 Annual Meeting is expected to attract more than 11,000 attendees from across the globe, including oncologists from all disciplines and members of the entire radiation oncology team.
More than 2,800 abstracts sharing results from clinical trials and other research studies will be presented in conjunction with educational sessions and keynote addresses that underscore the meeting's theme, "The Healing Art and Science of Radiation Oncology."
Led by ASTRO President Brian Kavanagh, MD, MPH, FASTRO, the 2017 meeting will feature keynote addresses from Richard D. Zane, MD, FAAEM, Chief Innovation Officer for the University of Colorado Health System; Lucy Kalanithi, MD, FACP, widow of Paul Kalanithi, MD, the best-selling author of "When Breath Becomes Air," with Heather Wakelee, MD, Paul's oncologist; and Vinay K. Prasad, MD, MPH, an assistant professor of medicine at the Oregon Health & Science University.
During the four-day meeting, more than 200 exhibitors will demonstrate cutting-edge technology and medical device innovations for radiation oncology.
Visit us online for more information about ASTRO's 59th Annual Meeting or press opportunities at the meeting.
ABOUT ASTRO
SAN DIEGO, September 24, 2017 -- A new study of patients at an urban cancer center points to a potentially serious problem that may limit the impact of clinical cancer care--undiagnosed depression.
Among the 40 percent of patients at the center who were diagnosed with depression, three in four had not previously been told they were depressed.
Female patients and disabled patients also were more likely to be depressed, according to research presented today at the 59th Annual Meeting of the American Society for Radiation Oncology (ASTRO).
A web of physiological, psychological and socioeconomic factors associated with having and being treated for cancer--such as severe physical pain, side effects of medication, financial concerns, fears and anxieties about mortality and changes in lifestyle and life plans--place cancer patients at a higher risk of developing depression.
The National Cancer Institute (NCI) estimates that 15 to 25 percent of the general cancer patient population has depression--a rate two to three times that of the general population.
Approximately seven percent of U.S. adults experience major depression, according to the National Institutes for Mental Health (NIMH).
"Depression prevalence continues to be high among cancer patients, especially those receiving treatment at an urban cancer center, as well as those who identify as female or are disabled by their disease.
Alarmingly, most of these patients remain undiagnosed and untreated, indicating an important gap in cancer care and an opportunity to improve patient outcomes," said Jason Domogauer, PhD, lead author of the study and an MD/PhD candidate at Rutgers University New Jersey Medical School in Newark, New Jersey.
Findings in this study are based on an examination of 400 cancer patients who received treatment at the University Hospital Cancer Center in Newark, New Jersey, between 2013 and 2016.
Researchers assessed depression using a minimum score of 16 on the Center for Epidemiologic Studies Depression Scale.
The average patient age was 55 years (range 20-86 years), and 53 percent of patients were female.
The racial/ethnic breakdown of patients was 48 percent African-American, 29 percent non-Hispanic white and 16 percent Hispanic.
Nearly equal numbers of patients reported being able to work (including those working full-time, part-time and unemployed) (49%) or being unable to work due to disability (51%).
Most patients (85%) received radiation therapy as part of their cancer treatment.
Depression was diagnosed in 40 percent of the patients at this urban cancer center, which is roughly twice as large as the NCI estimate of 15 to 25 percent for the general cancer patient population.
Moreover, depression was previously undiagnosed in three-fourths of these cases, meaning that roughly 30 percent of the patients at this cancer center suffered from undiagnosed and untreated depression.
Depression was more common among female patients (47%) than among male patients (32%) (odds ratio [OR] 1.9, p = 0.007) and among patients who were unable to work due to disability (48% vs. 33% of those able to work; OR 1.9, p = 0.005).
NIMH statistics for the general U.S. adult population also point to a higher risk of depression for women and people with a disability, particularly among older adults.
Depression prevalence did not differ significantly among racial/ethnic groups.
Logistic regression was used to compare depression prevalence among the patient groups.
Looking specifically at patients who were previously not diagnosed with depression, the effects of being female or unable to work persisted.
Among this subgroup, depression was more common among women (43% vs. 29% male; OR 1.9, p = 0.02) and disabled patients (43% vs. 31% able to work; OR 1.9, p = 0.03).
"Depression is widely recognized as an underdiagnosed disorder, particularly among older adults and cancer patients.
Our findings point to a clear need for action including depression screening during initial and continuing patient visits, initiation of mental health treatments for identified patients and increased collaboration with mental health providers in cancer treatment centers.
These efforts are particularly important for patients in urban centers, those who are female and those who are unable to work because of their disease," said Dr. Domogauer.
###

The abstract, "Study of total and undiagnosed depression in a cancer patient population at an urban cancer center," will be presented in detail during a news briefing and an oral abstract session at ASTRO's 59th Annual Meeting in San Diego (full details below).
To schedule an interview with Dr. Domogauer and/or outside experts, contact ASTRO's media relations team on-site at the San Diego Convention Center, September 24 through 27, by phone at 703-286-1600 or by email at press@astro.org.
ATTRIBUTION TO THE AMERICAN SOCIETY OF RADIATION ONCOLOGY (ASTRO) ANNUAL MEETING REQUESTED IN ALL COVERAGE.
This news release contains additional and/or updated information from the study author(s).
Full original abstract and author disclosures available from press@astro.org or at http://www.
astro.
org/ annualmeeting .
Study Presentation Details

Scientific Session: Sunday, September 24, 1:15 - 2:45 p.m. Pacific time, San Diego Convention Center, room 7A/B

News Briefing: Tuesday, September 26, 1:00 - 2:00 p.m. Pacific time, San Diego Convention Center, room 24C, webcast: http://www.
do/ astro17-3

Resources on Cancer and Radiation Therapy

Video: An Introduction to Radiation Therapy (Spanish version)

Additional brochures, videos and information on radiation therapy from RTAnswers.org

ASTRO's clinical practice statements and guidelines

ABOUT ASTRO'S ANNUAL MEETING

ASTRO's 59th Annual Meeting, the world's largest scientific meeting in radiation oncology, will be held September 24-27, 2017, at the San Diego Convention Center.
The 2017 Annual Meeting is expected to attract more than 11,000 attendees from across the globe, including oncologists from all disciplines and members of the entire radiation oncology team.
More than 2,800 abstracts sharing results from clinical trials and other research studies will be presented in conjunction with educational sessions and keynote addresses that underscore the meeting's theme, "The Healing Art and Science of Radiation Oncology."
Led by ASTRO President Brian Kavanagh, MD, MPH, FASTRO, the 2017 meeting will feature keynote addresses from Richard D. Zane, MD, FAAEM, Chief Innovation Officer for the University of Colorado Health System; Lucy Kalanithi, MD, FACP, widow of Paul Kalanithi, MD, the best-selling author of "When Breath Becomes Air," with Heather Wakelee, MD, Paul's oncologist; and Vinay K. Prasad, MD, MPH, an assistant professor of medicine at the Oregon Health & Science University.
During the four-day meeting, more than 200 exhibitors will demonstrate cutting-edge technology and medical device innovations for radiation oncology.
Visit us online for more information about ASTRO's 59th Annual Meeting or press opportunities at the meeting.
ABOUT ASTRO
Phase III trial demonstrates 32 percent overall survival rate at five years following standard-dose treatment for stage III NSCLC; trial also finds no additional benefit from cetuximab

SAN DIEGO, September 24, 2017 -- Long-term results of a phase III clinical trial indicate that survival rates for patients receiving chemoradiation for unresectable, locally advanced non-small cell lung cancer (NSCLC) may be more than twice as high as previous estimates.
At five years following treatment with a standard dose of 60 Gray (Gy) radiation delivered in 30 fractions, the overall survival rate was 32 percent, setting a new benchmark of survival for patients with inoperable stage III NSCLC.
The trial, RTOG 0617, also confirms that a standard dose of radiation therapy is preferable to a higher dose and that cetuximab offers no additional survival benefit for these patients.
Findings will be presented today at the 59th Annual Meeting of the American Society for Radiation Oncology (ASTRO) in San Diego.
Lung cancer claims the most cancer-specific deaths of any tumor type, both in the United States and worldwide.
The American Cancer Society, using data from the National Cancer Institute, estimates five-year survival rates of five to 14 percent for patients who present with stage III NSCLC.
RTOG 0617 was a phase III randomized trial that compared a standard radiation therapy dose of 60 Gy in 30 fractions with a higher dose of 74 Gy in 37 fractions for patients receiving concurrent chemotherapy with or without cetuximab for inoperable stage III lung cancer.
"Based on the two-year results reported in 2015, RTOG 0617 has already changed practice and established the standard radiation dose for patients receiving chemoradiation for stage III NSCLC," said Jeffrey D. Bradley, MD, FASTRO, the principal investigator of the RTOG 0617 trial and a professor of radiation oncology and director of the S.L.
King Center for Proton Therapy at the Washington University School of Medicine in St. Louis.
"When RTOG 0617 was initially reported, the results were surprising to most oncologists because the standard dose of 60 Gy was superior to the higher dose of 74 Gy in this setting.
There have been numerous secondary analyses investigating the reasons for this result, and the data point toward greater radiation exposure to the heart in the high-dose arm being the main problem.
"The current report establishes an overall five-year survival standard for patients receiving standard-dose chemoradiation for stage III NSCLC that is substantially higher than previously estimated.
This report also confirms that using a higher radiation dose is not beneficial and can lead to detrimental outcomes including lower survival rates and increase side effects."
Patients enrolled in RTOG 0617 were randomized to one of two chemoradiation dose groups.
Standard-dose treatment consisted of 60 Gy total radiation dose, and high-dose patients received 74 Gy total dose.
Radiation was delivered in 2 Gy daily fractions through either intensity-modulated radiation therapy (IMRT) or three-dimensional conformal radiation therapy (3-D CRT).
All patients received concurrent weekly chemotherapy with paclitaxel and carboplatin.
Patients also were randomized to receive either cetuximab or a placebo.
Full information on the trial design is available on the RTOG website.
Across the 185 institutions in the United States and Canada that participated in RTOG 0617, a total of 544 patients with unresectable stage III NSCLC were accrued and 496 were eligible for analysis.
The median patient age was 64 years (interquartile range [IQR] 57-70 years).
Most patients were male (59%) and white (41%).The median follow-up for surviving patients was 5.1 years (IQR 4.6 - 6 years).
Survival rates at five years following chemoradiation were higher for patients in the standard-dose treatment arm than for those in the high-dose arm.
Median overall survival (OS) following standard-dose treatment was 28.7 months (95% CI 24 - 38.4 months), compared with 20.3 months (95% CI 18-24 months) for the high-dose cohort (hazard ratio [HR] 1.35, p = 0.004).
Five-year OS rates were 32.1 percent and 23 percent for the standard-dose and high-dose arms, respectively (p = 0.004).
Progression-free survival (PFS) rates at five years were 18.3 percent and 13 percent for the standard-dose and high-dose arms, respectively (p = 0.055).
Survival rates were estimated using the Kaplan-Meier method and compared using the log-rank test and Cox proportional hazard models.
On multivariate analysis, differences in OS were driven by radiation dose (favoring the standard-dose regimen; p = 0.03), planning target volume (p = 0.022), the accrual volume of the treating institution (p = 0.017), presence of esophagitis/dysphagia (p = 0.008) and V5 heart dose/volume (p = 0.005).
Patterns of tumor recurrence, either in the same location or region as the initial tumor or further away, also favored the standard-dose regimen, although differences between the treatment arms did not reach statistical significance.
Failure rates for the standard-dose and high-dose arms, respectively, were as follows: local failure in 38.2 percent versus 45.7 percent (p = 0.068); regional failure in 35.7 percent versus 38.4 percent (p = 0.5); and distant failure in 52.3 percent versus 57.6 percent (p = 0.3).
Failure rates were estimated using the cumulative incidence method and evaluated through Fine-Gray models.
Treatment-related side effects were more pronounced with the high-dose regimen.
There were three treatment-related deaths in the standard-dose arm, compared with nine in the high-dose arm.
Rates of treatment-related grade 3 or higher toxicity for the standard-dose and high-dose arms, respectively, were as follows: dysphagia in 3.2 percent versus 12.1 percent (p < 0.0001); esophagitis in 5.0 percent versus 17.4 percent (p < 0.0001); and severe pulmonary events in 20.6 percent versus 19.3 percent (p > 0.05).
Cetuximab (delivered as a 400-milligram dose on day one, then 250-milligram doses weekly thereafter) did not confer a benefit for overall survival at five years.
Median OS for patients who received cetuximab was 24 months (95% CI 20.4 - 30 months), compared with 24 months (95% CI 20.5 - 28.8 months) for those who did not (HR 1.0, p = 0.048).
Moreover, there was no benefit of cetuximab for patients with epidermal growth factor receptor (EGFR) H-scores above 200, counter to earlier findings from the trial.
###

The abstract, "Long-term results of RTOG 0617; a randomized phase III comparison of standard dose versus high dose conformal chemoradiotherapy +/- cetuximab for stage III NSCLC," will be presented in detail during a news briefing and an oral abstract session at ASTRO's 59th Annual Meeting in San Diego (full details below).
To schedule an interview with Dr. Bradley and/or outside experts in lung cancer, contact ASTRO's media relations team on-site at the San Diego Convention Center, September 24 through 27, by phone at 703-286-1600 or by email at press@astro.org.
ATTRIBUTION TO THE AMERICAN SOCIETY OF RADIATION ONCOLOGY (ASTRO) ANNUAL MEETING REQUESTED IN ALL COVERAGE.
This news release contains additional and/or updated information from the study author(s).
Full original abstract and author disclosures available from press@astro.org or at http://www.
astro.
org/ annualmeeting .
Study Presentation Details

* News Briefing: Sunday, September 24, 1:00 - 2:00 p.m. Pacific time, San Diego Convention Center, room 24C, webcast: http://www.
do/ astro17-1

* Scientific Session: Tuesday, September 26, 4:45 - 6:15 p.m. Pacific time, San Diego Convention Center, room 7A/B

Resources on Lung Cancer and Radiation Therapy

* Videos: Radiation Therapy for Lung Cancer (Spanish version), An Introduction to Radiation Therapy (Spanish version)

* Digital brochure: Radiation Therapy for Lung Cancer (Spanish version)

* Additional brochures, videos and information on radiation therapy from ASTRO's patient site, RTAnswers.org

* ASTRO's clinical practice statements and guidelines

ABOUT ASTRO'S ANNUAL MEETING

ASTRO's 59th Annual Meeting, the world's largest scientific meeting in radiation oncology, will be held September 24-27, 2017, at the San Diego Convention Center.
The 2017 Annual Meeting is expected to attract more than 11,000 attendees from across the globe, including oncologists from all disciplines and members of the entire radiation oncology team.
More than 2,800 abstracts sharing results from clinical trials and other research studies will be presented in conjunction with educational sessions and keynote addresses that underscore the meeting's theme, "The Healing Art and Science of Radiation Oncology."
Led by ASTRO President Brian Kavanagh, MD, MPH, FASTRO, the 2017 meeting will feature keynote addresses from Richard D. Zane, MD, FAAEM, Chief Innovation Officer for the University of Colorado Health System; Lucy Kalanithi, MD, FACP, widow of Paul Kalanithi, MD, the best-selling author of "When Breath Becomes Air," with Heather Wakelee, MD, Paul's oncologist; and Vinay K. Prasad, MD, MPH, an assistant professor of medicine at the Oregon Health & Science University.
During the four-day meeting, more than 200 exhibitors will demonstrate cutting-edge technology and medical device innovations for radiation oncology.
Visit us online for more information about ASTRO's 59th Annual Meeting or press opportunities at the meeting.
ABOUT ASTRO
The scientist from the Peoples' Friendship University of Russia (PFUR), Centre National de la Recherche Scientific (France) and the University of Leicester (United Kingdom) has shown how the wealth of a country relates to its migration rates.
A new mathematical model has laid the basis for future research in this field.
The study was published in the Nonlinear Analysis journal.
The results were presented at the VIII International Conference on Differential and Functional Differential Equations DFDE-2017 held on August 13-20 in the PFUR.
Scientists are making increasingly gloomy forecasts for the future because the world population is growing uncontrollably and available resources are limited.
In order to make more accurate predictions they create mathematical models on the basis of different data - the birth rate of the population, its mortality, and the rates of migration from one region to another.
People move from place to place depending on various economic, cultural and political factors.
This movement is certainly more complicated than animal migration.
However, they change locations due to similar factors - richer resources and better quality of life in a new place.
Therefore, scientists used mathematical approaches designed to describe the movement of animal populations as a starting point for modeling population dynamics in different countries.
Researchers have analyzed how the distribution of wealth changes due to migration.
Scientists understand wealth as the amount of material goods and services people consume in different countries and whether their needs are satisfied.
Using a system of partial differential equations specialists have designed a mathematical model that describes migration both in random directions and in places where the greatest amount of resources necessary for well-being is concentrated.
"It is well known that production and consumption of wealth depend on the population density, while birth and death rates depend on the welfare level," said Vitaly Volpert, one of the authors of the study, senior CNRS (Centre National de la Recherche Scientific) researcher from the Institute of Camille Jordan at the University of Lyon 1 and a visiting professor and senior researcher from the Peoples' Friendship University of Russia.
"The objectives of the study were not only to define the correlation between these processes, but also to find out the regularity of distribution of population and wealth in various conditions."
Modeling also took into account the ways wealth moves from developed countries to poor regions: through trade, investment, production transfer from one country to another, and so on.
The new model linking population dynamics and wealth distribution will boost future research in this field.
"We expect mathematical models to help us face the challenges posed by migration in the global economy," Volpert concluded.
Infrared light provides valuable temperature data to forecasters and cloud top temperatures give clues about highest, coldest, strongest storms within a hurricane.
NASA's Aqua satellite provided that data and showed strongest storms were in Hurricane Maria's southwestern quadrant.
On Sept. 24 at 2:35 a.m. EDT (0635 UTC) the Moderate Resolution Imaging Spectroradiometer or MODIS instrument aboard NASA's Aqua satellite analyzed Maria's cloud top temperatures in infrared light.
MODIS found cloud top temperatures of strong thunderstorms in Maria's quadrant as cold as or colder than minus 80 degrees Fahrenheit (minus 62.2 Celsius).
Cloud top temperatures that cold indicate strong storms that have the capability to create heavy rain.
NOAA Hurricane Hunter aircraft showed that Maria now has a 30 nautical mile wide eye and indicated that air pressure within had dropped to 948 millibars.
On Sept. 23 at 11:20 a.m. EDT (1520 UTC) the MODIS instrument aboard NASA's Terra satellite captured a visible image of Hurricane Maria and it's 30 nautical mile wide eye, when the storm was located east of the Bahamas.
The National Hurricane Center has noted that Maria has taken slightly more of a westward jog, and cautioned "Interests along the Carolina and Mid-Atlantic coasts should monitor the progress of Maria.
Tropical storm or hurricane watches may be needed for a portion of the coast later today, Sunday, Sept.
At 5 a.m. EDT on Sunday, September 24, 2017, the eye of Hurricane Maria was located near 27.9 degrees north latitude and 72.7 degrees west longitude.
That's about 530 miles (855 km) south-southeast of Cape Hatteras, North Carolina.
Maria was moving toward the north near 9 mph (15 kph), and this general motion is expected to continue through Monday.
Reports from a NOAA Hurricane Hunter aircraft indicate that maximum sustained winds are now near 110 mph (175 kph) with higher gusts.
Maria is a category 2 hurricane on the Saffir-Simpson Hurricane Wind Scale.
Some fluctuations in intensity are likely during the next day or so.
Hurricane-force winds extend outward up to 60 miles (95 km) from the center and tropical-storm-force winds extend outward up to 240 miles (390 km).
Fluctuations in intensity appear likely during the next 24 to 36 hours as Maria remains over warm water and in an environment of light or moderate vertical wind shear.
After that time, the hurricane is likely to encounter the colder water left by Hurricane Jose, which should cause a weakening trend.
On the forecast track, the core of Maria will be moving well east of the United States southeast coast during the next two days.
Maria continues to generate dangerous ocean conditions.
Swells generated by Maria are increasing along portions of the southeastern United States coast and Bermuda and will be increasing along the Mid-Atlantic coast later today.
Swells also continue to affect Puerto Rico, the Virgin Islands, the northern coast of Hispaniola, the Turks and Caicos Islands, and the Bahamas.
These swells are likely to cause life-threatening surf and rip current conditions.
A Dartmouth-led study has demonstrated how the latest aerial thermal imagery is transforming archaeology due to advancements in technology.
Today's thermal cameras, commercial drones and photogrammetric software has introduced a new realm of possibilities for collecting site data.
The findings, published in Advances in Archaeological Practice, serve as a manual on how to use aerial thermography, as the co-authors hope to inspire other researchers to apply this methodology in their work.
Archaeologists have long used thermal infrared images to locate buried architecture and other cultural landscape elements.
The thermal infrared radiation associated with such archaeological features depends on several variables, including the make-up of the soil, its moisture content and vegetation cover.
Past conventional geophysics methods, such as fieldwalking, enabled archaeologists to obtain field data across one hectare of a site per day.
But now, aerial thermography makes it possible to gather field survey data across a much larger area in much less time.
New aerial thermography has other advantages, as well.
Older cameras were unable to record full spectrum data or temperature data for every pixel of an image.
Today's radiometric thermal cameras coupled with small inexpensive, easy to fly drones, which can be controlled by a smartphone or tablet, have made aerial thermography more accurate, comprehensive and accessible.
Mapping multiple aerial images together has also become easier through new photogrammetric software, which automatically aligns images and features ortho-image capabilities, which corrects an image to make the scale uniform.
The researchers conducted case studies at six archaeological sites in North America, the Mediterranean and the Middle East, to assess the effectiveness of aerial thermal surveys.
They analyzed how weather, environment, time of day, ground cover, and archaeological features may affect the results, and compared their findings to earlier research and historical images.
For example, at an ancestral Pueblo settlement in Blue J, N.M, the researchers were able to map detailed architectural plans of a dozen ancient house compounds-- a discovery enabled by the site's optimal conditions, the soil matrix, low density ground cover, and the environmental conditions at the time of the aerial thermography.
They were also able to recognize traces of long-removed historic buildings and pathways at the Shaker Village in Enfield, N.H.

"A lot of what we've learned from our research to date shows how much local environmental conditions and the timing of surveys can impact how well thermal imagery will reveal archaeological remains.
Yet, the more we understand these issues, the better we are able to deploy the technology.
I think our results demonstrate aerial thermography's potential to transform how we explore archaeological landscapes in many parts of the world," says Jesse Casana, an associate professor of anthropology at Dartmouth, who has been using drones in aerial thermography for five years in his archaeological research.
Casana is available for comment at: jesse.j.casana@dartmouth.edu.
Scientists from the University of Exeter studied how guppies behaved in various situations, and found complex differences between individuals.
The researchers tested whether differences could be measured on a "simple spectrum" of how risk-averse or risk-prone guppies were.
But they found variations between individuals were too complicated to be described in this way.
"The idea of a simple spectrum is often put forward to explain the behaviour of individuals in species such as the Trinidadian guppy," said Dr Tom Houslay, of the Centre for Ecology and Conservation (CEC) on the University of Exeter's Penryn Campus in Cornwall.
"But our research shows that the reality is much more complex.
"For example, when placed into an unfamiliar environment, we found guppies have various strategies for coping with this stressful situation - many attempt to hide, others try to escape, some explore cautiously, and so on.
"The differences between them were consistent over time and in different situations.
So, while the behaviour of all the guppies changed depending on the situation -- for example, all becoming more cautious in more stressful situations -- the relative differences between individuals remained intact."
The study, published in the journal Functional Ecology, examined the "coping styles" of guppies in conditions designed to cause varying levels of stress.
Mild stress was caused by transferring fish individually to an unfamiliar tank, and higher levels of stress were caused by adding models of predatory birds or fish.
The presence of predators had an effect on "average" behaviour -- making all of the guppies more cautious overall -- but individuals still retained their distinct personalities.
Professor Alastair Wilson, also from the CEC at the University of Exeter, added: "We are interested in why these various personalities exist, and the next phase of our research will look at the genetics underlying personality and associated traits.
"We want to know how personality relates to other facets of life, and to what extent this is driven by genetic -- rather than environmental -- influences.
"The goal is really gaining insight into evolutionary processes, how different behavioural strategies might persist as species evolve."
The paper is entitled: "Testing the stability of behavioural coping style across stress contexts in the Trinidadian guppy."
A completely new group of sponges has been discovered, which scientists believe could be a key indicator species in measuring future mining impact in a region targeted for deep-sea mining of polymetallic (metal-rich) nodules.
They are likely to be the most abundant nodule-dwelling animal in the area.
The new discovery, described in the journal Systematics and Biodiversity, was made in the vast 5 million square kilometre region of the central Pacific Ocean known as the Clarion-Clipperton Zone (CCZ).
This area is incredibly rich in useful metals in the form of polymetallic nodules, potato-sized accretions of mineral that sit on the seafloor at depths of 4000-5000m.
The new sponges, Plenaster craigi Lim & Wiklund, 2017 are described from the eastern region of the CCZ in exploration zones licenced to UK Seabed Resources Ltd and Ocean Mineral Singapore.
The animals were found living attached to the metal-rich nodules on two expeditions to the region in 2013 and 2015.
Sponge expert and lead author Swee-Cheng Lim from the National University of Singapore participated in the second expedition and commented, "The unique morphology of the star-shaped spicules convinced me that these were a completley new group of sponges never seen before".
Dr Helena Wiklund of the Natural History Museum, London, confirmed this with a detailed DNA-based study that placed them as a new genus of sponge.
The new sponges, although small, are remarkably abundant on the food-poor abyssal seafloor at a depth of 4000m.
Principal investigator of the Deep-Sea Research Group at the Natural History Museum, Dr Adrian Glover said, "We were simply astonished to discover that the most abundant animal living on the metal-rich nodules was not only a new species, but from a new genus as well, despite the region being subject to many surveys in the past.
It is clear that our taxonomic knowledge of the biodiversity in this region is still very limited".
The scientists believe that because the species is relatively easy to identify and count (now that it has been described) it could be a useful 'indicator' species to measure future mining impacts.
"The fact that this is a small filter-feeding animal sitting on these nodules just a few centimetres above the sediment makes it a clear target for impact from a deep-sea mining plume" said Dr Glover.
The scientists are now undertaking more detailed genetic and population studies of the animal to better understand its potential response to deep-sea mining.
Plenaster craigi have been named after their abundant stars inside their bodes, Plenaster, and the leader of the two successful survey expeditions, Prof Craig Smith of the University of Hawaii.
###

Additional information

Images

Download here https:/ / www.
dropbox.
com/ s/ 4ub82uk61xdadu5/ Images.
zip?dl= 0

Captions for images in order:

1 Aboard the Research Vessel Thomas G Thompson in the Clarion Clipperton Zone, where Plenaster craigi was discovered (Image credit Thomas Dahlgren, Adrian Glover)

2 Seafloor at 4000m depth, central Pacific Ocean where Plenaster craigi was discovered (Image credit Craig R Smith University of Hawaii)

3 A box core sample is brought on board the research vessel from 4000m depth, central Pacific mining frontier (Image credit Thomas Dahlgren, Adrian Glover)

4 Surface of a 4000m core sample 50x50cm which appears devoid of life on first inspection, but is rich in biodiversity (Image Adrian Glover)

5 The new sponge group is visible on the surface of a large polymetallic nodule (arrowed) but had never before been described - it could be an easy-to-count indicator species for mining impacts (Image credit Adrian Glover, Thomas Dahlgren, H Wiklund)

6 Detail of polymetallic nodules 3-4cm in size, each with a Plenaster craigi (arrowed on one nodule) from 4000m deep, central Pacific mining frontier (scale bar 3cm) (Image credit Adrian Glover, Thomas Dahlgren, Helena Wiklund)

7 Plenaster craigi, detail shot of live animal from polymetallic nodule just recovered from the central Pacific at 4000m depth (Image credit Adrian Glover, Thomas Dahlgren, Helena Wiklund)

Video

Sampling methodologies including box core sampling: https:/ / www.
youtube.
com/ watch?v= Io7NlFKUmYI

Links

Natural History Museum Deep-Sea Research Group: http://www.
uk/ deep-sea
Scientists at British American Tobacco (BAT) have created the most comprehensive database of scientific test results, to date, for an e-cigarette (Vype ePen).
The results of the studies provide evidence that suggests Vype ePen has the potential to be substantially reduced risk compared to traditional cigarettes.
This database was created using data collected from a series of scientific tests that could form the basis of a template to support health-related claims such as 'reduced risk' compared to conventional cigarettes for e-cigarettes, as well as for other innovative next generation products, like tobacco heating devices.
"This is a very new consumer category and both consumers and regulators rightly want as much information as possible about the products available," said Dr David O'Reilly, Group Scientific and R&D Director at British American Tobacco.
"We believe a science-based approach is vital to gathering the evidence needed to demonstrate the reduced-risk potential of e-cigarettes and other products, which is why testing products like Vype ePen in this way is so important.
We intend for this to be the first of many applications of our scientific assessment framework," he said.
The application of BAT's approach to the scientific assessment of potentially reduced-risk products is reported today in the journal Regulatory Toxicology and Pharmacology, where the results of 17 published studies on Vype ePen are described.
The tests used include:

Preclinical studies, which demonstrate the relatively simple composition of the Vype ePen vapour compared to conventional cigarette smoke (Figure 1) -- there are around 95% less toxicants in Vype ePen vapour.
Further tests revealed that this vapour has a much-reduced or no biological impact on human cells in the laboratory, compared to conventional cigarette smoke, depending on the test used (Figure 2).
Clinical Studies, which involve humans, revealed that Vype ePen vapour delivers nicotine to the consumer as efficiently as cigarette smoke -- this is an indicator of whether the product may provide smokers with a satisfactory alternative to a cigarette.
Population studies, which use predictive modelling to estimate an overall harm reduction effect of the product on a population.
BAT's studies indicate that the wide availability of an e-cigarette such as Vype ePen, can have an overall harm reduction effect because more people may quit smoking when e-cigarettes are widely available.
Taken together, these results form the basis of a comprehensive dossier of scientific data that lay the groundwork for establishing this product's reduced-risk potential compared to cigarettes*.
This dossier of results presents the kind of information that regulators like the US Food and Drug Administration want when any company submits a Modified Risk Tobacco Product application in order to introduce novel reduced-risk tobacco products to the US market.
It can take years to create such a dossier and our scientists say that it would be impractical to create a new dossier every time a product is tweaked.
"This category is so fast moving that there are new and improved products appearing all the time.
If for example, a scientific dossier was required before these products could go on the market, this could drastically impact the availability of new and improved products and their value in tobacco harm reduction," said Dr James Murphy, Head of Reduced Risk Substantiation at British American Tobacco.
"Importantly, this sort of framework could provide datasets for product families so that full scientific tests wouldn't need to be done with every new generation of the same product -- making the innovation process faster whilst still giving consumers and regulators assurances around the relative risk of each product.
This could mean improved products with harm reduction potential can be developed, assessed and brought to market more quickly without duplicating tests.
We are urging regulators and public health officials to look at this methodology in this context."
Murphy concluded.
Employees who experience sexual harassment by supervisors, colleagues or subordinates in the workplace may develop more severe symptoms of depression than employees who experience harassment by clients or customers, according to a study involving 7603 employees from across 1041 organizations in Denmark.
The research is published in the open access journal BMC Public Health.
Dr. Ida Elisabeth Huitfeldt Madsen, National Research Centre for the Working Environment, Denmark, the corresponding author said: "We were surprised to see the differences between the effects of harassment by clients or customers compared to harassment by other employees.
This is not something that has been shown before.
Previous research showed an increased risk of long term sickness absence for employees exposed to sexual harassment by a colleague, supervisor or subordinate but an increased risk was not always found in association with sexual harassment by clients or customers."
Dr. Madsen added: "Our findings suggest that sexual harassment from clients or customers has adverse consequences and should not be normalized or ignored.
In this study we found that sexual harassment from clients or customers, which is more prevalent than harassment from other employees, is associated with an increased level of depressive symptoms.
This is important as some workplaces, for example in person-related work like care work or social work, may have an attitude that dealing with sexual harassment by clients or customers is 'part of the job'."
The researchers found that compared to employees not exposed to sexual harassment, employees harassed by clients or customers scored 2.05 points higher on the Major Depression Inventory (MDI) - a self-report mood questionnaire that generates a diagnosis of depression together with an estimate of symptom severity.
Scores on the MDI range from 20 for minor depression to 30 or more for major depression.
Employees harassed by a colleague, supervisor or subordinate scored 2.45 points higher compared to employees who had experienced sexual harassment by clients or customers.
When looking at clinical depression only, the researchers found no increased risk among those harassed by clients or customers compared to those not exposed to harassment, whereas those harassed by colleagues, supervisors or subordinates had a significantly higher risk of clinical depression.
Out of the 7603 employees who participated in this study, 2.4% (180) were exposed to sexual harassment by clients or customers, while 1.0% (79) were exposed to harassment by colleagues.
Women were more likely to be exposed than men, with 169 out of 4116 women reporting sexual harassment by clients or customers compared to 11 out of 3487 men, and 48 women reporting sexual harassment by colleagues compared to 31 men.
Participants employed in care work were more often exposed to sexual harassment by clients or customers - 152 out of 2191 (6.9%) - than participants employed in other occupational groups such as education, service or industrial work.
The authors note that as the number of exposed individuals in this study was relatively low, this increases the uncertainty of the reported estimates, especially for men.
The observed associations may thus largely be reflective of women's experiences.
The cross-sectional observational design of this study does not allow for conclusions about cause and effect.
Also, the use of self-reported data that relied on participant recall may have led to sexual harassment being under- or over-reported.
Despite these limitations, the authors suggest that it is important to investigate sexual harassment from clients or customers and sexual harassment by colleagues, supervisors or subordinates as distinct types of harassment and to identify methods to prevent sexual harassment and the development of depressive symptoms.
###

Media Contact

Anne Korn

Communications Manager

BioMed Central

T: +44 (0)20 3192 2722

E: anne.korn@biomedcentral.com

Notes to editor:

1.
Research article:

Workplace sexual harassment and depressive symptoms: a cross-sectional multilevel analysis comparing harassment from clients or customers to harassment from other employees amongst 7603 Danish employees from 1041 organizations

Friborg et al.
BMC Public Health 2017

DOI: 10.1186/s12889-017-4669-x

For an embargoed copy of the research article, please contact Anne Korn at BioMed Central.
After the embargo lifts, the article will be available here: https:/ / bmcpublichealth.
biomedcentral.
com/ articles/ 10.
1186/ s12889-017-4669-x

Please name the journal in any story you write.
If you are writing for the web, please link to the article.
All articles are available free of charge, according to BioMed Central's open access policy.
BMC Public Health is an open access, peer-reviewed journal that considers articles on the epidemiology of disease and the understanding of all aspects of public health.
The journal has a special focus on the social determinants of health, the environmental, behavioral, and occupational correlates of health and disease, and the impact of health policies, practices and interventions on the community.
A pioneer of open access publishing, BMC has an evolving portfolio of high quality peer-reviewed journals including broad interest titles such as BMC Biology and BMC Medicine, specialist journals such as Malaria Journal and Microbiome, and the BMC series.
At BMC, research is always in progress.
We are committed to continual innovation to better support the needs of our communities, ensuring the integrity of the research we publish, and championing the benefits of open research.
BMC is part of Springer Nature, giving us greater opportunities to help authors connect and advance discoveries across the world.
The National Hurricane Center issued their final advisory on Post-Tropical Cyclone Jose on Sept. 22 at 5 p.m. EDT.
NOAA's GOES East satellite saw the circulation of Jose on Sept. 23 off the New England coast as it continued to weaken.
All Tropical Storm Warnings have been discontinued.
At 5 p.m. EDT (2100 UTC) on Sept. 22 in the National Hurricane Center's final advisory, the center of Post-Tropical Cyclone Jose was located near 39.3 degrees north latitude and 69.1 degrees west longitude.
That's about 140 miles south-southeast of Nantucket, Massachusetts.
The post-tropical cyclone was moving toward the southeast near 3 mph (6 kph) and a slow southeastward drift is forecast for the next day or two.
Maximum sustained winds are near 45 mph (75 kph) with higher gusts.
The estimated minimum central pressure is 996 millibars.
NOAA's GOES East satellite provided this visible view of post-tropical cyclone Jose on Saturday, Sept. 23 at 8:15 a.m. EDT.
The storm continued weakening and generated light rain.
The northwestern quadrant was still over Cape Cod, Massachusetts.
Swells generated by Jose are affecting Bermuda and much of the U.S. east coast and will likely cause dangerous surf and rip current conditions during the next couple of days.
Cape Cod, Nantucket and Martha's Vineyard maintained a high surf advisory on Sept. 23.
Satellite imagery from NASA shows that Hurricane Maria continues to move in a northerly direction while its eye remained east of the Bahamas.
NASA's Aqua satellite showed high clouds over the eye on Sept. 22 and one side of the hurricane over the Bahamas.
On Sept. 23, NOAA's GOES East satellite revealed a more clear eye but a less symmetric storm as Maria moved away from the Bahamas.
On Sept. 22 at 1:45 p.m. EDT (17:45 UTC) Hurricane Maria was over Turks and Caicos Islands, and the Bahamas the MODIS instrument aboard NASA's Aqua satellite captured an image of the hurricane.
At the time of the image Maria's eye had become filled in with high clouds.
The next day, NOAA's GOES East satellite provided a visible view of Hurricane Maria.
On Saturday, Sept. 23 at 8:15 a.m. EDT Maria was moving past the Bahamas and regained a clear eye.
The National Hurricane Center noted of the image, "the 35 nautical mile wide eye has become better defined in satellite imagery."
The GOES image also showed Maria is experiencing about 15 knots of southwesterly vertical wind shear, "which is likely the reason for an asymmetric distribution of convection in the eyewall at this time," said National Hurricane Center forecaster Jack Beven.
NOAA manages the GOES East satellite and the NASA/NOAA GOES Project at NASA's Goddard Space Flight Center in Greenbelt, Md.
created the image.
The National Hurricane Center noted on Saturday, September 23, 2017 that warnings on Maria were discontinued for the Bahamas and The Turks and Caicos Islands while high swells are expected to increase along portions of the Southeastern United States coast.
At 5 a.m. EDT (0900 UTC) on Saturday, September 23, 2017 the center of Hurricane Maria was located near 24.8 degrees north latitude and 72.0 degrees west longitude.
That's about 165 miles (270 km) east-northeast of San Salvador and 340 miles (545 km) east of Nassau.
Maria was moving toward the north-northwest near 9 mph (15 kph), and the National Hurricane Center expects this general to continue through tonight.
Maximum sustained winds are now near 120 mph (195 kph) with higher gusts.
Maria is a category 3 hurricane on the Saffir-Simpson Hurricane Wind Scale.
Fluctuations in intensity are expected during the next couple of days.
The estimated minimum central pressure is 952 millibars.
A turn toward the north is expected on Sunday, Sept. 24.
On the forecast track, Maria should move away from the Bahamas into the open waters of the western Atlantic today.
Swells generated by Maria are affecting Puerto Rico, the Virgin Islands, the northern coast of Hispaniola, the Turks and Caicos Islands, and the Bahamas.
These swells will begin to increase along portions of the southeastern United States coast and Bermuda today.
These swells are likely to cause life-threatening surf and rip current conditions.
A study conducted by researchers in Brazil shows patients with chronic migraine are three times as likely to suffer from severe temporomandibular disorder.
In a study, researchers at the University of So Paulo's Ribeiro Preto School of Medicine (FMRP-USP), in Brazil, finds that the more frequent the migraine attacks, the more severe will be the so-called temporomandibular disorder, or TMD.
The temporomandibular joint acts like a sliding hinge connecting the jawbone to the skull, therefore the disorder's symptoms includes difficulty chewing and joint tension.
"Our study shows that patients with chronic migraine, meaning attacks occurring on more than 15 days per month, are three times as likely to report more severe symptoms of TMD than patients with episodic migraine," said Lidiane Florencio, the first author of the study, which is part of the Thematic Project "Association study of clinical, functional and neuroimaging in women with migraine", supported by the So Paulo Research Foundation - FAPESP.
Previous studies already indicated that migraine is somehow associated with pain in the chewing muscles.
However, this research was the first to consider the frequency of migraine attacks when analyzing its connection with TMD: eighty-four women in their early to mid-thirties were assessed, being that 21 were chronic migraine patients, 32 had episodic migraine, while 32 with no history of migraine were included as controls - the results were published in the Journal of Manipulative and Physiological Therapeutics.
Signs and symptoms of TMD were observed in 54% of the control participants without migraine, 80% of participants with episodic migraine, and 100% of those with chronic migraine.
For Florencio, central sensitization may explain the association between the frequency of migraine attacks and the severity of TMD.
"The repetition of migraine attacks may increase sensitivity to pain," she said.
"Our hypothesis is that migraine acts as a factor that predisposes patients to TMD.
On the other hand, TMD can be considered a potential perpetuating factor for migraine because it acts as a constant nociceptive input that contributes to maintaining central sensitization and abnormal pain processes."
Nociceptive pain is caused by a painful stimulus on special nerve endings called nociceptors.
Migraine and TMD have very similar pathological mechanisms.
Migraine affects 15% of the general population, and progression to the chronic form is expected in about 2.5% of migraine sufferers.
On the other hand TMD is stress-related as much as it has to do with muscle overload.
Patients display joint symptoms - such as joint pain, reduced jaw movement, clicking or popping of the temporomandibular joint - but also develop a muscular condition, including muscle pain and fatigue, and/or radiating face and neck pain.
Which came first?
TMD and migraine are comorbidities.
However, while people who suffer from migraine are predisposed to have TMD, people with TMD will not necessarily have migraine.
"Migraine patients are more likely to have signs and symptoms of TMD, but the reverse is not true.
There are cases of patients with severe TMD who don't present with migraine," said Dbora Grossi, the lead researcher for the study and principal investigator for the Thematic Project.
The researchers believe that TMD may increase the frequency and severity of migraine attacks, even though it does not directly cause migraine.
"We do know migraine isn't caused by TMD," Florencio said.
"Migraine is a neurological disease with multifactorial causes, whereas TMD, like cervicalgia - neck pain - and other musculoskeletal disorders, is a series of factors that intensify the sensitivity of migraine sufferers.
Having TMD may worsen one's migraine attacks in terms of both severity and frequency."
The journal article concludes that an examination of TMD signs and symptoms should be clinically conducted in patients with migraine.
"Our findings show the association with TMD exists but is less frequent in patients with rare or episodic migraine," Grossi said.
"This information alone should change the way clinicians examine patients with migraine.
If migraine sufferers tend to have more severe TMD, then health professionals should assess such patients specifically in terms of possible signs and symptoms of TMD."
###

About So Paulo Research Foundation (FAPESP)
NASA's asteroid sample return spacecraft successfully used Earth's gravity on Friday to slingshot itself on a path toward the asteroid Bennu, for a rendezvous next August.
At 12:52 p.m. EDT on Sept. 22, the OSIRIS-REx (Origins, Spectral Interpretation, Resource Identification, and Security - Regolith Explorer) spacecraft came within 10,711 miles (17,237 km) of Antarctica, just south of Cape Horn, Chile, before following a route north over the Pacific Ocean.
OSIRIS-REx launched from Cape Canaveral Air Force Station in Florida on Sept. 8, 2016, on an Atlas V 411 rocket.
Although the rocket provided the spacecraft with the all the momentum required to propel it forward to Bennu, OSIRIS-REx needed an extra boost from the Earth's gravity to change its orbital plane.
Bennu's orbit around the Sun is tilted six degrees from Earth's orbit, and this maneuver changed the spacecraft's direction to put it on the path toward Bennu.
As a result of the flyby, the velocity change to the spacecraft was 8,451 miles per hour (3.778 kilometers per second).
"The encounter with Earth is fundamental to our rendezvous with Bennu," said Rich Burns, OSIRIS-REx project manager at NASA's Goddard Space Flight Center in Greenbelt, Maryland.
"The total velocity change from Earth's gravity far exceeds the total fuel load of the OSIRIS-REx propulsion system, so we are really leveraging our Earth flyby to make a massive change to the OSIRIS-REx trajectory, specifically changing the tilt of the orbit to match Bennu."
The mission team also is using OSIRIS-REx's Earth flyby as an opportunity to test and calibrate the spacecraft's instrument suite.
Approximately four hours after the point of closest approach, and on three subsequent days over the next two weeks, the spacecraft's instruments will be turned on to scan Earth and the Moon.
These data will be used to calibrate the spacecraft's science instruments in preparation for OSIRIS-REx's arrival at Bennu in late 2018.
"The opportunity to collect science data over the next two weeks provides the OSIRIS-REx mission team with an excellent opportunity to practice for operations at Bennu," said Dante Lauretta, OSIRIS-REx principal investigator at the University of Arizona, Tucson.
"During the Earth flyby, the science and operations teams are co-located, performing daily activities together as they will during the asteroid encounter."
The OSIRIS-REx spacecraft is currently on a seven-year journey to rendezvous with, study, and return a sample of Bennu to Earth.
This sample of a primitive asteroid will help scientists understand the formation of our solar system more than 4.5 billion years ago.
NASA's Goddard Space Flight Center provides overall mission management, systems engineering and the safety and mission assurance for OSIRIS-REx.
Dante Lauretta of the University of Arizona, Tucson, is the principal investigator, and the University of Arizona also leads the science team and the mission's science observation planning and data processing.
Lockheed Martin Space Systems in Denver built the spacecraft and is providing flight operations.
Goddard and KinetX Aerospace are responsible for navigating the OSIRIS-REx spacecraft.
OSIRIS-REx is the third mission in NASA's New Frontiers Program.
NASA's Marshall Space Flight Center in Huntsville, Alabama, manages the agency's New Frontiers Program for the Science Mission Directorate in Washington.
INDIANAPOLIS -- A new IUPUI study funded by the U.S. Department of Agriculture answers a long-debated agricultural question: whether no-tillage alone is sufficient to prevent water pollution from nitrate.
The answer is no.
Researchers in the Department of Earth Sciences in the School of Science at IUPUI conducted a meta-analysis to compare runoff and leaching of nitrate from no-till and conventional tillage agricultural fields.
Surface runoff and leaching are two major transportation pathways for nitrate to reach and pollute water.
Due to its mobility and water solubility, nitrate has long been recognized as a widespread water pollutant.
"What we found is that no-till is not sufficient to improve water quality," said Lixin Wang, an assistant professor and corresponding author of the paper.
"In fact, we found that no-till increased nitrogen leaching."
The study suggests that no-till needs to be complemented with other techniques, such as cover cropping and intercropping or rotation with perennial crops, to improve nitrate retention and water-quality benefits.
After studying concentration of nitrate -- nitrate amount per water volume unit -- and nitrate load, or total amount of nitrate, researchers found surface runoff from no-till fields to contain a similar nitrate load to surface runoff from conventional tillage fields.
In contrast, nitrate load via leaching was greater with no-till fields than with conventional tillage fields.
No-till leaves crop residue on the soil surface and limits soil disturbance except for small slits to add fertilizer.
An estimated 20 percent of all croplands in the U.S. are under no-till management.
It reduces soil erosion by avoiding tilling year after year, which leads to soil getting washed away into lakes and rivers.
Because reducing soil loss reduces nutrient loss, it was assumed that no-till would reduce water pollution, Wang said.
"Overall, we found the adoption of no-till resulted in increased nitrate loss via leaching due to the frequent occurrence of macropores, such as those created by dead roots and earthworm burrows, in soils that have been under long-range no-tillage management," Wang said.
Researchers examined how nitrate loss through surface runoff and leaching were impacted by other factors, including aridity, rainfall variability, soil texture, crop species, duration of tillage and fertilizer type.
###

The research findings are presented in a paper, "Impacts of no-tillage management on nitrate loss from corn, soybean and wheat cultivation: A meta analysis," that was published Sept. 21 in the journal Scientific Reports.
The research presented is in collaboration with Pierre-Andr Jacinthe, the lead principal investigator of the USDA project and a professor in the Department of Earth Sciences; Lin Li, a professor in the Department of Earth Sciences; Pam Martin, a professor in the Department of Earth Sciences; and Stefani Daryanto, a postdoctoral researcher, who is the leading author of this paper.
This knowledge can provide targets in the search for novel bone-loss therapeutics to treat osteoporosis.
BIRMINGHAM, Ala. - A major health problem in older people is age-associated osteoporosis -- the thinning of bone and the loss of bone density that increases the risk of fractures.
Often this is accompanied by an increase in fat cells in the bone marrow.
University of Alabama at Birmingham researchers have now detailed an underlying mechanism leading to that osteoporosis.
When this mechanism malfunctions, progenitor cells stop creating bone-producing cells, and instead create fat cells.
Knowledge of this mechanism can provide targets in the search for novel bone-loss therapeutics to treat human osteoporosis with minimal side effects.
The UAB researchers found that a protein called Cbf-beta plays a critical role in maintaining the bone-producing cells.
Furthermore, examination of aged mice showed dramatically reduced levels of Cbf-beta in bone marrow cells, as compared to younger mice.
Thus, they propose, maintaining Cbf-beta may be essential to preventing human age-associated osteoporosis that is due to elevated creation of fat cells.
Bone is a living tissue that constantly rebuilds.
Bones need a constant new creation of cells specific to their tissue, including the bone-producing cells called osteoblasts.
Osteoblasts live only about three months and do not divide.
The progenitor cells for osteoblasts are bone marrow mesenchymal stem cells.
Besides osteoblasts, mesenchymal stem cells can also differentiate into the chondrocyte cells that make cartilage, the myocyte cells that help form muscles and the adipocytes, or fat cells.
Thus, the same progenitor cell has four possible tracks of differentiation.
UAB researchers and colleagues focused on the molecular mechanism that controls the lineage commitment switch between the osteoblast and adipocyte tracks.
Led by Yi-Ping Li, Ph.D., UAB professor of pathology, and Wei Chen, M.D., UAB associate professor of pathology, they investigated the key role played by Cbf-beta, or core-binding factor subunit beta.
Study details

The team led by Li and Chen generated three mouse models by deleting Cbf-beta at various stages of the osteoblast lineage.
All three mouse models showed severe osteoporosis with accumulation of fat cells in the bone marrow, a pathology that resembles aged bone from enhanced adipocyte creation.
Bone marrow mesenchymal stem cells and bone cells from the skulls of Cbf-beta-deficient mice showed increased expression of adipocyte genes.
Looking at the mechanism downstream, the researchers found that the loss of Cbf-beta impeded the canonical Wnt signaling pathway, particularly through decreased Wnt10b expression.
In nonmutant mice, they found that the protein complex composed of Cbf-beta and the Runx2 transcription factor binds to the Wnt10b promoter to drive Wnt10b expression.
The Cbf-beta/Runx2 complex also inhibited expression of the enhancer protein C/EBP-alpha that promotes differentiation of adipocytes.
In addition, the researchers showed that Cbf-beta maintains the osteoblast lineage commitment in two ways -- through the Wnt paracrine pathway to affect nearby cells and through endogenous signaling within the cell to suppress adipogenesis gene expression.
Altogether, this knowledge of the mechanism driven by Cbf-beta can help explain the imbalance in bone maintenance seen in older people.
###

Besides Li, corresponding author, and Chen, co-corresponding author, co-authors of the paper, "Cbf-beta governs osteoblast-adipocyte lineage commitment through enhancing beta-catenin signaling and suppressing adipogenesis gene expression," published in Proceedings of the National Academy of Sciences, are Mengrui Wu and Yiping Wang, UAB Department of Pathology; Jian-Zhong Shao, Zhejiang University, Hangzhou, China; and Jue Wang, UAB Department of Pathology

Funding came from National Institutes of Health grants AR-044741 and DE-023813 to Li, and AR-070135 to Chen.
Assistance at UAB also was provided by the Center for Metabolic Bone Disease, the Small Animal Phenotyping Core, the Metabolism Core and the Neuroscience Molecular Detection Core Laboratory.
Proper health care is more difficult in remote parts of India.
But one UTA business professor has published a study showing how to make telemedicine affordable and sustainable in those remote areas through micro-entrepreneurship.
RadhaKanta Mahapatra, a professor in the Department of Information Systems and Operations Management in the UTA College of Business, conducted the study, A Collaborative Approach to Creating ICT-based Sustainable Development, which was published as part of the Americas Conference on Information Systems' proceedings earlier this year.
ICT is Information and Communication Technology.
Former Odisha Chief Secretary Sahadeva Sahoo co-authored the study. "
We discovered that one key was making telemedicine sustainable, from technological and job skills standpoints but especially from a financial standpoint," Mahapatra said.
"Creating public-private partnerships to establish these telemedicine centers is another key to lasting, thriving telemedicine outposts."
A telemedicine outlet could cost between $9,000 and $12,000 to start in one of these remote areas, a large amount of money considering the average income level of rural India.
Mahapatra looked at the strategy and operations of OTTET Telemedicine for his case study.
OTTET is a non-profit organization based in the state of Odisha in India.
It promotes the use of telemedicine technology to expand the reach of healthcare to rural areas.
Odisha, one of the 29 states of India, covers about 60,000 square miles with a population of about 42 million.
About 83 percent of the population lives in rural areas, according to the 2011 Census of India and Mahapatra's study.
The state of healthcare delivery in India is very poor, especially in these remote areas.
The public health system is run by the state government.
A major hurdle in successful implementation of ICT-based development projects is lack of local ownership.
What Mahapatra discovered was that when funding agencies' grants expire, the system suffered or failed and people's healthcare was often sacrificed.
"One key to OTTET's success in implementing telemedicine projects in rural areas is getting someone in the community to invest in a telemedicine project's success," Mahapatra said.
"OTTET also tackles the lack of technical manpower in rural areas by training unemployed rural youths on telemedicine technology, another factor essential in the project's success."
What the study calls for is effective public-private partnership.
"We discovered that OTTET Telemedicine offers a viable approach to incremental and sustainable implementation of Information and Communications Technology-based development projects as long as there is local ownership and the public-private partnership model at work," Mahapatra said.
Chandra Subramaniam, interim dean of the UTA College of Business, said Mahapatra's work speaks directly to health and the human condition, one of the pillars of UTA's strategic plan.
"The findings of this paper will help create sustainable health delivery models in less developed regions around the world," Subramaniam said.
"Citation of this study by a major newspaper (the New Indian Express) in India brings positive publicity to UTA and helps extend its influence around the world."
###

Mahapatra started at UTA in 1998.
His research interests include ICT for global development, healthcare information systems, data warehousing, data quality, healthcare information systems, agile software development and IT project management.
His research publications have appeared in various journals including MIS Quarterly, Decision Support Systems, Information & Management and Communications of the ACM.
He is a recipient of the Distinguished Research Publication Award and the Distinguished Professional Publication Award from the College of Business.
Collaboration between the University of Maryland, College Park, and the University of Maryland, Baltimore, finds two group A Streptococcus genes involved in invasive, spreading infection underneath skin.
Group A Streptococcus bacteria cause a variety of illnesses that range from mild nuisances like strep throat to life-threatening conditions including pneumonia, toxic shock syndrome and the flesh-eating disease formally known as necrotizing fasciitis.
The life-threatening infections occur when the bacteria spread underneath the surface of the skin or throat and invade the underlying soft tissue.
A 2005 study published in The Lancet attributed half a million deaths worldwide each year to group A Streptococcus.
"In 24 to 48 hours, you can go from being healthy to having a limb amputated to save your life," said Kevin McIver, professor of cell biology and molecular genetics at the University of Maryland, College Park.
"And we don't really know why or how the bacteria do that."
In a new study, McIver's laboratory and researchers at the University of Maryland School of Medicine identified two genes important for invasive group A Streptococcus infections in mice.
The genes, subcutaneous fitness genes A (scfA) and B (scfB), may prove to be promising clinical targets in the fight against these infections, as there are no vaccines against group A Streptococcus or effective treatments for invasive infections.
The study was published online on August 23, 2017, in the journal PLOS Pathogens.
Led by Yoann Le Breton, the study's first author and a research assistant professor in McIver's group, the researchers discovered scfA and scfB by performing transposon sequencing on the entire group A Streptococcus genome.
Transposons, also known as jumping genes, are short sequences of DNA that physically move within a genome, mutating genes by jumping into them.
If the mutation causes an interesting effect, scientists can identify the mutated gene by locating the transposon, sequencing the DNA surrounding the transposon and mapping its location in the genome.
"Invasion under the skin, or subcutaneously, is not the norm for group A Streptococcus bacteria; it's actually very rare," McIver explained.
"We hypothesized that there must be genes in the bacteria important for invading soft tissues and surviving under the skin.
And we tested that theory by using transposons to make thousands of different individual mutants that we used to infect a subcutaneous environment in mice."
McIver and his colleagues used a transposon called Krmit--which they created in a previous study--to generate a collection of approximately 85,000 unique mutants in a group A Streptococcus strain.
They injected the mutant strains into mice, which resulted in humanlike infections.
The transposon was named for the Muppets character Kermit the frog, whose creator Jim Henson, a 1960 College Park alumnus, died of toxic shock syndrome following group A Streptococcal pneumonia.
"We were particularly interested in the mutations that didn't come out the other end--the ones not found in the surviving bacteria from the infected tissue," McIver said.
"These genes would be good targets for a vaccine or treatment because the bacteria missing these genes did not flourish in the infection site."
The researchers identified 273 scf genes as potentially involved in establishing infection under the skin, but two genes stood out: scfA and scfB.
Based on patterns in their DNA sequences, these genes likely encode proteins in the bacterial membrane.
This is a prime location for gene products involved in infection because many dangerous bacteria secrete toxins or proteins through the membrane to attack the host.
Additional experiments showed that bacteria lacking scfA or scfB had difficulty spreading from under the skin to the bloodstream and other organs.
The results suggest that these two genes are involved in the invasion process and may be potential targets for therapeutics.
"The next steps will be to expand the study to include multiple animal models, and these experiments are already underway," said Mark Shirtliff, a co-author of the study and a professor in the Department of Microbiology and Immunology at the University of Maryland School of Medicine and the Department of Microbial Pathogenesis at the University of Maryland School of Dentistry.
"We can also begin to formulate improved therapies and vaccines against group A streptococcus infections and their complications such as rheumatic heart disease, pneumonia and necrotizing fasciitis."
McIver also looks forward to using transposon sequencing to study other ways bacteria attack humans.
"Transposon sequencing can be used to probe how bacteria infect humans in any environment you can think of," McIver said.
"Like group A Streptococcus, many pathogenic bacteria have completely sequenced genomes, but we don't know what most of the genes are doing.
We're excited to have a method to interrogate all that unknown genetic material to better understand human infections."
###

Other study co-authors affiliated with the UMD Department of Cell Biology and Molecular Genetics include Professor Najib El-Sayed, postdoctoral fellow Ashton Belew, graduate student Ganesh Sundar and laboratory technician Emrul Islam.
This work was supported by the National Institute of Allergy and Infectious Diseases at the National Institutes of Health (Award Nos.
AI047928, AI134079 and AI094773) and a University of Maryland, Baltimore and University of Maryland, College Park Seed Grant.
The content of this article does not necessarily reflect the views of these organizations.
The research paper, "Genome-wide discovery of novel M1T1 group A streptococcal determinants important for fitness and virulence during soft-tissue infection," Yoann Le Breton, Ashton Belew, Jeffrey Freiberg, Ganesh Sundar, Emrul Islam, Joshua Lieberman, Mark Shirtliff, Herv Tettelin, Najib El-Sayed and Kevin McIver, was published online in the journal PLOS Pathogens on August 23, 2017.
Media Relations Contact: Irene Ying, 301-405-5204, zying@umd.edu

University of Maryland

College of Computer, Mathematical, and Natural Sciences

2300 Symons Hall

College Park, MD 20742

http://www.
edu

@UMDscience

About the College of Computer, Mathematical, and Natural Sciences?
The College of Computer, Mathematical, and Natural Sciences at the University of Maryland educates more than 7,000 future scientific leaders in its undergraduate and graduate programs each year.
The college's 10 departments and more than a dozen interdisciplinary research centers foster scientific discovery with annual sponsored research funding exceeding $150 million.
After an election year marked by heated exchanges and the distribution of fake news, Twitter bots earned a bad reputation--but not all bots are bad, suggests a new study co-authored by Emilio Ferrara, a USC Information Sciences Institute computer scientist and a research assistant professor at the USC Viterbi School of Engineering's Department of Computer Science.
In a large-scale experiment designed to analyze the spread of information on social networks, Ferrara and a team from the Technical University of Denmark deployed a network of algorithm-driven Twitter accounts, or social bots, programmed to spread positive messages on Twitter.
"We found that bots can be used to run interventions on social media that trigger or foster good behaviors," says Ferrara, whose previous research focused on the proliferation of bots in the election campaign.
But it also revealed another intriguing pattern: information is much more likely to become viral when people are exposed to the same piece of information multiple times through multiple sources.
"This milestone shatters a long-held belief that ideas spread like an infectious disease, or contagion, with each exposure resulting in the same probability of infection," says Ferrara.
"Now we have seen empirically that when you are exposed to a given piece of information multiple times, your chances of adopting this information increase every time."
To reach these conclusions, the researchers first developed a dozen positive hashtags, ranging from health tips to fun activities, such as encouraging users to get the flu shot, high-five a stranger and even Photoshop a celebrity's face onto a turkey at Thanksgiving.
Then, they designed a network of 39 bots to deploy these hashtags in a synchronized manner to 25,000 real followers during a four-month period from October to December 2016.
Each bot automatically recorded when a target user retweeted intervention-related content and also each exposure that had taken place prior to retweeting.
Several hashtags received more than one hundred retweets and likes, says Ferrara.
"We also saw that every exposure increased the probability of adoption - there is a cumulative reinforcement effect," says Ferrara.
"It seems there are some cognitive mechanisms that reinforce your likelihood to believe in or adopt a piece of information when it is validated by multiple sources in your social network."
This mechanism could explain, for example, why you might take one friend's movie recommendation with a grain of salt.
But the probability that you will also see that movie increases cumulatively as each additional friend makes the same recommendation.
Aside from revealing the hidden dynamics that drive human behavior online, this discovery could also improve how positive intervention strategies are deployed on social networks in many scenarios, including public health announcements for disease control or emergency management in the wake of a crisis.
"The common approach is to have one broadcasting entity with many followers, but this study implies that it would be more effective to have multiple, decentralized bots share synchronized content," says Ferrara.
He adds that many communities are isolated from certain accounts due to Twitter's echo chamber effect: social media users tend to be exposed to content from those whose views match their own.
"What if there is a health crisis and you don't follow the Centers for Disease Control and Prevention account?
By taking a grassroots approach, we could break down the silos of the echo chamber for the greater good," says Ferrara.
###

The study, entitled "Evidence of complex contagion of information in social media: An experiment using Twitter bots," was published in PLOS ONE on Sept. 22.
In a paper to be published in Science on 22 September, the Pierre Auger Collaboration reports observational evidence demonstrating that cosmic rays with energies a million times greater than that of the protons accelerated in the Large Hadron Collider come from much further away than from our own Galaxy.
Ever since the existence of cosmic rays with individual energies of several Joules was established in the 1960s, speculation has raged as to whether such particles are created there or in distant extragalactic objects.
The 50 year-old mystery has been solved using cosmic particles of mean energy of 2 Joules recorded with the largest cosmic-ray observatory ever built, the Pierre Auger Observatory in Argentina.
It is found that at these energies the rate of arrival of cosmic rays is ~6% greater from one half of the sky than from the opposite one, with the excess lying 120?
away from the Galactic centre.
In the view of Professor Karl-Heinz Kampert (University of Wuppertal), spokesperson for the Auger Collaboration, which involves over 400 scientists from 18 countries, "We are now considerably closer to solving the mystery of where and how these extraordinary particles are created, a question of great interest to astrophysicists.
Our observation provides compelling evidence that the sites of acceleration are outside the Milky Way."
According to Professor Luis Anchordoqui, one of three Lehman College collaborators, the results are a fantastic achievement for astrophysics.
"It is the first time an experiment is able to discover patterns in the sky that could help track the origin of the most energetic particles in the Universe," he said.
Cosmic rays are the nuclei of elements from hydrogen (the proton) to iron.
Above 2 Joules the rate of their arrival at the top of the atmosphere is only about 1 per sq km per year, equivalent to one hitting the area of a football pitch about once per century.
Such rare particles are detectable because they create showers of electrons, photons and muons through successive interactions with the nuclei in the atmosphere.
These showers spread out, sweeping through the atmosphere at the speed of light in a disc-like structure, similar to a dinner-plate, several kilometres in diameter.
They contain over ten billion particles and, at the Auger Observatory, are detected through the Cherenkov light they produce in a few of 1600 detectors, each containing 12 tonnes of water, spread over 3000 km2 of Western Argentina, an area comparable to that of Rhode Island.
The times of arrival of the particles at the detectors, measured with GPS receivers, are used to find the arrival directions of events to within ~1?.
By studying the distribution of the arrival directions of more than 30000 cosmic particles the Auger Collaboration has discovered an anisotropy, significant at 5.2 standard deviations (a chance of about two in ten million), in a direction where the distribution of galaxies is relatively high.
Although this discovery clearly indicates an extragalactic origin for the particles, the actual sources have yet to be pinned down.
The direction of the excess points to a broad area of sky rather than to specific sources as even particles as energetic as these are deflected by a few 10s of degrees in the magnetic field of our Galaxy.
The direction, however, cannot be associated with putative sources in the plane or centre of our Galaxy for any realistic configuration of the Galactic magnetic field.
Cosmic rays of even higher energy than the bulk of those used in this study exist, some even with the kinetic energy of well-struck tennis ball.
As the deflections of such particles are expected to be smaller, the arrival directions should point closer to their birthplaces.
These cosmic rays are even rarer and further studies are underway using them to try to pin down which extragalactic objects are the sources.
Knowledge of the nature of the particles will aid this identification and work on this problem is targeted in the upgrade of the Auger Observatory to be completed in 2018.
Lehman College is a collaborating institution in The Pierre Auger Observatory in Pampa Amarilla, Argentina, the largest cosmic ray observatory in the world.
The observatory tracks and observes particle showers produced by ultra-high energy cosmic rays.
###

The City University of New York is the nation's leading urban public university.
Founded in New York City in 1847, the University comprises 24 institutions: 11 senior colleges, seven community colleges, and additional professional schools.
The University serves nearly 275,000 degree-credit students and 218,083 adult, continuing, and professional education students.
Fires that span across the Northern Territory and Western Australia appear to have broken out in areas that have already been burned in previous fires.
Areas that sport "burn scars", those areas that are a darker, almost red-brown color, are surrounded by fires that are anywhere from a few hours old to 7 days old.
The areas that are seven days old can be attributed to fires that spread but areas that are just a few hours old may be fires that have presumably been put out only to have them break out again.
The Northern Territory of Australia experienced a higher than normal amount of rain this past season allowing the plants and trees that fuel fires to become even more overgrown and subject to becoming fire fodder.
The fire map from the Northern Australia and Rangelands Fire Information site has been superimposed over the Suomi NPP fire image.
The red marks are from the Suomi NPP image showing fires that are currently burning.
The other colors come from the NAFI map that shows areas burned at other times.
The legend on the lower right-hand side show which colors correspond to the months in 2017 where other fires made their marks on the landscape.
###

The Suomi NPP satellite's Visible Infrared Imaging Radiometer Suite (VIIRS) instrument captured this image on September 21, 2017.
NASA image courtesy Jeff Schmaltz, MODIS Rapid Response Team.
Caption: NASA/Goddard, Lynn Jenner with information from the Northern Australia and Rangelands Fire Information site.
AURORA, Colo. (Sept. 22, 2017) -- Efforts to remove barriers to accessing emergency contraception (EC) scored victories in 2013, when the U.S. Food and Drug Administration removed age restrictions on over-the-counter sales of the levonogestrel drug Plan B.
But those who need EC can still encounter cost and availability barriers.
Researchers at the University of Colorado School of Medicine discovered this when they asked 633 Colorado pharmacies in 2014 about EC access.
They found EC completely accessible to just 23 percent of those who use them.
They report their findings in the latest issue of the journal Women's Health Issues, "Barriers to Single-Dose Levonorgestrel-Only Emergency Contraception Access in Retail Pharmacies."
The study was selected by the editor of Women's Health Issues as an Editor's Choice article for the September/October 2017 edition.
Women's Health Issues is the official journal of the Jacobs Institute of Women's Health, which is based in the Department of Health Policy and Management at Milken Institute School of Public Health (Milken Institute SPH) at the George Washington University.
Study author Van (Mimi) Chau, a student at CU School of Medicine, under the mentorship of Carol Stamm, MD, along with colleagues that included Laura Borgelt, PharmD, a professor at the University of Colorado Skaggs School of Pharmacy and Pharmaceutical Sciences used the Little Blue Book 2014, which physicians use for referrals, to identify Colorado pharmacies, and then had three researchers call the pharmacies posing as women seeking levonogestrel-only emergency contraception (LNG-EC).
Chau was part of the University's Leadership, Education, Advocacy and Development (LEADS) track while she worked on the project.
They asked each pharmacy whether they had LNG-EC in stock, whether it was located on the shelf or had to be requested from the pharmacy, whether a generic version was available, how much the product cost, and whether any additional documentation -- such as proof of age or a prescription -- was required to purchase the drug.
The authors defined EC as being "completely accessible" at a pharmacy if the responding employee reported having it available on store shelves that day for purchase without presenting an ID or prescription.
Accessibility is important because EC must be taken within 120 hours of intercourse, and research suggests it is most effective within the first 24 hours.
Chau and her colleagues found that 87 percent of pharmacies reported having LNG-EC in stock, but it was only completely accessible at 23 percent of the stores surveyed.
Of the stores with the drug in stock, 42 percent reported it was behind the counter -- i.e., had to be requested from a pharmacy employee -- and 56 percent told callers an ID or prescription was required for purchase.
Independent pharmacies were significantly less likely to have EC in stock (58 percent of independent stores vs. 90 percent of chain stores and 100 percent of 24-hour stores) or demonstrate complete access (10 percent vs. 25 percent and 15 percent), the authors report.
Requiring EC purchasers to request the drug from a pharmacy employee and present additional documentation are potentially substantial barriers, the authors note, because people may find it embarrassing to interact with an employee about reproductive healthcare and may lack the requested documents.
Adolescents may not have identification or may not meet the age limit pharmacy employees believe to be in place.
When considering why pharmacy employees report outdated policies for documentation and behind-the-counter access, the authors point out that the age cutoff for LNG-EC products changed four times before being lifted completely, and suggest "delays in updating store policies or lag in information dissemination may explain the variability in knowledge among pharmacy staff about FDA regulations and requirements."
"Although federal policy restrictions on LNG-EC have been removed, this study demonstrates that retail pharmacy-level policies can still create tangible hindrances in obtaining appropriate health care," Chau and her co-authors write.
The study, "Barriers to Single-Dose Levonorgestrel-Only Emergency Contraception Access in Retail Pharmacies," has been published in the September/October issue of Women's Health Issues.
Pearls are among nature's most beautiful creations, and have been treasured for countless centuries.
Beneath one's iridescent surface lies a tough and resilient structure made of intricately arranged tiles of calcium carbonate organized by a crew of proteins that guide its formation and repair.
While it is known that pearls are made of calcium carbonate with an organic matrix core, the role of the proteins modulating the organization of these crystals has, until recently, been unclear.
Researchers at New York University College of Dentistry (NYU Dentistry) reported the role of two such proteins, the first two-protein study of its kind, that regulate the processes leading up to the formation of pearl.
The study was published online in July in the journal Biochemistry, a journal of the American Chemical Society.
A pearl is a byproduct of an oyster's defense mechanism, formed in response to injury to the mantle tissue by an irritant, such as a parasite or grain of sand.
Detached cells fall into the inner tissue where they multiply and form an enclosed sac-like structure to seal off the injured remnants.
This cavity is then filled with matrix proteins followed by mineral.
The mineral consists of two calcium carbonate components: an inner prismatic layer known as calcite and an outermost layer known as aragonite or the lustrous layer.
Both layers are chemically similar to the oyster shell itself.
"In the case of Pinctada fucata, a Japanese pearl oyster that creates precious pearls for the pearl industry, the pearl formation process is mediated by a 12-member protein family known as Pinctada Fucata Mantle Gene, or PFMG.
PFMG1 and PFMG2 are part of this PFMG proteome that not only forms the pearl, but also acts as 'maintenance crew' participating in the formation and repair of the shell," explained John S. Evans, DMD, PhD, professor of basic science and craniofacial biology at NYU Dentistry and the study's corresponding author.
Little is known about these proteins except that they are expressed in mantle tissue of the oyster.
Using the recombinant versions of PFGM1 and PFMG2, the authors used several characterization techniques to study the behavior of proteins and crystals in various conditions that mimic the ocean water.
"What we found is that PFMG1 and PFMG2 combine to form a hydrogel, and within this hydrogel each protein plays a specific role.
PFMG2 determines the size of the hydrogel assemblies and regulates the internal structure of the protein films, whereas PFMG1 enhances the stability of tiny ionic clusters that combine to form calcium carbonate layers of pearl," said Gaurav Jain, PhD, a postdoctoral associate in Dr. Evans's lab and the study's lead author.
"However, once mineral crystals form, PFMG1 and PFMG2 work together and put the finishing touches to the pearl by synergistically modifying the mineral crystal surfaces and creating internal porosities.
The interactions between both proteins is enhanced by calcium ions possibly due to interactions between different domains of PFMG1 and PFMG2," said Martin Pendola, PhD, also a postdoctoral associate in Dr. Evans's lab a study co-author.
"Pearl - which is essentially an inside-out version of the mollusk shell - consists of 95 percent calcium carbonate and 5 percent organic matrix.
This composition makes pearl approximately 1,000 times tougher than pure calcium carbonate - and one of the most resilient and lightweight materials found in a living organism," said Jain.
This research not only advances the understanding of underlying molecular mechanisms of pearl formation, which could have implications for quality and productivity in the pearl industry, but could also aid in the development of fracture resistant materials.
These resilient materials could have a variety of applications, including in the manufacturing of improved dental implants, materials for aerospace applications, or energy transmission.
###

In addition to Evans, Jain, and Pendola of NYU Dentistry, study authors include Yu-Chieh Huang and Denis Gebauer of Universitt Konstanz and Jose Juan Colas of University of York.
Portions of this research (mineralization assays, LM/EM/AFM visualization, flow cytometry, molecular modeling) were supported by the U.S. Department of Energy, Office of Basic Energy Sciences, Division of Materials Sciences and Engineering (Award DE-FG02-03ER46099).
QCM-D studies were supported by grants from the EPSRC (EP/M028127/1) and the MRC (Discovery Grant MCPC15073).
About NYU College of Dentistry
The ability to successfully navigate in the environment is essential both for animals searching for food or escaping predators, as well as for human urban dwellers.
It is something we take for granted, but under the hood, it is supported by still incompletely understood brain networks that continuously calculate our position in the environment.
Moreover, the location where certain experience occurred is an indispensable building block of memory.
In the study, which appears in the September issue of the journal Neuron, the team led by Bruce McNaughton, UCI distinguished professor of neurobiology and behavior at the Ayala School of Biological Sciences, presents the findings that improve our understanding of the brain's ability to tell how fast and in which direction our location is changing.
"The parietal cortex is part of the brain that processes visual and other sensory information in order to continuously update the speed and direction of movement", said Dr. Ivan Skelin, co-first author of the study and postdoctoral researcher at the Canadian Centre for Behavioural Neuroscience at the University of Lethbridge and the Department of Neurobiology and Behavior at the Ayala School of Biological Sciences.
"In this study, we found that there is a division of work between large groups of cells, or modules, each being active when the experimental rat's speed/direction were in a certain range [e.g.
10-20 cm/sec and heading north-east]."
Based solely on the activity of these cell modules, researchers were able to predict animal speed and direction with high accuracy.
The newly acquired knowledge of where and how this information is organized in the brain is a potentially useful guidance for the development of brain-machine interfaces, which are already helping paralyzed people interact with the environment using their thoughts.
###

Other researchers who contributed to this work were Aaron Wilber and Wei Wu from Florida State University.
The study was supported by the National Institutes of Health and Alberta Innovates - Health Solutions.
About the University of California, Irvine: Founded in 1965, UCI is the youngest member of the prestigious Association of American Universities.
The campus has produced three Nobel laureates and is known for its academic achievement, premier research, innovation and anteater mascot.
Led by Chancellor Howard Gillman, UCI has more than 30,000 students and offers 192 degree programs.
It's located in one of the world's safest and most economically vibrant communities and is Orange County's second-largest employer, contributing $5 billion annually to the local economy.
For more on UCI, visit http://www.
Media access: Radio programs/stations may, for a fee, use an on-campus ISDN line to interview UCI faculty and experts, subject to availability and university approval.
For more UCI news, visit news.uci.edu.
Additional resources for journalists may be found at communications.uci.edu/for-journalists.
A research led by Jordi Surralls, professor of the Department of Genetics and Microbiology at the Universitat Autnoma de Barcelona, director of the Genetics Unit at the Hospital de la Santa Creu i Sant Pau and lead researcher at the Centre for Biomedical Network Research on Rare Diseases (CIBERER), has identified a new genetic syndrome caused by mutations in both copies of the FANCM gene, also known as biallelic mutations.
The results, published in Genetics in Medicine, the official journal of the American College of Medical Genetics and part of the Nature group, suggest that these mutations predispose the body to early formations of tumours and chemotherapy toxicity.
In the article, lead author Massimo Bogliolo from the CIBERER research group led by Jordi Surralls analysed biallelic mutations in the FANCM gene in three individuals.
Despite the number of patients always being low in these types of studies, given that they deal with rare diseases, it was observed that there was an early onset of cancer and toxicity to chemotherapy, but the patients did not present any congenital malformations or haematological phenotype which could suggest being affected by Fanconi anaemia, a rare disease which affects one out of every 100,000 children.
Until now it was believed that the FANCM gene was related to this disease, given that in 2005 the biallelic mutation was observed in patients suffering from Fanconi anaemia.
In another article published in the same volume of the journal, researchers from Dr Surralls' group and the research group led by Javier Bentez at the CNIO and the CIBERER confirmed that women with biallelic mutations in the FANCM gene did not develop Fanconi anaemia, but did present a higher risk of breast cancer, chemotherapy toxicity and chromosomal fragility.
This article was coordinated by Paolo Peterlongo of the Milan Institute of Molecular Oncology and included the participation of several hospitals and research centres of Italy, Germany, Spain and Sweden.
"Until now it was thought that biallelic mutations in the FANCM gene caused Fanconi anaemia, but we have now demonstrated that it is not so, given that in the two studies there were eight patients with these mutations and none of them had anaemia", affirms Jordi Surralls.
The patients however had suffered from cancer at very early ages and also presented chemotherapy toxicity.
Therefore, in view of the new syndrome, the authors recommend modifying the clinical monitoring of patients with biallelic mutations in the FANCM gene and taking precautions when using chemotherapy and radiation therapies due to the acute toxicity they may produce.
Genetic Complementation Test

For the study, researchers conducted functional complementation tests, a very important type of analysis in mass sequencing projects in which there are several mutated genes and it is not clear which ones are the source of the disease.
The patient's cells have a clear phenotype of chemical hypersensitivity to DEB, an agent which causes damage to DNA (patient's cells do not survive high doses of DEB).
In contrast, when a healthy FANCM gene copy was transferred into the cells of patients with the help of a virus (using lentiviral transductions), researchers observed the reversal of this phenotype and cells behaving as if they were healthy (a response similar to that of a healthy donor).
This functional study is the genetic demonstration that the gene causing the disease is FANCM and, therefore, the mutations observed in this gene are of a pathogenic nature.
WASHINGTON, D.C., September 22, 2017 -- Pure diamond consists of carbon atoms in a perfect crystal lattice.
But remove a few carbons and swap some others for nitrogen, and you get a diamond with special quantum-sensing properties.
These properties are useful for quantum information applications and sensing magnetic fields, and as a platform for probing the mysteries of quantum physics.
When a nitrogen atom is next to the space vacated by a carbon atom, it forms what is called a nitrogen-vacancy (NV) center.
Now, researchers have shown how they can create more NV centers, which makes sensing magnetic fields easier, using a relatively simple method that can be done in many labs.
They describe their results this week in Applied Physics Letters, from AIP Publishing.
Magnetic field sensing presents a prime example for the importance of this sensing.
Green light can induce the NV centers to fluoresce and emit red light, but the amount of this fluorescence changes in the presence of a magnetic field.
By measuring the brightness of the fluorescence, diamond NV centers can help determine magnetic field strength.
Such a device can make magnetic images of a range of sample types, including rocks and biological tissue.
The sensitivity of this type of magnetic detection is determined by the concentration of NV centers while vacancies that are not paired with nitrogen create noise.
Efficient conversion of vacancies into NV centers, therefore, as well as maximizing the concentration of NV centers, plays a key role in advancing these detection methods.
Researchers typically purchase nitrogen-doped diamonds from a separate company.
They then bombard the diamond with electrons, protons or other particles, which strip away some of the carbon atoms, leaving behind vacancies.
Finally, a heating process called annealing nudges the vacancies next to the nitrogen atoms to form the NV centers.
The problem is that irradiation often requires sending your sample to a separate facility, which is expensive and time-consuming.
"What is special about our approach is that it's very simple and very straightforward," said Dima Farfurnik of the Hebrew University of Jerusalem in Israel.
"You get sufficiently high NV concentrations that are appropriate for many applications with a simple procedure that can be done in-house."
Their method uses high energy electron bombardment in a transmission electron microscope (TEM), an instrument accessible to many researchers, to locally create NV centers.
Normally, a TEM is used to image materials down to subnanometer resolutions, but its narrow electron beam can also irradiate diamonds.
Others have shown TEMs can create NV centers in specialized diamond samples, but the researchers in this study successfully tested the method on several commercially available diamond samples.
In a typical, untreated sample, less than 1 percent of the nitrogen atoms form NV centers.
But by using a TEM, the researchers increased this conversion efficiency to as high as 10 percent.
In certain cases, the samples reached their saturation limit, and more irradiation was no longer effective.
For other samples, however, the researchers didn't hit this limit, suggesting that additional irradiation could boost efficiencies further.
With higher conversion efficiencies, and small irradiation volumes possible with a TEM, devices like magnetic sensors could be more compact.
To make sure the method didn't hinder the effectiveness of NVs in applications like sensing magnetic fields, the researchers confirmed that the length of time the NV centers remain in their states -- the coherence time -- didn't change.
Packing enough NV centers in a diamond would allow physicists to probe the quantum interactions among the centers themselves.
This research could enable the creation of a unique quantum state called a squeezed state, which has never been demonstrated before in a solid and could push the sensing capabilities of these systems beyond today's classical limits.
"We hope the enhanced number of NV centers due to irradiation will serve as a stepping stone for this long-term and ambitious goal," Farfurnik said.
###

The article, "Enhanced concentrations of nitrogen-vacancy centers in diamond through TEM irradiation," is authored by D. Farfurnik, N. Alfasi, S. Masis, Y. Kauffmann, E. Farchi, Y. Romach, Y. Hovav, E. Buks and N. Bar-Gill.
The article appeared in Applied Physics Letters Sept. 19, 2017 [DOI: 10.1063/1.4993257] and can be accessed at http://aip.
scitation.
org/ doi/ full/ 10.
1063/ 1.
4993257 .
ABOUT THE JOURNAL
CHICAGO--The nation's emergency departments had low rates of complying with recommended HIV and syphilis screening for at-risk adolescents, though larger hospitals were more likely to provide such evidence-based care, according to a study presented during the 2017 American Academy of Pediatrics (AAP) national conference.
Nearly 1 million cases of pelvic inflammatory disease (PID) are diagnosed each year, and 20 percent of those diagnoses are for females younger than 21.
PID is a complication of undiagnosed or undertreated sexually transmitted infection and can signal patients at heightened risk for syphilis or HIV, according to a study led by Monika Goyal, M.D., M.S.C.E., director of research in the Division of Emergency Medicine at Children's National Health System.
"Adolescents account for half of all new sexually transmitted infections (STIs) and often view the emergency department (ED) as the primary place to receive health care.
If we are able to increase screening rates for sexually transmitted infections in the ED setting, we could have a tremendous impact on the STI epidemic," Dr. Goyal says.
Although gonorrhea and chlamydia are implicated in most cases of PID, The Centers for Disease Control and Prevention (CDC) recommend that all women diagnosed with PID be screened for HIV and also recommends syphilis screening for all people at high risk for infection.
The research team conducted a cross-sectional study using a database that captures details from 48 children's hospitals to determine how often the CDC's recommendations are carried out within the nation's EDs.
The research team combed through records from 2010 to 2015 to identify all ED visits by adolescent women younger than 21 and found 10,698 PID diagnoses.
The girls' mean age was 16.7.
Nearly 54 percent were non-Latino black, and 37.8 percent ultimately were hospitalized.
"It is encouraging that testing for other sexually transmitted infections, such as gonorrhea and chlamydia, occurred for more than 80 percent of patients diagnosed with PID.
Unfortunately, just 27.7 percent of these young women underwent syphilis screening, and only 22 percent were screened for HIV," Dr. Goyal says.
Cosmic rays are atomic nuclei that travel through space at speeds close to that of light.
Low-energy cosmic rays come from the Sun or from our own Galaxy, but the origin of the highest-energy particles has been the subject of debate ever since they were first discovered fifty years ago: do they come from our Galaxy or from distant extragalactic objects?
The question has now been settled by studying 30 000 cosmic-ray particles with energies a million times greater than those of the protons accelerated in the LHC .
They were detected from 2004 to 2016 at the largest cosmic ray observatory ever built, the Pierre Auger Observatory in Argentina.
Analysis of the arrival directions of the particles showed that at such energies the flux of cosmic rays coming from a region of the sky located 120 degrees from the galactic center is approximately 6% higher than if the flux were perfectly uniform.
This direction cannot be associated with potential sources in either the galactic plane or galactic center, providing the first convincing evidence that these cosmic rays have an extragalactic origin.
The flux of these very high-energy cosmic rays (exceeding 2 joules) is about one particle per square kilometer per year .
When the cosmic rays collide with molecules in the upper atmosphere, they create cascades of over 10 billion secondary particles, known as air showers, which can cover an area exceeding 40 square kilometers by the time they reach the ground.
The Pierre Auger Observatory detects some of these secondary particles (electrons, photons and muons) by means of an array of 1 600 detectors, i.e.
tanks of pure water spaced 1.5 kilometers apart and covering 3 000 square kilometers in the Argentinian pampas, an area slightly larger than Luxembourg.
By comparing the arrival times of particles at the different detectors it is possible to determine where the cosmic ray particle that produced the air shower came from.
This discovery clearly indicates an extragalactic origin for these cosmic rays, since there is a probability of only one in five million that the pattern observed in the sky is due to chance.
However, the study has not yet succeeded in locating the sources precisely.
This is because the region where cosmic rays are brightest covers a large part of the sky, where the number of galaxies is relatively high.
In addition, our Galaxy's magnetic field deflects the paths of these charged particles , making it more difficult to locate their sources.
Some cosmic rays have even higher energies than those focused on in this survey.
They have the disadvantage of being even more unusual, but also the advantage that they are not as deflected by the magnetic field of our own Galaxy.
Their direction of arrival may therefore more accurately indicate the region where they were produced.
In 2007, an earlier study pointed to a correlation between active galactic nuclei and the arrival directions of the highest-energy cosmic rays then detected , but this correlation subsequently turned out to be not very significant.
Research is currently being carried out on a much larger sample of ultrahigh-energy cosmic rays, and may provide some answers.
At the same time, an upgrade program is underway at the Pierre Auger Observatory, which should make it easier to identify the sources.
400 scientists from 18 countries take part in the Pierre Auger Collaboration, which develops and runs the observatory of the same name.
The CNRS is the observatory's principal French funding agency.
The following French laboratories contribute to the collaboration:

the Institut de Physique Nuclaire d'Orsay (CNRS/Universit Paris-Sud) ;

the Laboratoire de Physique Nuclaire et des Hautes Energies (CNRS/UPMC/Universit Paris Diderot) ;

the Laboratoire de Physique Subatomique et de Cosmologie (CNRS/Universit Grenoble Alpes/ Grenoble INP).
NASA-NOAA's Suomi NPP satellite provided a look at Maria's temperatures to find the strongest sides of the storm, while NOAA's GOES satellite revealed the extent of the storm in a visible image as it moved toward the Bahamas.
On Sept. 22 at 3:18 a.m. EDT (0718 UTC) the VIIRS instrument aboard NASA-NOAA's Suomi NPP satellite provided a thermal image of Hurricane Maria north of Hispaniola and nearing the Bahamas.
The image showed highest coldest clouds around the eyewall and in bands of thunderstorms to the northeast and south and southeast of the center, stretching over Hispaniola and Puerto Rico.
Those clouder clouds have the capability of producing heavy rainfall.
At 5:04 a.m. AST/EDT the National Weather Service (NWS) in San Juan, Puerto Rico reported those bands of thunderstorms were still dropping heavy rain.
Satellite estimates indicate heavy rain over eastern Puerto Rico and heavier rain about to move into western Puerto Rico.
The heavy rain will cause flooding.
NWS noted "Some locations that are or will experience flooding include San Juan, Ponce, Arecibo, Yauco, Fajardo, Guayama, Coamo, Jayuya, Adjuntas, Santa Isabel, Canovanas, Sabana Seca, San Sebastian, Naranjito, Humacao, Mayaguez, San German, Lajas, Hatillo and Penuelas."
Flood warnings are in effect until 5:45 p.m. EDT today, Friday, September 22, 2017.
In hilly terrain there are hundreds of low water crossings which are potentially dangerous in heavy rain.
A visible image of Hurricane Maria was taken from NOAA's GOES East satellite on Sept. 22 at 10 a.m. EDT (1400 UTC) showed the storm just north of Turk Island and nearing the Bahamas.
The National Hurricane Center (NHC) reported "Maria is still producing 125-mph winds as it passes northeast of the Turks and Caicos Islands.
Both the Suomi NPP and GOES East images were created at NASA's Goddard Space Flight Center in Greenbelt, Md.
NOAA manages the GOES series of satellites and the NASA/NOAA GOES Project uses the data to create images and animations.
The Rapid Response Team at Goddard produces imagery from the Suomi NPP satellite.
A Hurricane Warning is in effect for Turks and Caicos Islands and the Southeastern Bahamas and a Tropical Storm Warning is in effect for the Central Bahamas.
At 11 a.m. EDT (1500 UTC), the eye of Hurricane Maria was located near 22.3 degrees north latitude and 71.0 degrees west longitude.
That's about 55 miles (90 km) north of Grand Turk Island, and 445 miles (715 km) east-southeast of Nassau.
Maria was moving toward the northwest near 8 mph (13 kph).
Data from an Air Force Reserve Hurricane Hunter aircraft indicate that maximum sustained winds remain near 125 mph (205 kph) with higher gusts.
Maria is a category 3 hurricane on the Saffir-Simpson Hurricane Wind Scale.
A gradual weakening is forecast during the next 48 hours.
The minimum central pressure based on data from the reconnaissance aircraft is 958 millibars.
NHC said a turn toward the north-northwest is expected later today, followed by a turn toward the north by late Saturday.
On the forecast track, Maria's core will move away from the Turks and Caicos Islands today, and pass northeast and east of the Bahamas through Sunday, Sept. 24.
###

For updated forecasts on Maria, visit: http://www.
By Rob Gutro

NASA's Goddard Space Flight Center
One of the few large studies to report long-term outcomes in cardiac patients treated in childhood with extracorporeal membrane oxygenation (ECMO) has found overall favorable outcomes among survivors, as reported by families.
ECMO provides short-term breathing and heart support for critically ill children while doctors treat the underlying illness.
A research team from Children's Hospital of Philadelphia (CHOP) published the ECMO study in the August 2017 issue of Pediatric Critical Care Medicine.
The team analyzed a cohort of 396 patients with cardiac disease treated with ECMO at CHOP from 1995 to 2012.
Overall mortality was 66 percent at a median follow-up of 6 years after ECMO therapy, which remains consistent with outcomes seen in previous decades.
In phone surveys or written surveys among the families of survivors, a majority reported positive outcomes regarding health and physical limitations.
Over 90 percent of families reported good or excellent health, and approximately 86 percent reported no or mild physical limitations.
However, the authors noted a discrepancy between family-reported favorable outcomes and a relatively high rate of medical and behavioral issues revealed by more detailed questioning.
Almost 25 percent of patients had below-average school performance and required special education, and almost 50 percent had parental-reported learning disabilities.
These results may help families define realistic expectations regarding long-term outcomes for children supported with ECMO due to an underlying cardiac condition.
Matthew D. Elias, MD, a pediatric cardiologist at CHOP and first author of the study, noted that ECMO use in children with congenital heart disease (CHD) has increased markedly over the past several decades, as increased experience in pediatric cardiology and cardiac surgery has allowed ECMO use to expand to more complex patients.
Senior author Matthew J. O'Connor, MD, also a CHOP pediatric cardiologist, added that "several factors have potentially improved long-term outcomes, such as increasing experience with ECMO and CHD in general.
But the inclusion of a more medically complex population in the recent era may mitigate these improvements in outcomes, accounting for the fact that overall mortality rates haven't changed much."
Although this single-center study represents one of the largest cohorts of ECMO patients undergoing detailed assessments of outcomes and quality of life, Elias said that further research in larger, multicenter studies should further investigate family experiences and long-term patient outcomes.
He added, "In the meantime, our findings should allow for improved family counseling in discussing long-term quality-of-life for children with heart disease."
###

The ECMO Center at Children's Hospital of Philadelphia recently received the "Platinum Level ELSO Award for Excellence in Life Support" from the Extracorporeal Life Support Organization (ELSO), an international consortium of centers offering ECMO (extracorporeal membrane oxygenation) for support of failing organ systems in infants, children and adults.
CHOP's ECMO Center has been recognized as an ELSO Center of Excellence since 2008.
The Platinum Level is the highest awarded honor, and is rarely achieved by ELSO member institutions, especially pediatric centers.
The ECMO Center at CHOP is the only ECMO center in the Philadelphia region designated by ELSO as a Platinum Center of Excellence, and is one of the most active in the country.
It has supported more than 1,300 patients since it was established in 1990.
The program's multidisciplinary team is comprised of pediatric surgeons, neonatologists, intensivists, anesthesiologists, perfusionists, specially trained nurses and respiratory therapists.
Matthew D. Elias, et al.
"Long-Term Outcomes of Pediatric Cardiac Patients Supported by Extracorporeal Membrane Oxygenation," Pediatric Critical Care Medicine, August 2017. http://doi.
org/ 10.
1097/ PCC.
0000000000001227
Jose continues to bring tropical storm conditions to southern New England although the storm has become post-tropical.
NASA's Terra satellite caught a view of the storm sitting almost stationary about 100 miles from Nantucket Island, Massachusetts.
On Sept. 21 at 11:25 a.m. EDT (1525 UTC), the MODIS instrument or Moderate Resolution Imaging Spectroradiometer aboard NASA's Terra satellite took a visible picture of Jose.
The image showed Jose's clouds in the northwestern quadrant continued sweep over southern New England, southeastern New York, and northeastern New Jersey.
Jose is a large storm and the clouds in its northeastern quadrant are just south of Nova Scotia, Canada.
Tropical-storm-force winds extend outward up to 220 miles (350 km) from the center.
Jose formed on Sept. 2 and has been around 20 days.
The National Hurricane Center issued their 68th advisory on Jose on Sept. 22.
A Tropical Storm Warning is in effect for Woods Hole to Sagamore Beach, including Cape Cod, Massachusetts, Block Island, Martha's Vineyard and Nantucket Island.
At 5 a.m. EDT, National Hurricane Center (NHC) forecaster Zelinsky noted "Since the last advisory, a small burst of deep convection has been observed near the center of Jose.
The cyclone is still embedded within a dry environment and located over cold sea surface temperatures, so it will be a little surprising if the convection is maintained for an extended period of time this morning."
NASA's Aqua satellite observed Jose in infrared light on Sept. 22 at 2:40 a.m. EDT (6:40 UTC) that provided temperatures.
The coldest clouds were the strongest storms.
The Moderate Resolution Imaging Spectroradiometer or MODIS instrument aboard NASA's Aqua satellite found two areas where cloud top temperatures were coldest and as cold as minus 50 degrees Fahrenheit (minus 45.5 degrees Celsius).
Those areas of coldest cloud tops and strongest storms were located in Jose's northeastern quadrant around the center of circulation and in a fragmented band of thunderstorms that stretched from New Jersey to western Massachusetts and northeast to southeastern Maine.
At 8 a.m. EDT (1200 UTC) on Sept. 22 the center of Post-Tropical Cyclone Jose was located near 39.7 degrees north latitude and 69.0 degrees west longitude.
That's about 115 miles (185 km) south-southeast of Nantucket, Massachusetts.
The post-tropical cyclone is moving slowly toward the west near 2 mph (4 kph).
Jose is expected to meander well off the coast of New England for the next several days.
Maximum sustained winds are near 50 mph (85 kph) with higher gusts.
Gradual weakening is forecast for the next 48 hours.
The estimated minimum central pressure is 993 millibars.
Minor coastal flooding is possible along portions of the coast of southern New England during the next few days.
Swells generated by Jose are affecting Bermuda and much of the U.S. east coast, and will likely cause dangerous surf and rip current conditions for the next couple of days in these areas

All of the dynamical models remain in good agreement that Jose will remain trapped in weak steering flow while gradually spinning down for the next several days.
Hearing loss, sometimes associated with other disorders such as balance defects, is the most common sensory deficit, affecting more than 280 million people worldwide, according to WHO.
In France, one child in 700 is born with severe or profound hearing loss, and one in every 1,000 will lose their sense of hearing before adulthood.
Over the past 20 years, scientists have made remarkable progress in deciphering the genetic origins of congenital hereditary hearing loss, which is usually caused by inner ear dysfunction.
The inner ear comprises the hearing organ or cochlea, together with the five balance organs (the saccule, utricle and three semicircular canals), which contain the sensory cells, or hair cells, that detect mechanical vibrations and convert them into electrical signals.
To date, mutations in more than 100 genes have been associated with inner ear defects, and it is estimated that mutations in more than 100 genes can cause genetic forms of deafness.
The various hereditary forms of hearing loss include Usher syndrome type 1 (USH1), a particularly severe clinical form of deaf-blindness, and specifically the USH1G genetic form.
USH1G patients are profoundly deaf and have no balance function at birth, and they subsequently suffer from prepubertal-onset sight loss leading to blindness.
USH1G syndrome is due to mutations in the gene encoding the scaffold protein sans, which is essential for the cohesion of the hair bundle of the inner ear hair cells.
Patients with hearing loss and balance dysfunction are currently fitted with auditory prostheses and may be given balance rehabilitation therapy, but the outcomes are variable.
One possible alternative for treating such hereditary inner ear defects is gene therapy.
This approach entails transferring a healthy (non-mutant) copy of the defective gene to restore the expression of the missing protein.
So far, gene therapy attempts have only resulted in partial improvements of hearing in mouse models of specific human deafness forms that did not include severe anomalies in hair cell structure.
In this context, scientists from the Institut Pasteur, Inserm, the CNRS, Collge de France, University Pierre et Marie Curie, and University Clermont Auvergne*, have now succeeded in restoring hearing and balance in a mouse model of USH1G syndrome using gene therapy.
With a single local injection of the USH1G gene just after birth, the scientists observed a restoration of the structure and mechanosensory function of the inner ear hair bundles - profoundly damaged before birth -, resulting in a long-term partial recovery of hearing, and complete recovery of vestibular function in these mice.
These results unexpectedly establish that inner ear defects due to major morphogenetic abnormalities of the hair bundle can be reversed even after birth, with durable efficacy, by gene therapy.
The scientists injected the USH1G gene into the inner ear using the innocuous AAV8 virus, which enabled them to specifically target the hair cells.
The expression of the therapeutic gene was detected 48 hours after injection.
The team demonstrated that a single injection to restore the production and localization of the missing protein in hair cells successfully improved hearing and balance functions in the young mice.
These findings suggest that the therapeutic protein was able to interact normally with its binding partners among the USH1 molecular complex (the proteins cadherin 23, protocadherin 15, myosin VIIA and harmonin), as required for the mechanoelectrical transduction apparatus of the hair bundle to function correctly.
As Saad Safieddine, CNRS Director of Research at the Institut Pasteur and co-senior author of the study with Prof. Christine Petit (head of the Genetics & physiology hearing unit at the Institut Pasteur), explains, "we have just shown that it is possible to partially correct a specific form of hereditary hearing loss accompanied by balance problems using local gene therapy performed after the embryogenesis of the ear, which is primarily affected by the mutation responsible for the disorder.
This suggests that the time window for effectively treating USH1 syndrome using gene therapy may be larger than initially thought."
This study represents a significant step towards the development of clinical trials in gene therapy for the curative treatment of hereditary deafness and balance loss in humans.
###

*From the Genetics & Physiology of Hearing Laboratory (Institut Pasteur/Inserm/UPMC), the Genes, Synapses and Cognition Laboratory (CNRS/Institut Pasteur, the Center for Neurophysics, Physiology and Pathology (CNRS/Paris-Descartes University), and the Sensory Biophysics Laboratory (University Clermont Auvergne).
Antibiotic resistance continues to grow around the world, with sometimes disastrous results.
Some strains of bacteria no longer respond to any currently available antibiotic, making death by infections that were once easily treatable a renewed reality.
Avoiding this fate is possible, research suggests, if antibiotic prescribers do five essential things correctly: Give the right patient the right medication at the right dose through the right route at the right time.
Medical residents -- doctors who have finished medical school but are still receiving training at clinics and hospitals by working under more experienced physicians -- are key to this strategy since they often are part of the frontline care team that selects and initiates antibiotic therapies.
However, it has been unclear whether their prescribing patterns match these five "rights," says Geovanny F. Perez, M.D., a pulmonologist at Children's National Health System.
"Residents often decide which antibiotics to start a patient on, so they could become the first line of defense against antibiotic resistance," Dr. Perez says.
"They also could be an important target for education efforts if their prescribing patterns aren't aligned with current guidelines."
To determine whether residents are prescribing in ways that best avoid antibiotic resistance, Dr. Perez and colleagues sent an email survey to all 189 residents at two large children's hospitals: Children's National, a tertiary care center that serves patients throughout the greater Metropolitan Washington area at its main campus and network of primary care clinics; and Nicklaus Children's Hospital, the largest freestanding pediatric hospital in South Florida.
The survey was divided into two parts.
The first aimed to assess the knowledge of these residents about which antibiotics are most appropriate to treat five common pediatric infections: Acute otitis media (ear infection), group A streptococcal pharyngitis (strep throat), sinusitis (sinus infection), pneumonia and urinary tract infections.
The second part of the survey was meant to ascertain how residents acquired their antibiotic knowledge and prescribing behaviors.
It asked about their awareness of antibiograms -- a profile of which medications are effective against different local bacterial strains that is updated periodically at most hospitals -- whether residents ever prescribed antibiotics for viral infections and the major influences on their prescribing decisions.
About one-half of the residents returned their surveys.
Their answers suggested that most of them followed prescribing guidelines for the recommended drugs to treat otitis media, streptococcal pharyngitis and urinary tract infections.
However, there were significant variations from guidelines for treating sinusitis and pneumonia, with many residents choosing antibiotics that were against current recommendations.
Additionally, only 3 percent of respondents indicated that they frequently used antibiograms, an important tool in selecting the most effective antibiotics.
About one-half indicated that they sometimes used antibiograms, and one-quarter said that they never used an antibiogram.
An additional 17 percent disclosed that they did not know what an antibiogram was.
Even among those that knew about this important resource, about one-half said that they didn't know where to access antibiograms specific to their hospitals.
Three-quarters of respondents indicated that they had prescribed antibiotics to patients who they considered to have a viral infection, rather than a bacterial one -- a scenario in which antibiotics have no effect.
In a follow-up question assessing the reasons for these decisions, 63 percent answered that they were following instructions from an attending physician or senior resident.
More experienced physicians also played a more general role in shaping residents' antibiotic knowledge: About 54 percent of residents said that their general pediatric inpatient attending physician--who oversees their training efforts -- was their most influential source of knowledge in this area.
The findings, published in the September 2017 issue of Hospital Pediatrics, provide eye-opening insights into how residents prescribe antibiotics and their motivations for these choices, says Dr. Perez -- particularly how the training they receive from mentors steers decisions many residents must make multiple times a day.
He adds that antibiotic stewardship programs, which provide instruction to health care providers about current prescribing guidelines and practices, should focus on both residents and their resident charges for maximum impact.
"Ideally, we should be matching the guidelines 100 percent or at least close to it," Dr. Perez says.
"We think this goal is definitely attainable with the right training for both residents and their mentors alike."
COLUMBUS, Ohio - New research led by The Ohio State University Wexner Medical Center found a potential therapeutic strategy to prevent infections in patients with spinal cord injuries.
This research using mice with spinal cord injuries breaks new ground in the development of treatments to prevent and reduce the incidence of infections without the use of antibiotics, and its results have been published online in the journal Nature Neuroscience.
The study builds on previous Ohio State-led research that found spinal cord injury causes the immune system to become "paralyzed," and thus less able to fight off infections such as pneumonia.
Pneumonia is the main cause of death in patients both after acute and chronic spinal cord injury.
Decreasing disability infections has a strong impact on the lives of people with spinal cord injury.
"Despite its clinical relevance, the underlying mechanisms of how spinal cord injury causes a systemic immune shut down are far from being understood.
After eight years of work, we were able to identify an entirely new mechanism for how spinal cord injury weakens the immune system," said principal investigator Dr. Jan M. Schwab, neurologist and physician at Ohio State's Neurological Institute, who collaborated with researchers from several institutes in Germany, along with the University of Alabama in Birmingham, Harvard Medical School and Boston's Children's Hospital.
Researchers demonstrated that susceptibility to spontaneous pneumonia and severe lymphopenia after spinal cord injury resulted from a maladaptive sympathetic-neuroendocrine reflex involving the adrenal glands.
Lymphopenia is an abnormally low level of lymphocytes or white blood cells that manage microbial host defense.
The identification of this two-stage pathological reflex arc - consisting of nerve pathways between the spinal cord and the adrenal glands, as well as a hormone-mediated link with the immune system - helps to deepen our understanding of the interconnections between the nervous and immune system.
The discovery of this 'immune system paralysis' and its underlying mechanisms represents an important step on the path to improving the treatment of spinal cord injury patients.
Rather than merely experiencing the more obvious symptom of motor-sensory paralysis, paraplegic patients also experience a paralysis of the immune system.
"Based on our findings, we hypothesize that therapeutic normalization of the glucocorticoid and catecholamine imbalance in spinal cord injury patients could be a promising treatment strategy," Schwab said.
"This could lead to new treatments to prevent or reduce infections in patients suffering with these injuries without antibiotics, thereby reducing disability and mortality."
Disrupting nerve fibers to the adrenal glands by high-level but not low-level thoracic spinal cord transection resulted in almost complete suppression of circulating norepinephrine levels and profound stimulation of systemic corticosterone levels.
Identical findings were seen in human patients with traumatic complete spinal cord injury, researchers wrote.
Given that infections are highly prevalent in spinal cord injured patients, orthodox antibiotic treatments start to lose their effectiveness with time due to the development of resistances.
###

The research team included members from Charit - Universitatsmedizin Berlin and German Center for Neurodegenerative Diseases, both in Germany, and the National Spinal Cord Injury Statistical Center at the University of Alabama at Birmingham.
The study received funding from the German Academic Exchange Service, German Research Foundation, Wings for Life Spinal Cord Research Foundation, Else Krner Fresenius Sifting, German legal accident insurance, the Era-Net-NEURON Program of the European Union, The Ohio State University Discovery Theme and the W.E.
Hunt & C.M.
Curtis Endowment to Jan M. Schwab.
The National Spinal Cord Injury Database is funded by the National Institute on Disability, Independent Living and Rehabilitation Research, U.S. Department of Health and Human Services.
Researchers have presented the first report of a new microfluidics-based approach for detecting circulating cancer biomarkers in blood samples

Future Science Group (FSG) today announced the publication of an article in Future Science OA presenting early data from a novel assay for the non-invasive detection of PD-L1 and other biomarkers in patient blood samples.
Response rates to immunotherapies targeting the PD-1 pathway vary, and efforts are ongoing to improve the discovery of those who will and will not benefit from such therapies.
Expression of PD-L1, among other biomarkers, is associated with response; however, owing to tumor heterogeneity and the fact this biomarker is not static, biopsies are not suitable.
Furthermore, biopsies are invasive and unsuitable for repeated testing.
Novel research from an international team led by Jinkai Teo (Merck Research Laboratories, Singapore) sought to solve this problem using peripheral blood samples, and a less-invasive approach.
Whole blood from both healthy donors and breast cancer patients underwent circulating tumor cell enrichment and was loaded onto a microfluidic chip, undergoing chipcytometry.
The results demonstrated that the workflow had a mean detection rate of 22.8%, and could determine PD-L1 and PD-L2 expression levels.
"We believe the main advantages of chipcytometry lie in the iterative staining process that allows retrospective evaluation of additional markers and the potential to measure a large number of parameters without the spillover/compensation problems encountered with flow cytometry," commented the authors.
"This approach allows the analysis of additional immunomodulatory targets on tumor cells beyond PD-L1 and PD-L2, which is particularly critical, considering high dimensional analysis of these markers is likely to become increasingly relevant as immunotherapy moves beyond the administration of single immunomodulatory agents toward combinations that synergize in their antitumor immune response."
Furthermore, the potential to include more positive or negative markers could allow increased confidence that identified cells are CTCs.
However, the authors note that these data are preliminary, and further experimentation is needed to fully establish feasibility of the approach.
###

Available from: https:/ / www.
future-science.
com/ doi/ 10.
4155/ fsoa-2017-0079

About Future Science OA

Launched in March 2015, Future Science OA is the inaugural gold open access journal from Future Science Group.
It publishes articles covering research of application to human health, and utilizes a CC-BY license.
Future Science OA embraces the importance of publishing all good-quality research with the potential to further the progress of medical science.
Both negative and early-phase research is considered.
The journal also features review articles, editorials and perspectives, providing readers with a leading source of commentary and analysis.
More open access oncology research can be found at https:/ / www.
future-science.
com/ journals/ fso/ category/ oncology

About Future Science Group
Mr. Vaughans sentiment is echoed by a cadre of researchers who place mantises in a class of their own among the swarming Class Insecta, and who are discovering a range of skills and predilections that make mantises act like aspiring vertebrates.
Praying mantises are the only insects able to swivel their heads and stare at you.
Those piercing eyes are much like yours, equipped with 3-D vision and a fovea  a centralized concentration of light receptors  the better to focus and track.
A mantis can jump as unerringly as a cat, controlling its trajectory through an intricate series of twists and turns distributed across its legs and body, all to ensure a flawless landing on a ridiculously iffy target nearly every time.
Advertisement Continue reading the main story

The mantis appetite likewise turns out to leap and bound, and with scant regard for food-chain decorum.
By the standard alimentary sequence, insects feed on plants or one another, and then birds hunt down insects.
But just as there are carnivorous plants like the Venus flytrap, mantises prey on hummingbirds and other small-to-middling birds more often than most people realize.
James V. Remsen of the Museum of Natural Science at Louisiana State University and his colleagues documented 147 cases of mantis-on-bird predation in 13 countries representing all continents but Antarctica  not surprising, Dr. Remsen said in an interview, since there are no mantises on Antarctica.
Hummingbirds were the most common target, but mantises also went after warblers, sunbirds, honeyeaters, flycatchers, vireos and European robins.
Large species like the Chinese mantis, which grows to four inches in length, were the most avid avivores, and females were responsible for virtually all the bird-killing observed worldwide.
In two reported cases, females feasted on birds while copulating with males.
Sometimes the mantises would tuck in through the birds breastbone, but more often they went for the head, Dr. Remsen said.
They bite in and eat the brains, he said, which might imply this is something theyre professionals at.
Some mantises in North America now seem to view hummingbird feeders as happy hunting grounds.
Kris Okamoto, a retired nurse in San Juan Capistrano, Calif., recently came running when the young son of her house painter cried out that a praying mantis had snatched a hummingbird from her feeder.
Seeing that the bird was already dead, its skull pierced, Ms. Okamoto and the boy settled down and watched the mantis eat its fill of bird brain.
When the postprandial mantis crept back up the feeder, Ms. Okamoto gently pushed it off with a stick.
Not good enough.
Advertisement Continue reading the main story

It started crawling back toward the feeder, she said.
So we took it away completely and put it over the fence.
Researchers emphasize that bird predation by mantises remains rare and is insignificant compared to the carnage linked to, say, free-roaming cats.
Nevertheless, that the insects have learned to seek out bird feeders for a meal signifies another step in cognition, Dr. Remsen said.
Were lucky praying mantises arent our size.
A Certain Personality

Hunting is a professional trademark of the mantid order: the 2,500 known species are all predators, usually of insects and other small invertebrates.
Some mantises chase down their prey, but many are consummate ambush artists, waiting with Zen stillness in the grass or among flowers for the right moment to strike.
Their closest relatives are the cockroaches, from which they diverged about 250 million years ago, said Gavin J. Svenson, curator of invertebrate zoology at the Cleveland Museum of Natural History and a leading authority on praying mantises.
The family resemblance can still be seen in the long, slender antennae and the triangular, movie-alien shape of the head, among other features.
But praying mantises rise above the flattened scuttling posture that makes cockroaches look so  verminy.
Praying mantises are unusually charismatic, said William D. Brown, who studies them at the State University of New York at Fredonia.
Those large eyes, the way they turn to look at you, gives them a certain personality that most insects lack, he added.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
Molecular and evolutionary studies suggest that mantises diversified in parallel with angiosperms  not because they had anything to do with flowering plants directly, but in part to more effectively prey on the insects that ate or pollinated the plants.
Some mantises evolved to look like showy blossoms, a cancan of deadly come-ons.
Those show-offs dont like playing wallflower.
The orchid mantises of Asia, for example, generally avoid lingering around the flowers they imitate, and instead seek out patches of green vegetation.
They themselves become the flower, Dr. Svenson said.
Theyre a conspicuous beacon for pollinating insects.
The bigger the floral pollinators, the bulkier grew their predatory mantises, the better to catch, control and consume even well-armed bumblebees and wasps.
Other mantises resemble gnarled twigs, scraps of tree bark or decomposing leaves, blending in beautifully with forest underbrush, tree trunk or canopy, a cryptic approach to fool would-be prey and their own predators alike.
The smallest mantises flit around in the leaf litter of Australia and are no bigger than your pinkie nail, Dr. Svenson said.
Yet the stick-mimicking mantises of Africa can be nearly as long as your forearm.
These difficulties make cacao ever less appealing to producers; yields and profits are low, and the average cacao farmer is aging.
The next generation seems to be abandoning the family business.
Yet demand for chocolate is rising, especially as gargantuan markets like China and India indulge a taste for what used to be a treat primarily for American and European consumers.
A chocolate shortage may be on the horizon.
That is where Dr. Phillips-Moras project comes in.
The genetic diversity of cacao, on full display in the International Cacao Collection at C.A.T.I.E., may avert a chocolate crisis.
A Hybrid Solution

In the early 1980s, Dr. Phillips-Mora worked to identify the most naturally tolerant and productive cacao trees, then painstakingly hybridized the candidates to create novel varieties.
Breeding hybrid cacao clones is a lengthy process, and experts worldwide have largely failed in this endeavor.
But in 2006, Dr. Phillips-Mora released his first batch of hybrid cacao varieties.
Photo

Photo

In terms of disease resistance and yield, the differences were astonishing.
Dr. Phillips-Moras six hybrids produce on average about three times more cacao than standard varieties; under ideal conditions, the most prolific hybrids can produce six times more cacao.
After an 11-year trial, a hybrid called C.A.T.I.E.-R6 experienced a 5 percent frosty pod rot infection rate, compared to 75 percent infection for a control variety.
Advertisement Continue reading the main story

Our goal is not just to produce cacao, Dr. Phillips-Mora said.
Its also to give the basic living conditions to the farmers.
Most cacao farmers are very poor, because the system is based on material that doesnt have good yielding capacity.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
Trees that buck this trend could make the family business look more enticing to the next generation of cacao growers.
The C.A.T.I.E.
hybrids are now growing in all Central American countries, as well as in Mexico and Brazil.
Agricultural yield and disease resistance may benefit farmers, but a cacao crop is worthless if it produces bland or foul-tasting chocolate.
Chocolate is the epitome of gastronomic hedonism.
But unlike nearly every other modern effort to increase crop yields, Dr. Phillips-Moras breeding program incorporates fine flavor as a prerequisite.
Cacao varieties that dont impress expert palettes are discarded, no matter how well they grow.
Photo

The result of this protocol is that unlike many other crops favored for agronomics  the Red Delicious apple, the Cavendish banana  C.A.T.I.E.s cacao actually tastes good.
Chocolate makers are beginning to roast and package Dr. Phillips-Moras varieties.
Dandelion Chocolate, based in San Francisco, recently released a bar made from a mix of all six C.A.T.I.E.
hybrids.
I think honestly its going to be one of our most popular bars, said Greg DAlesandre, who heads cacao sourcing at Dandelion.
It has this nice balance of chocolaty and caramel notes, but it keeps it very accessible.
Dr. Phillips-Moras hybrid cacao varieties do not offer a perfect solution to all the crops challenges.
They cannot all self-pollinate, and some of the beans are small; they havent been properly tested in Africa or Asia, and they are not yet resistant to all the pathogens that afflict cacao globally.
Field trials are nearing completion on a new batch of clones bred to address some of these issues.
Advertisement Continue reading the main story

Moreover, the current roster of C.A.T.I.E.
clones were bred in response to known cacao production threats; the future will present new demands.
Pathogens evolve.
Unstable political situations in the developing world can affect agriculture.
Climate change will alter landscapes in unpredictable ways.
The solution is not to replace all cacao with the six available C.A.T.I.E.
varieties, but to be able to continue to diversify the cacao materials growing worldwide.
Like the Svalbard Global Seed Vault, the International Cacao Collection is a contingency against future disasters of unknown character.
Whatever fungal mutation may arise, wherever drought may strike, however chocolate tastes may change  there will likely be cacao genes somewhere in the collection that can form the basis of new hybrids to meet future challenges.
Photo

Still, Dr. Phillips-Mora worries about the future.
Though he works with deep-pocketed companies like Mars, Nestl and Hershey, the funds he receives are generally earmarked for specific research projects rather than for the maintenance of the collection and program for the future.
He estimates that he receives less than 5 percent of the funds necessary for proper upkeep of the collection each year.
So although Dr. Phillips-Mora retired three years ago, he plans to keep working until the solvency of the collection is ensured.
I will be very happy when I leave this institution to know that the collection will be protected financially, he said.
Its a treasure for everybody, for all the cocoa lovers.
Alexandra Horowitz, a psychologist at Barnard College who studies the behavior of dogs and has written several books about them, decided to give dogs a chance at showing self-recognition on their own, smelly terms.
In a recent study, she concludes that they do recognize the smell of their own urine.
While some researchers find the study intriguing, the scientist who first developed that mirror mark test doesnt think the evidence supports her conclusion.
Still, even the idea of a smell mirror is mind (nose?)
boggling.
I had always flirted with the idea in my head that there should be an olfactory mirror, Dr. Horowitz said, acknowledging that it could be horrifying for humans.
Marc Bekoff, a biologist and animal behavior specialist at the University of Colorado, Boulder, broke the ice, or actually the snow, in this kind of research around 20 years ago with what has become know as the yellow snow study.
He found that his dog, Jethro, recognized his own scent.
The evidence was that Jethro was more interested in snow marked by another dogs urine than in snow marked by his own, even if it had been surreptitiously moved  by Dr. Bekoff.
The research had its down side.
People who saw me move the pee around thought I was weird, and someone wrote a letter to the editor of the local paper that questioned what he was doing, he said in an email.
Dr. Horowitz took the testing a bit further, adding something like the mark on a chimps face: she set up dishes with different smells.
More Reporting on Dogs

A dogs own urine.
An unfamiliar dogs urine.
A dogs own urine along with another scent.
And, in some control tests, no urine and just the unfamiliar added scent.
Advertisement Continue reading the main story

She tested 36 pet dogs to see how long they spent with the different scents.
In many behavior tests, the time spent on a scent or a sight is taken as evidence of interest.
As she reported in Behavioral Processes, the dogs were least interested in their own urine, somewhat interested in another dogs urine, and most interested in their own altered urine.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
What that means, of course, is a matter for discussion.
She says it shows that the dogs recognize their own scent, finding it less interesting unless it has been messed with.
I dont think its precisely parallel to the mirror mark test, Dr. Horowitz said.
In an odor test, you cant use the mirror to restore what you think you should look like.
But, she said, her test and the mirror tests all show that theres this selective sensory investigation of something that comes from yourself, but is changed.
The scientist who developed the mirror mark test, Gordon Gallup, disagrees.
I dont think the results support the conclusions, he said.
He defines the self-awareness assessed in the mirror mark test as the ability to become the object of your own attention.
If you present a familiar odor and a modified version of that familiar odor, he said, that will increase a dogs attention.
The same would happen when a dog is presented with the odor of the owner who lives in the same house.
A definitive test would need to have a component in which the animal identifies the source and refers to it, the way chimpanzees point to the mark on their own faces.
Advertisement Continue reading the main story

Dr. Gallup also questions the tests of dolphins.
They twist their body around to look at a mark, but cant point to it as chimpanzees and other apes and human children do.
Laurie Santos, director of the Canine Cognition Center at Yale, said the study was a really important innovation.
She said that by using a mirror test based on smell, Dr. Horowitz was able to observe cognitive capacities that we didnt realize dogs had.
Because the mirror mark test depends on visual ability, many researchers, including Dr. Gallup, have been interested in extending testing to other senses.
Frans de Waal, a primatologist at Emory University, said by email, We need to move away from the mark test as sole source of information.
My view is that all animals have some level of self-awareness, they need to, and that the mirror mark test taps into a special kind, perhaps a rare kind, but we need more ways of testing.
Dr. Horowitz plans further tests, including using the scent of familiar dogs and modifications to that scent.
The testing methods may vary, but one thing is likely: There will be urine.
This could be your chance to make Sputnik beep again.
It was on Oct. 4, 1957, just 60 years ago, that the Soviet Union launched the first Earth satellite, Sputnik.
It was little more than an aluminum beach ball with a radio transmitter that sent out a regular series of radio beeps, but it expanded the Cold War to outer space, shook up American technological smugness and probably helped John Kennedy get elected president in 1960.
On Wednesday, just ahead of the 60th anniversary of its launch, a replica of the famous satellite is going on sale at Bonhams in New York City as part of their Air and Space Sale.
Another item on the block is the harness, complete with camera, and an oxygen tank for the rhesus monkeys that preceded Americas Mercury astronauts into space.
The original Sputnik fell out of orbit and burned up three months after its launch.
But test models and engineering replicas, allegedly from the laboratory where the legendary Sergei Korolev built them, have surfaced in museums and collections in recent years  some more authentic than others, said Robert Pearlman, a journalist and space historian who runs the website Collectspace.
Africa proved a bigger challenge.
There were fewer skeletons in museums, and most searches for genetic material failed.
The environment was partly to blame: DNA is more likely to survive in colder places.
Its been mad, watching all the advances in what we understand about European prehistory, said Jessica C. Thompson, an archaeologist at Emory University who does field work in Malawi.
Dr. Thompson was heartened by the discovery of ancient DNA in Ethiopia in 2015.
Those scientists succeeded for two reasons: The skeleton they discovered had been lying for thousands of years in a cool cave in the Ethiopian highlands, and the researchers developed new technological methods increasing the odds of finding even tiny bits of DNA.
More recently, Dr. Thompson teamed up with experts in ancient DNA and began searching for skeletons in Malawi.
Much of the country comprises tropical lowlands, but it also includes high-elevation plateaus where nighttime temperatures can plunge below freezing.
Eventually she and her colleagues discovered DNA-bearing skeletons as old as 6,000 years in caves in the highlands.
Other bones were discovered by archaeologists working in African countries, as well as in museum collections.
David Reich, a geneticist at Harvard Medical School and a co-author of the new study, and his colleagues analyzed DNA from 16 of these fossils, along with the one previously found in Ethiopia, comparing the genetic material to that of living people throughout Africa as well as on other continents.
This analysis allowed them to determine how living Africans descended from ancient populations, which are older in Africa than anywhere else on Earth.
Advertisement Continue reading the main story

Africa is now going to be fully included in the ancient genomics revolution, Dr. Reich said.
Were going to be able to do a lot of things in Africa that weve been able to do in Europe and elsewhere.
Africa is where our species evolved at least 300,000 years ago.
Previous genetic analysis of living Africans had suggested that their ancestors began splitting into distinct groups over 200,000 years ago.
Roughly 70,000 years ago some Africans moved out of Africa, becoming the ancestors of non-Africans.
In earlier studies, researchers had concluded that the hunter-gatherers who live today in the Kalahari Desert and other parts of southern Africa descend from the branch believed to be the first to have divided from other Africans.
But the new study suggests that there may be even older branches in the tree.
Something more complicated is going on, Dr. Reich said.
Dr. Reich and his colleagues found that some people in West Africa share a unique collection of genetic variants that suggest an even deeper ancestry, raising the possibility that an earlier population of humans in West Africa diverged from rest.
Thats quite a big new idea, Dr. Busby said.
The new study also sheds light on exactly which Africans spread to other continents.
The 4,500-year-old Ethiopian man discovered in 2015 had DNA linking him to non-Africans.
Today, only a single, small population of living Africans shares the same genetic link: Tanzanian hunter-gatherers called the Hadza.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
Theyre the group of living Africans most closely related to non-Africans, Dr. Reich said.
Once humans expanded out of Africa, there was little or no flow of genes between Africans and non-Africans for tens of thousands of years, the new study indicates.
But Dr. Reich and his colleagues discovered that a 3,100-year-old girl in Tanzania was profoundly different from the older East Africans.
A third of her ancestry could be traced to early farmers in the Near East.
Advertisement Continue reading the main story

Previous studies of living East Africans had hinted at some Near Eastern ancestry.
But the new analysis shows that people from the Near East spread into East Africa at least 3,100 years ago.
This puts a time stamp on this connection, said Pontus Skoglund, a postdoctoral researcher in Dr. Reichs lab and co-author of the new study.
Near Eastern genes were also found in a skeleton from South Africa about 1,200 years old; according to the researchers, some living South Africans carry this DNA today.
In all, these genetic patterns suggest that early farmers or herders from the Near East swept down through Egypt into East Africa several thousand years ago.
They then kept expanding over the centuries until their descendants reached the southern edge of the continent.
Around the same time, another expansion driven by agriculture was taking place in West Africa.
A people known as the Bantu spread from the region around present-day Cameroon and Nigeria.
They left a trail of distinctive iron tools that archaeologists have used to trace their migration into southern and eastern Africa about 2,000 years ago.
Archaeologists have studied this expansion for decades to learn what happened as the Bantu arrived in other parts of the continent.
The new genetic findings suggest that in some places, they may have pushed out the hunter-gatherers.
Up until 2,000 years ago, Dr. Thompson and her colleagues found, people in Malawi belonged to the same ancestral group as hunter-gatherers in southern Africa.
This was a hugely widespread population, she said.
But something happened: Living Malawians have no genetic connection to those who lived there before.
These ancient people must have disappeared virtually without descendants in Malawi.
Advertisement Continue reading the main story

Its possible, Dr. Thompson said, that Bantu farmers drove hunter-gatherers out of places like Malawi.
The surviving hunter-gatherers ended up in deserts and other places that werent good for crops and livestock.
In East Africa, the transition may not have been so stark.
There, modern people can trace much of their ancestry to the Bantu, suggesting a blending of populations.
But some people also inherited a mix of other ancestries, including genes from the Near East and some from the ancient East African hunter-gatherers.
Dr. Thompson is now digging into archaeological sites for evidence of the Bantu arrival in Malawi, looking for tools, bones and perhaps even more DNA.
We want to see if we can catch the timing of that transition and see if there was trade between the groups, or if the whole area was taken over, she said.
Ancient DNA in skeletons from western Africa would be just as valuable; it may hold profound secrets about the early history of our species.
But it wont be easy to find: The early archaeological record there is sparse, and there are few cold caves to search.
It is the major gap in our ancient DNA coverage, Dr. Skoglund said.
On their way over, they ran into Ms. Bedbrook and excitedly relayed their plan.
Theres no way these jellyfish sleep, she said, before joining them.
Photo

In the darkened lab, they observed a tankful of jellyfish pulsing infrequently and staying still for long periods of time  jellyfish that looked, in other words, like they were sleeping.
Ms. Bedbrook started to believe they were onto something.
To prove that jellyfish sleep, the students had to demonstrate that they fulfill three behavioral criteria.
First, the animals must undergo a period of diminished activity, but they must also be able to be aroused from this state, to distinguish sleep from other states, like comas.
Over six days and nights, the researchers monitored 23 jellyfish, which pulsed about 30 percent less at night than during the day.
If fed or poked in the middle of the night, the jellyfish would temporarily stir.
Next, the animals must exhibit decreased responsiveness to stimuli while sleeping.
Upside-down jellyfish get their name from the fact that they sit upside-down on the seafloor  they dont like to be suspended in water.
To test their responsiveness, the scientists placed the jellyfish in little cubbies with removable bottoms that were elevated within the tank.
When the researchers pulled the cubby bottoms out during the day, the creatures would immediately swim to the bottom of the tank.
At night, however, they would sluggishly float around at first.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
Last, the animals must show an increased need for sleep if they are kept from it, so the biologists pulsed water through the jellyfishs tank every 20 minutes at night to prevent the creatures from sleeping continuously.
The following day and night, the jellyfish exhibited much lower levels of activity than normal, suggesting sleep deprivation.
As a bonus, the researchers also showed that jellyfish get sleepy when exposed to melatonin, just as humans do  a hint that their underlying sleep mechanism may be similar to ours.
Together, these experiments do a good job of demonstrating that jellyfish fulfill the most fundamental criteria for sleep, William Joiner, a sleep researcher at the University of California, San Diego, said in an email.
The study also provides new fodder for thinking about the origins and functions of sleep, both of which are still unknown, said Dr. Joiner, who was not involved in the study.
Sleep is often associated with having a brain because the behavior seems to be required for memory and learning, and because shared genes and mechanisms for sleep have been identified in all sorts of animals with brains, from worms and flies to mice and humans.
But what weve found, at least in this jellyfish, is you dont need a brain to sleep, Mr. Abrams said.
Dr. Rosas and his team studied ancient Neanderthal remains recovered from a cave system in Spain known as El Sidrn, where archaeologists have found the remains of more than a dozen individuals, including the childs mother and younger brother.
The first thing the researchers needed to do with their child specimen was determine how old he was.
So they peeked inside his mouth, which had a mix of 30 baby and adult teeth.
By cutting into his teeth they were able to use a microscope to count bands in the enamel, which grow similarly to tree rings.
From their investigation they determined the child was just under eight years old when he died.
They did not find any signs on his bones that would have clued them into the cause of his death.
By investigating the boys cranium, the researchers found that it was only 87.5 percent the size of a full grown Neanderthals cranium.
That differs from anatomically modern human children, who at age seven have craniums that are about 95 percent the size of an adults.
Photo

Because cranium size is a good indicator of brain size, the findings suggest that Neanderthals large brains took longer to grow to adult size than our brains do.
Though the team did not have the childs complete skull, they were able to compare the available fragments with a skull from a different Neanderthal and reconstruct the missing parts.
Advertisement Continue reading the main story

The team also found the Neanderthal child still had several unfused vertebrae.
In modern human children, those vertebrae are fused around the ages of four to six.
Despite the differences in brain and spine development, the team found that in many ways the Neanderthal child was no different from a modern human child, especially in the elbows, wrists, hands and knees.
Both seem to have experienced similar growth patterns, like having arms and legs that grew slowly between infancy and puberty, according to Dr. Rosas.
Photo

As he went through the whole skeleton, comparing it to skeletons of Homo sapiens, these particular differences stood out, contributing to the mystery of what accounts for the early differences between the two species.
We were surprised because we were expecting some differences, said Dr. Rosas.
But it was, Similar, similar, similar  oh, different  similar, similar  oh, different.
Luis Ros, a paleoanthropologist also at The National Museum of Natural Sciences in Madrid and co-author on the paper, said at a news conference that the new finding about growth rates fits with the generally held idea that Neanderthal brains were bigger.
But to confirm that hypothesis, they will need to further investigate the craniums in the cave, looking for remains between childhood and adulthood, to complete their life cycle of Neanderthals.
Dr. Delmer pioneered research on how cotton synthesizes cellulose, the primary compound in its fibers.
She wanted to see evidence that the researchers novel molecules formed stable chemical bonds with cellulose.
If they didnt, she said, then the prospects that they can survive harsh treatments when incorporated into fabrics would seem less certain.
Photo

Filipe Natalio, lead author of the study and a scientist at the Weizmann Institute of Science in Israel, said his team had similar concerns.
He and his colleagues used chemical and physical analytical techniques to show that the cellulose in the fibers had undergone changes.
But thats not the same as showing that the new molecules were chemically woven, via enzymatic reactions, into strands of cellulose, Dr. Delmer said.
Instead, Dr. Natalios team could just be picking up signatures of their molecules hanging out in the cells, but not forming long-term linkages.
Dr. Natalio responded that English is not his native language, and to him, words like incorporated or integrated covered the possibility that the molecules got into the cotton fibers without binding to its components.
We didnt claim also that there were linkages, he said.
Given limited room to explain every last detail in the paper, he continued, we had to make a lot of sentences that were very vague and encapsulate information without proving it, which is awkward.
Beyond being unconvinced of the papers central claim, cotton researchers were skeptical about whether Dr. Natalios system would ever evolve past proof of concept.
He spoke in an interview about a not-so-distant future with cotton and other plants growing in hydroponic greenhouses, bathed in fluids with all sorts of customized molecules.
But Dr. Natalio and his colleagues were experimenting with small amounts of cotton embryos in the lab, not whole plants.
Dr. Natalio acknowledged that for the technology to work with actual plants, he would have to synthesize whole new sugars, one of the most delicate chemistries you can do.
Moreover, Dr. Natalios group found that the modified fibers they produced were actually weaker than raw cotton fibers.
Thats a big no-no in the cotton industry, since it wreaks havoc in yarn production and goes against the durability consumers want from cotton, said Don Jones, a director of agricultural research for Cotton Incorporated, a trade organization based in North Carolina.
Advertisement Continue reading the main story

Cotton experts also noted that the authors cultured their cotton for an unusually short amount of time, many experiments presented in the paper had no information on replication and there were statistics missing in places one might expect to find them.
Mislabeled chemicals in two supplemental figures led Science to publish its editorial expression of concern.
Several experts wondered if the people involved in the peer review of the study were materials scientists, chemists or physicists, not biologists.
When asked what fields the editors and reviewers for this study belonged to, the journal said in an email that Science papers are assigned to a staff editor who identifies the types of expertise needed to evaluate all aspects of the manuscript under consideration, but offered no specifics.
Two of the three biologists who are experts in cotton only agreed to speak on the condition of not being named, out of concern that publicly raising questions about other scientists work could boomerang on their careers.
One of them spotted the misnamed chemicals and contacted Science, leading to the journals expression of concern about the paper.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
Ivan Oransky, co-founder of Retraction Watch, a site that reports on problems with scientific research, said the researchers concerns were understandable, but in the long term its bad for science if researchers are not willing to have public, respectful debates.
He added that there are ways to make the peer review process more transparent, like having journals publish reviewers reports and editorial decisions along with articles.
Dr. Natalio said his team is preparing a list of corrections to be attached to the article.
He added that they have preliminary data that suggests the stable chemical linkages that Dr. Delmer had wanted to see.
But he refrained from putting that data in this paper because it was incomplete and he believed it would prolong the review process.
One of the rules of publication?
Dont claim what you cant prove, he said.
But they also learned that some sea turtles are still declining  like leatherbacks in the Eastern and Western Pacific.
Their findings support assessments made by the International Union for Conservation of Nature, which lists six of seven species as vulnerable, endangered or critically endangered.
Advertisement Continue reading the main story

In contrast with some other at-risk species, perhaps sea turtles have been easier to manage because their threats are more tangible: They are accidentally trapped by fishermen or harvested by others as delicacies, aphrodisiacs or decoration.
In the most extreme cases, like in Tortuguero, Costa Rica, nearly all female green turtles at one point had been exported for turtle soup.
But conservation efforts there dating back to the 1950s made an impact, and protecting beaches, regulating fishing and establishing marine protected areas have helped save turtles in many locations.
This isnt often the case in conservation stories of animals, like endangered caribou, which face threats that are more difficult to manage.
Photo

But to truly know how well conservation is working, the researchers found, its best to look at long-term trends (although short-term data has its uses).
Thats because most sea turtle species only nest when foraging is good, and from year to year, the number of nests found on a beach can vary dramatically.
Detecting whether a juvenile sea turtle survives long enough to make babies can take 10 to 30 years while it matures.
They were surprised to find that with adequate protection, even small populations of turtles have a chance of survival.
In an area called French Frigate Shoals in Hawaii, for example, green sea turtles increased nest numbers from around 200 in 1973, when the Endangered Species Act was signed, to around 2,000 in 2012.
This species is now considered of least concern, by the International Union for Conservation of Nature.
Yet research is still lacking.
For all sea turtles, most male to female ratios are unknown, which is an important aspect of reproduction and appears to be altered with increasing sand temperatures, skewing births toward more females.
And a huge initiative to collect more data on flatback turtles in and around Australia may be complicated by a recent announcement that the country will shrink its marine protected areas.
Dr. Mazaris said his paper is a tale of cautionary optimism.
He commends conservationists working to save turtles over the past 70 years, but long term efforts need to be supported.
Photo

Q.
Who were the artists who made prehistoric cave paintings in France and Spain?
Directly proving anything definitive about the executors of paintings made 33,000 years ago, in the case of the Chauvet Cave in France  or even as long as 40,800 years ago, in the case of the El Castillo cave in Spain  is nearly impossible.
Given the passage of time, most clues are circumstantial and aesthetic.
Several theories about the well-hidden and anatomically accurate portrayals of animals suggest that they involved important and secret magical or religious rituals, so that only the best work would be worthy.
Hunter-artists would presumably be very familiar with the anatomy of animals after close observation of their prey.
More Reporting on Archaeology

After the famous discovery of the Lascaux Cave galleries of animals, the early archaeologist Henri Breuil theorized that they involved rituals of sympathetic magic designed to insure a good hunt.
However, some of the animals portrayed in the Chauvet cave, like lions, panthers, bears and hyenas, were not hunted for food.
Later ideas included the possibility that the art was produced in shamanic rituals under the influence of hallucinogenic substances, or that the paintings were sometimes simply outpourings of creativity by professional and well supported artists.
Still another researcher, R. Dale Guthrie, points out that the whole body of cave art shows a wide range of abilities: There are many unskilled Paleolithic drawings that are rarely reproduced in art books.
Times journalists around the world bring you a new 360 video every day.
The science itself  like most attempts to link brain biology to behavior  is murkier.
In recent decades, researchers have made extraordinary strides in understanding the workings of brain cells, neural circuits and anatomy.
Photo

Yet drawing a direct line from those basic findings to what people do out in the world is dicey, given the ineffable interplay among circumstance, relationships and personality.
What scientists  from such diverse fields as psychiatry, neurology and substance use  can say is that the arrows seem to be pointing in the same direction.
A number of brain states raise the risk of acting out violently, and the evidence so far, while incomplete, suggests that C.T.E.
may be one of them.
Dr. Samuel Gandy, director of the N.F.L.
neurology program at Mount Sinai Medical Center, said his research showed that rage and irritability are far and away the most prominent symptoms among former players with likely C.T.E.
His group has identified 10 of 24 former players who probably have the disease.
Scientists at Boston University, who reported the findings on Mr. Hernandez, have described similar behavior in many of the more than 100 players they have evaluated.
The caveat for both research efforts is that these samples are selective: Almost all of the players had signs of possible C.T.E.
before being studied, which led the players and their families to participate and to donate their brains for research.
It may still be that most of the athletes in violent sports who develop the signature brain pathology, especially at modest levels, are no more irritable than anyone else.
But an important hint to the contrary comes from a more mature corner of brain science: dementia research.
People with advanced dementia often begin to act in uncharacteristically aggressive ways, as many caregivers can attest.
In a recent study of dementia patients, Swedish researchers found that 97 of 281 dementia patients had a history of aggression.
Advertisement Continue reading the main story

Those who acted out earliest in the progression of their disease had so-called frontotemporal degeneration  that is, damage concentrated in the frontal and temporal lobes of the brain.
This is where C.T.E.
shows up, too.
In frontotemporal degeneration, a purported association has been made with criminal behavior, said Kevin Bieniek, a research fellow in the Dickson Neuropathology Lab at the Mayo Clinic College of Medicine in Jacksonville, Fla.
Different disease, but some clinical and pathological parallels to C.T.E.
All of this is far from definitive, given the wide variety of factors that affect motivation and impulsive behavior.
Substance abuse is a prominent example.
Mr. Hernandez was no stranger to illicit drugs, according to testimony at his murder trial.
Studies strongly suggest that people who are mentally unstable, particularly those with severe paranoia, are more likely to become violent when under the influence of alcohol or other drugs.
Photo

The link between steroids  such as testosterone  and rage is another confounding factor.
It is rarely clear in mature athletes whether they have used performance-enhancing drugs, or how much.
Testosterone aggravates aggression in the absence of pathology, Dr. Gandy said.
If theres pathology, its likely to make things worse.
As cases like Mr. Hernandezs and others continue to move into the courts, judges will be making decisions based on limited, piecemeal scientific evidence.
These wont be easy decisions to make.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
The only way to diagnose C.T.E.
is at autopsy.
A number of scientists, including Dr. Gandys group, are working to refine brain imaging tests intended to detect the signature pathology in living patients.
Those tests are not yet definitive.
Over the past decade, many courts have been reluctant to admit brain scans as exculpatory evidence.
Theres no serious argument about whether violence comes from the brain, said Owen Jones, director of the MacArthur Foundation Research Network on Law and Neuroscience at Vanderbilt University.
Advertisement Continue reading the main story

Its just hard to make a credible claim that a particular brain injury has caused a particular act of violence.
Photo

More than half of all American teenagers are getting vaccinated against human papillomavirus, and the rate is rising over time, according to a new report from the Centers for Disease Control and Prevention.
Sixty percent of adolescents received one or more doses of the HPV vaccine in 2016, an increase of 4 percentage points from 2015, researchers found.
About a decade ago, the figure was less than 30 percent.
Were really encouraged to see this finding, said Shannon Stokley, a co-author of the report and associate director for science at the Immunization Services Division of the C.D.C.
The vaccine protects against strains of HPV that can cause cancers of the cervix, penis, anus and back of the throat.
Close to half of all Americans are infected at any given time, and nearly 32,000 get cancer from the virus each year.
Advertisement Continue reading the main story

About 90 percent of those cases could be prevented with the vaccine, according to the C.D.C.
The agency used to recommend three doses, but new guidelines introduced last year amended that to two doses for adolescents younger than age 15.
See how monkeys teach manners, elephants show empathy and ants imitate water in ScienceTake, combining cutting-edge research from the world of science with stunning footage of the natural world in action.
Osiris-Rex  a shortening of Origins, Spectral Interpretation, Resource Identification, and Security, Regolith Explorer  was launched last year and circled the sun, returning for Fridays flyby.
It is to arrive at Bennu in about a year.
The asteroid periodically crosses Earths orbit, and theres even a 1-in-2,700 chance that it could hit Earth between 2175 and 2196.
Advertisement Continue reading the main story

Scientists believe that Bennu, a dark asteroid about 500 yards in diameter, is full of carbon-rich molecules dating back to the birth of the solar system 4.5 billion years ago.
Those molecules might have been the ingredients that led to life on Earth.
Osiris-Rex will attempt to collect a few pounds of rock and dirt from Bennu by gently bouncing off the surface like a pogo stick and collecting material that it disturbs with a burst of nitrogen gas.
It will bring the samples back to Earth in 2023 for closer study.
For the flyby, there is no chance that Osiris-Rex, about the size of an S.U.V., will veer off course and slam into Earth.
Spacecraft navigators have become adept at using precise flybys as slingshots to steer spacecraft through the solar system.
NASAs New Horizons spacecraft, for example, added nearly 9,000 miles per hour to its speed with a Jupiter flyby in 2007, shortening its travel time to Pluto (It still took another eight years to get there).
Dr. Moreau said Osiris-Rex will pass within a kilometer of the targeted spot above Earth.
The timing is precise too, within a few tenths of a second.
It will make its closest approach to Earth at 12:52 p.m. Eastern time on Friday.
Dr. Moreau said his team will face larger navigational challenges once Osiris-Rex gets to Bennu in 2018.
Its the smallest object that has ever been orbited by a spacecraft, he said.
And thats exciting.
The European Space Agencys Rosetta spacecraft spent a couple of years exploring Comet 67P/ChuryumovGerasimenko, a comet about 2.5 miles wide.
Bennu is about one-eighth that diameter, and Osiris-Rex will come within a kilometer of the center of Bennu, Dr. Moreau said.
We are much closer to the object than Rosetta was, he said.
It means a lot of the errors in your estimation of the trajectory and navigation are less forgiving.
With gravity near the asteroid so slight, the navigators need to keep track of even very minute forces, including heat from the spacecraft radiators and the momentum imparted by particles of light hitting the solar panels.
Photo

For Friday, the Osiris-Rex team is encouraging amateurs to photograph the passing spacecraft and share the images on the mission website.
The Desert Fireball Network, a project based at Curtin University in Perth, Australia, will use high-end digital single lens reflex cameras to photograph Osiris-Rex from different angles.
Usually, the project tracks meteors burning up in the Earths atmosphere.
Advertisement Continue reading the main story

This time, the different angles will allow scientists to reconstruct the three-dimensional path that Osiris-Rex took as it swung by.
To fulfill his flow-finding mission, Mr. Wheal wants to bring what he calls his Dojo Domes to locations around the world.
Next year, he and his partners hope to build a one-million-square-foot complex in Vancouver, British Columbia, with a medical emphasis, combining brain-imaging technology with simpler equipment.
Advertisement Continue reading the main story

Mr. Wheal began to envision gatherings of this sort in 2007, while he was teaching at Esalen, the spiritual retreat in California.
With Steven Kotler, a journalist, he founded the Flow Genome Project, based in Austin, Tex., and dedicated to gathering the latest science behind flow states.
Its board of advisers includes neuroscientists, filmmakers and a kiteboarder.
It was his book, Stealing Fire, written with Mr. Kotler and published earlier this year, that attracted many of the flow campers to Utah.
In it, Mr. Wheal and Mr. Kotler consider the question of peak human performance, describing how so many powerful companies and individuals are now trying to optimize their own brains, in ways both legal and illegal.
They offer case studies from the Navy SEALs and Google, arguing that what the world today faces wicked problems, unprecedented and complex, that require creative solutions, the kinds that are most likely to come not from staid meetings in conference rooms but rather from non-ordinary states.
Flow, they write, is associated with six neurotransmitters: dopamine, serotonin, oxytocin, norepinephrine, anandamide and endorphins.
Knowing the neurochemical profile of flow means, in theory, people can devise ways of achieving it more often, more reliably and more quickly.
The new generation of flowsters are excited, perhaps, that using the advances of neuroscience, they might not have to meditate every day for 10 years to gain access to these layers of their brains.
Photo

This was a significant investment of time and money for me  this tells you how compelled I was to come here, said Alexandre Lang-Willar.
At 28, Mr. Lang-Willar is in some ways the embodiment of Mr. Wheals target demographic: the high achiever who grasps the brass ring, only to discover he craves something more.
Mr. Lang-Willar quit his job as a Goldman Sachs analyst and has created a dating app with his father called Invite and Meet, centered on live activities, that will be introduced later this year.
Reading Stealing Fire, Mr. Lang-Willar said, he became convinced that nothing less than a cultural awakening was underway.
Advertisement Continue reading the main story

By 8 a.m. each morning, the flow campers lay prone and shoeless in the Dojo Dome, moving back and forth on brightly colored foam rollers.
Other daily activities included balancing and bouncing on big yellow balls; acro-yoga, in which partners learn to lift each other in the air; and strapping into special contraptions, like Mr. Wheals 360 Swing, which allows those courageous enough to propel themselves, standing up, all the way around the swings axle in a complete loop.
Newsletter Sign Up Continue reading the main story Of the Moment The lifestyle newsletter from the Styles, Travel and Food sections, offering the latest trends to news you can use.
Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
All of these undertakings were in the service of honing a crucial element in flow, what Mr. Wheal refers to as embodied cognition: integrating our whole minds and bodies through specific exercise, based on the science showing that physical movement directly affects how we think and feel.
They are tapping into spiritual intelligence that before now was only really talked about in a religious context, Kristen Ulmer said, sitting outside the Dojo Dome one morning.
Ms. Ulmer, formerly the top ranked extreme skier in the world, has also written a book, The Art of Fear.
She went on: A lot more people are saying theyre spiritual but not religious  but what does that really mean?
I would say sports and movement are the most oft way we access a spiritual experience and transcend our ego, but theyre the least discussed and least understood.
Other patients would require a slightly higher or lower concentration to achieve the same effect, but the variations were not large.
That led to their introduction in 1965 of a concept, called the minimum alveolar concentration, or MAC, that quickly became the standard measure of potency for anesthetic gases.
Because powerful anesthetics work at lower concentrations and weaker ones require higher doses, a lower MAC value would indicate a stronger drug.
Anesthesiologists use MAC values when planning doses needed for surgery.
The values are highly consistent from one patient to another and even among animals.
For any given drug, about the same concentration can anesthetize a 200-pound man, a smaller woman, a dog or a rat.
The amount needed to reach that concentration differs depending on the patients size, but the effective concentration itself does not change.
Dr. Shafer said the technique devised by Dr. Eger and his colleagues made the administering of anesthesia far safer and has saved millions of lives.
Photo

In later work, Dr. Eger identified new drugs that could be used as anesthesia, such as isoflurane, sevoflurane and desflurane, which are still the most widely used general anesthetics.
Ted Eger revolutionized modern anesthetic practice, and led the way to the development of the anesthetic gases used tens of millions of times a year, Dr. Michael A. Gropper, the chairman of the department of anesthesia and perioperative care at the University of California, San Francisco, wrote in an email.
Advertisement Continue reading the main story

Edmond I. Eger II was born on Sept. 3, 1930, in Chicago.
(His parents gave him a middle initial but not a middle name.)
His father was an advertising executive, and his mother, the former Miriam Newmann, was a homemaker.
As a boy, Dr. Eger skipped at least one grade, became a whiz at checkers and led the Hyde Park High School checker team to two city championships.
He graduated at 15, but, as a bored and indifferent student, wound up in the bottom 20 percent of the class.
He was soon hired to sell womens shoes, but after only one day on the job he decided he had had enough and resolved to apply for college.
He was accepted at Roosevelt College in Chicago, where he went from not working at all to working his butt off, Dr. Shafer said.
After a year, he transferred to the University of Illinois, where he majored in chemistry with a minor in math.
He went to medical school at Northwestern University.
In 1955, the same year he graduated from Northwestern, he married Dollie Ross, a speech therapist.
The marriage ended in divorce in 1983.
In 1996, he married Dr. Lynn Spitler, an immunologist, who survives him.
Dr. Eger is also survived by three daughters, Cris Cadence Waste, Doreen J. Eger and Renee R. Eger, and a son, Edmond Eger III, all from his first marriage; a half brother, Larry Eger; two stepchildren; seven grandchildren; and six step-grandchildren.
After completing his internship and residency, Dr. Eger served for two years as a captain in the medical corps, based at the Army hospital at Fort Leavenworth, Kan. From 1960 to 2006 he was a faculty member at the University of California, San Francisco.
He was an author of more than 500 scientific articles and an author or editor of seven books.
He received every award known to man in his specialty, Dr. Shafer said.
Dr. Eger spent the last 20 years of his career trying to understand how inhaled anesthetics work.
The drugs and their effects remain a mystery.
The same concentration that knocks out a person will anesthetize a sea slug or an amoeba, and will even paralyze a fern that normally curls up when touched, Dr. Shafer said.
The universality of those reactions suggests that the drugs are tapping into some biological mechanism that evolved eons ago.
Dr. Eger regretted that he had not been able to discover that mechanism, writing in his autobiography, The ah ha!
moment, the thrill of solving the hardest puzzle in all of pharmacology, awaits another investigator.
The alliance includes California, Colorado, Connecticut, Delaware, Hawaii, Massachusetts, Minnesota, New York, North Carolina, Oregon, Rhode Island, Vermont, Virginia and Washington; plus Puerto Rico.
All but two states are led by Democratic governors.
Advertisement Continue reading the main story

Yet theres a caveat to this announcement: Because the states in the alliance only represent 36 percent of the nations population, the United States as a whole is still expected to fall short of Mr. Obamas pledge.
A previous Rhodium Group analysis estimated that total United States emissions would likely drop just 15 to 19 percent by 2025 as Mr. Trump dismantled federal climate policies.
For the country to meet its commitments under the Paris agreement, further action by states would be needed.
The alliance could try to persuade other governors to ratchet up their ambitions, though those prospects are uncertain, since barriers to climate policy in Republican-leaning states are often as much political as technical.
Or the alliance states could pursue even deeper cuts themselves.
But here, experts say, they may face practical limits on how far they can go to tackle global warming on their own.
What States Can, and Cant, Do

In theory, state governments have plenty of ways to cut emissions without federal help.
They can require electric utilities to use more renewable power, modify building codes and impose tougher efficiency standards on appliances.
They can shape transportation infrastructure.
California is allowed to require automakers to sell more electric vehicles, and any state can join its program, as several in the Northeast have done.
Within the climate alliance, most of the efforts to date have focused on cleaning up electric grids.
Collectively, emissions from electricity in the alliance states are expected to drop by half between 2005 and 2025, the Rhodium Group analysis found.
But many experts consider these changes in the power sector the low-hanging fruit of climate policy, aided by a boom in natural gas production that has forced many coal plants into early retirement.
The real test, analysts say, will come as states try to juggle ever-greater shares of intermittent renewable power and tackle other, harder-to-decarbonize sectors like transportation and industry.
Here, the outlook is murkier.
According to the Rhodium Group, emissions from cars and trucks in the alliance states are expected to fall just 18 percent by 2025.
By contrast, emissions from sectors like buildings, heavy industry and agriculture are hardly expected to decline at all.
These sectors are expected to make up more than 60 percent of alliance states emissions by 2025.
Photo

That hints at one limit states may face in pursuing further climate action.
New technologies  like better batteries to help integrate wind and solar, or carbon capture for cement plants  could make the task of deeper decarbonization easier.
But historically, the federal government has led the way in researching and developing these technologies.
And with the Trump administration proposing deep cuts in federal energy research, it is unlikely that process will speed up anytime soon.
I see state action as important, but ultimately, if were serious about deep decarbonization, the federal government needs to get back involved, said David M. Hart, who studies energy policy at the Information Technology and Innovation Foundation.
Advertisement Continue reading the main story

There are other risks to a states-only approach.
According to Christopher Clack, chief executive of the grid-modeling firm Vibrant Clean Energy, the best way to fully decarbonize the United States electricity system with renewable energy would be through a national grid that allows optimally placed wind and solar resources from far-flung regions to balance each other out in the face of weather fluctuations.
But such a system would most likely require federal planning.
Right now, solar and wind are still a relatively small slice of electricity, so this isnt a big problem yet, Mr. Clack said.
But as these sources grow, he said, individual state efforts to build out their own renewable bases without broader coordination could lead to a system that is less well-suited to handling large quantities of wind and solar.
A Virtue of Necessity

For their part, the alliance states are trying to overcome these hurdles.
New York, for instance, is trying to nurture energy innovation on a small scale through a state "green bank" that helps companies bring riskier new technologies to market.
While this is no substitute for basic energy research at the national labs, state officials say it can help advance incremental innovation around technologies that are closer to market.
Were trying to make a virtue of necessity, said Richard Kauffman, Governor Cuomos chairman of energy and finance.
In an ideal world, it would be fantastic if we had the federal government providing leadership and investing in R&D and energy infrastructure.
But thats not only not the world were in  with this administration, its not even close to the world that were in.
States also face the risk that the Trump administration could try to thwart their efforts.
Officials in California, for instance, are preparing to challenge any effort by the federal government to pre-empt their electric vehicle mandate on automakers.
And it remains to be seen if the climate alliance can keep adding members.
There was already heavy political pressure in these states to move forward on clean energy, said David G. Victor, a climate policy expert at the University of California, San Diego.
But just because these states demonstrate that it can be done doesnt mean the politics suddenly shift in places like Kentucky or Kansas.
The ultimate significance of these state efforts, Dr. Victor said, may be to help prevent international climate efforts from collapsing, by reassuring other countries that the United States has not totally abandoned the issue.
Now that the rest of the world is over the initial reaction to Trump, theyre trying to figure out whats still real and whats not in U.S. policy, he said.
And these states can offer a starting point for other countries to gauge U.S. climate action, even when whats happening in Washington is chaotic.
But, he added: It all happened really quickly.
Mr. Friedemann, the administrative director of Lebus, could not be reached by email or telephone on Wednesday.
But he was quoted by the German news organization RBB24 as saying that he had made the decision to shoot when he was informed that the animal could be dangerous.
European bison, also known as wisent, are listed as vulnerable, or at risk of extinction, by the International Union for Conservation of Nature, whose Red List is the worlds most comprehensive inventory of threatened plant and animal species.
They are native to Belarus, Lithuania, Poland, Romania, Russia, Slovakia and Ukraine, according to the I.U.C.N.
There are about 4,000 free-roaming bison in herds, and a few thousand more in parks or other designated but open areas, according to the European Bison Conservation Center.
Threats include a lack of knowledge about the animal, habitat loss, a narrow genetic base making it weak against disease and the absence of a shared strategy among European nations to support the bison population, said Rewilding Europe, a conservation group that put the bison's population at about 3,000.
Wild bison had not been seen in Germany for over 200 years, said Christoph Heinrich, the director of conservation at the WWF in Germany, in announcing the lawsuit.
Efforts to reintroduce the animals, which can weigh over one ton, have taken place in recent years in western Germany.
The sighting in the east was an anomaly, despite being in line with the impulses of male bison, Mr. Klose said, which tend to roam more than female ones.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
It was obviously a sign that there are suitable habitats in Germany, and if there is a possibility that these big herbivores can come to it, it is a good example that we have to be prepared for that, he said.
A legal framework exists, but if people are overwhelmed and dont know what to do, there is a wildlife management issue.
He said the WWF was suing based on conservation laws, which designated the bison as a protected animal.
We want to make a sign that endangered, rare, protected animals do have a right to live here in Germany and form an integral part of our ecosystem, and we think there needs to be harmonization between the countries, he said.
Animals dont know country borders.
Advertisement Continue reading the main story

He said the body of the bison was being examined and would be prepared for display, likely in a museum in Potsdam.
Conservationists are working on designating new places for large bison populations to live and breed, said Wanda Olech-Piasecka, a coordinator for the European Bison Conservation Center.
She said the bison had roamed for days near the border and his presence was reported to nature protection authorities in Poland.
She understood from news reports in Germany that local officials in Lebus, who could not be reached on Wednesday, had tried to find a veterinarian to handle the animal but they apparently panicked, she said.
For me, this is the reason we must tell people all around Europe and the world what wildlife means, Ms. Olech said.
Those animals must live with us, and we must learn how to treat them and to let them live.
She added: The problem was they did not start to look for somebody who could solve the problem and answer questions about the species.
The decision was made without research and this is a pity.
Photo

One year after giant pandas graduated from endangered to vulnerable, a welcome designation after 28 years, Chinese scientists have sobering news: The animals natural habitat in China is in serious danger.
In a study published in Nature Ecology & Evolution on Monday, researchers report that suitable panda habitats have significantly and steadily declined since 1990, the year the International Union for the Conservation of Nature first classified the animals as endangered.
That could make any gain in Chinas wild panda population a short-lived conservation victory.
Logging, human encroachment, road construction and agriculture have conspired to divide panda habitats into tiny sections, a process known as fragmentation, the study said.
Photo

Ouyang Zhiyun, a professor of environmental science at the Chinese Academy of Sciences, and his colleagues studied 40 years of satellite data to reach their conclusions, and are urging the Chinese government to take specific steps to restore panda-friendly environments.
Advertisement Continue reading the main story

Giant pandas are a national icon of China, the only place in the world where they live outside of captivity.
Years of Chinese government efforts to reverse their dwindling numbers, such as the restoration of bamboo forests and establishment of national habitat reserves, helped bring the animals back from the brink.
They were declared no longer endangered in September 2016 after population estimates reached 1,864  not counting cubs.
Thats up from a low of about 1,200 in the 1980s.
Photo

Two strong earthquakes, 12 days apart, have shaken Mexico this month, crumpling buildings, sending panicked people into the streets, and together killing hundreds of people who were unable to escape the destruction.
Just before midnight on Sept. 7, a magnitude-8.1 earthquake  the most powerful to hit Mexico in a century  rattled the country, doing the brunt of its damage to the southern part, which was closest to the quakes epicenter off the Pacific Coast.
Then, on Tuesday, as officials continued their cleanup and recovery efforts, an earthquake with a preliminary magnitude of 7.1 struck about 100 miles southeast of Mexico City, causing severe and sustained shaking in the capital.
It occurred on the anniversary of a quake in 1985 that killed as many as 10,000 people in Mexico.
Although it might seem unusual for two strong earthquakes to hit relatively near each other in such a short time, scientists say strong earthquakes can sometimes alter stresses nearby, leading to subsequent quakes.
But they are not sure yet if that is what happened with these two.
Heres a look at some of what they know about earthquakes, how often they strike, and where the most powerful ones can occur.
Why does Mexico keep getting hit with powerful earthquakes?
Photo

Mexicos location makes the country prone to strong earthquakes because it is in a so-called subduction zone.
Subduction zones are the parts of the earth where one slab of the crust is slowly sliding under another.
In Mexicos case, an oceanic plate  the Cocos  is gradually sinking beneath a continental plate  the North American.
Over time, stress builds because of friction between the slabs, and at some point, the strain becomes so great that all the pent-up energy is released in the form of an earthquake.
The subduction zone responsible for the two recent quakes runs along the western coast of Central America, from Central Mexico to Panama, said Gavin Hayes, a research geophysicist with the United States Geological Survey.
Other subduction zones are found across the globe  and experts say they are responsible for the worlds most powerful earthquakes.
In fact, earthquakes with a magnitude of 9.0 or higher can occur only in subduction zones, Dr. Hayes said.
Relatively recent examples of such megathrust quakes include a magnitude-9.1 quake off Japan in 2011, a magnitude-9.1 quake in Indonesia in 2004, a magnitude-9.2 quake that struck Alaska on Good Friday in 1964 and a magnitude-9.5 quake that struck Chile in 1960  the strongest quake ever recorded.
Why werent the Mexico quakes even stronger?
Both earthquakes that struck Mexico this month occurred within the sinking Cocos Plate, rather than between the Cocos Plate and the North American.
Had the recent quakes occurred between the plates, it would have produced a megathrust.
Quakes at plate boundaries usually involve larger faults and thus release more energy, generating shaking over larger areas.
But they also usually occur farther from the surface, Dr. Hayes said.
Newsletter Sign Up Continue reading the main story Get the Morning Briefing by Email What you need to know to start your day, delivered to your inbox Monday through Friday.
Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
Earthquakes that happen inside a plate tend to be weaker, but closer to the surface.
Consequently they can cause major damage to whatever is sitting above them.
The Sept. 7 earthquake was stronger than the one that struck less than two weeks later, but experts said it could have less of an impact because the epicenter was farther from densely populated areas.
The more recent quake was much closer to Mexico City, which Dr. Hayes said is built on a sedimentary basin.
That kind of geology amplifies an earthquakes shaking more so than, say, an area with more bedrock.
How often do strong quakes happen?
Typically, about one quake of magnitude 8 or higher occurs somewhere in the world every year; there are about a dozen quakes of magnitude 7 or higher annually, Dr. Hayes said.
So far, 2017 has actually been a quiet year for earthquakes, Dr. Hayes said.
According to U.S.G.S.
data, about 4,200 earthquakes of magnitude 4.5 or higher have occurred around the world so far this year.
Over the same period in 2016 and 2015, about 5,100 quakes of the same strength occurred.
In 2014 there were closer to 6,000.
Where might a powerful quake strike in the United States?
There are two subduction zones in the United States.
One, which includes Alaska, generated the 9.2 quake in 1964, and therefore, Dr. Hayes said, another quake of that strength probably wont happen for hundreds of years.
The other, the Cascadia subduction zone, runs along the Pacific Coast on the western borders of Oregon and Washington.
There, the Juan de Fuca Plate is edging east and slipping slowly beneath the North American Plate.
This Cascadia subduction zone last generated a magnitude-9.0 earthquake in the Pacific Northwest in 1700, and based on what we know about the frequency of such quakes, Dr. Hayes said that another one of similar strength could occur any day now.
A quake that big, and the tsunami it would generate, would be devastating to both Oregon and Washington, especially their coasts, Dr. Hayes said.
Our operating assumption is that everything west of Interstate 5 will be toast, an official with the Federal Emergency Management Agency told The New Yorker.
Oklahoma has had issues in recent years with what Dr. Hayes called human-induced earthquakes, which are the result of wastewater being pumped into the ground.
They have been recorded with magnitudes as high as about 5.8, but its not clear how much stronger they can get.
The San Andreas fault, which creates something of a spine that runs north to south along most of western California, is capable of producing an earthquake with a magnitude as high as 8.2, Dr. Hayes said.
Such a quake would be relatively shallow, he added, and experts say it could be catastrophic for the densely populated state.
The memory of that day seems to have been woven into the DNA of Mexicans, even those who did not live through that tragedy.
At one site, Santiago Borden, 10, was straining to help, carrying a heavy jug of water over his shoulder.
Eventually he gave up and passed the burden to his father.
Youre a kid so you cant expect to do everything, his father, Abraham Borden, a lawyer and local politician, said to comfort him.
I want to show solidarity, Santiago said.
His father replied: Of course you do.
Youre Mexican, after all.
The work has been nonstop.
Overnight, whirring generators powered floodlights to illuminate the disaster scenes.
And almost always, accompanying the rescue workers were volunteers clearing debris and distributing water, surgical masks and mustard-colored work gloves.
The scene at a collapsed building on Laredo Street took a grim turn shortly after dawn, as two bodies were unearthed from the wreckage.
Still, work continued.
We will continue to work to try and rescue everybody who lives in the building, said Karen Pia, a doctor in charge of distributing medicine for the area.
Five people had been rescued, but there was still no word of Gabriela Jan Pimienta, 43.
Her uncle, Miguel ngel Pimienta, had fainted with exhaustion as he waited for news on Wednesday morning.
His face covered by a surgical mask against the dust raised by the debris, he wept as he acknowledged the grim truth behind the wait.
With every hour that passes, there is less possibility, he said.
The work was taking its toll on rescue workers, pushing many to the brink.
As dawn broke over two collapsed residential buildings in the middle-class neighborhood of Del Valle, workers paused to rest as they waited for replacements.
They believed 40 people were still trapped inside.
Theres a breaking point, and were of no help like this, said one government rescue worker with tears in his eyes.
He asked not to be named because he was not authorized to speak publicly.
Ive been doing this 20 years, but its difficult to find people who almost made it out but didnt, he added.
There was a mother and daughter in a door frame and they were so close.
Newsletter Sign Up Continue reading the main story Breaking News Emails Sign up to receive an email from The New York Times as soon as important news breaks around the world.
Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
But even where the daily routine returned, as dog walkers emerged in the early light and cafes opened to people scanning the news and messages on their phones, the unfolding tragedy, sometimes just blocks away, was evident.
Ambulance sirens interrupted the silence, and police trucks rumbled by.
Volunteers carrying shovels headed to the rescue sites ready to take over from those who had been working all night.
Social media ricocheted with messages: photos of missing people, appeals for aid.
Poor neighborhoods in Xochimilco and Iztapalapa without much help, wrote Ricardo Becerra, an economist, on Twitter, referring to areas in the citys south and east.
Come with picks and shovels.
Over and over, variations on the list of supplies were repeated.
Hammer drills, work gloves, helmets, electrolytes, IV fluid, adrenaline, insulin.
And through it all, there were notes of hope: Found, read one message on Twitter.
Leonardo Faras from the Enrique Rebsmen school, the school where the 30 children died.
But the anguish was never far away.
Leonardo, pictured in a happier time wearing his knapsack and waving, was in the hospital.
He is in delicate condition, the message said.
The research here is part of a worldwide push that is growing increasingly urgent.
After decades of accumulating damage, followed by a huge die-off in 2015 and 2016, some scientists say they believe half the coral reefs that existed in the early 20th century are gone.
Advertisement Continue reading the main story

Instead of standing around watching the rest of them die, a vanguard of reef experts is determined to act.
In Florida, they are pioneering techniques that may allow the rapid re-establishment of reefs killed by heat stress.
In Hawaii, they are studying the biology of corals that somehow managed to cling to life as an earlier generation of people dumped raw sewage into a magnificent bay.
In the Caribbean, countries are banding together to create a genetic storage bank for corals, a backup plan if todays reefs all die.
We created these problems, said Michael P. Crosby, president of the Mote Marine Laboratory & Aquarium in Sarasota, Fla., one of the institutions leading this work.
We have to get actively involved in helping the corals come back.
Photo

Yet this new push to aid the worlds reefs comes with its own risks, and with many questions.
A large-scale restoration effort could be expensive, and so far, governments have put up only modest sums, despite the hit that their multibillion-dollar tourism industries could take from continued deterioration of the reefs.
Private philanthropists  including Paul G. Allen, the co-founder of Microsoft  are paying for much of the early work, spending millions.
But will they ultimately commit billions?
And while scientists are trying modest approaches first, the most effective strategy for saving reefs in the long run might be through genetic methods, including selective breeding or transferring heat-resistance genes into corals.
That type of thing has been done for crops, but would it be ethical to do it in the wild?
How do you decide what interventions are right and when to intervene?
said Madeleine van Oppen, a professor of marine biology at the University of Melbourne who is leading the experiments in Australia, aiming at what she calls the assisted evolution of coral reefs.
Theres a long road ahead; thats why were starting now.
Questions like these appear to be an inescapable part of the human future, and they go beyond coral reefs.
Already, some species of fish and birds are being kept alive only because they are bred in pens or hatcheries and then returned to the wild.
Forests are under stress on a rapidly warming planet, and scientists are wondering whether to manipulate their fate by planting more heat-resistant trees.
Creatures are fleeing toward the poles to escape rising heat; should humanity give them a lift?
Advertisement Continue reading the main story

Even the scientists who have plunged into this kind of work are asking themselves if it is the right thing and it if would ever be enough given the scale of climate changes predicted impact.
To think weve had to turn our science this way is kind of terrifying, but that is what weve had to do, said Ruth Gates, a coral researcher who is heading up the work in Hawaii.
Photo

UNITED NATIONS  As Hurricane Maria thunders through the Caribbean, island leaders still reeling from Hurricane Irma are calling on international organizations to provide money to help vulnerable countries recover from devastating storms linked to climate change.
In the Bahamas, emergency evacuations crippled the tourism on which the islands depend, said Darren A. Henfield, the countrys minister of foreign affairs.
The Dominican Republic, spared the worst of Hurricane Irma, fears a future of devastated beaches undermining decades of investment, President Danilo Medina said.
And on Barbuda, where Hurricane Irma destroyed everything in its path this month, there is not a single person left, officials said.
In one day, the population of neighboring Antigua swelled when it took in about 1,400 men, women and children who fled Barbuda.
Rodney Williams, the governor general of Antigua and Barbuda, said that in addition to the estimated $300 million cost of rebuilding Barbuda, Antigua was grappling with how to provide shelter, schools and medical care to hundreds of displaced people.
Photo

Today I ask how your governments will respond to this international crisis.
We ask the international community to help us, not because we want to outstretch a begging bowl, but because forces far beyond our control have pushed us to this dire situation, Mr. Williams told the United Nations on Monday.
Rebuilding Barbuda is not a task we can undertake alone.
Roosevelt Skerrit, the prime minister of Dominica, where Hurricane Maria made landfall late Monday as a Category 5 hurricane, pressed friendly nations and organizations to provide a helicopter so that he could survey the widespread devastation, which he described as mind-boggling.
In a special session convened by Secretary General Antnio Guterres before the official opening of the 72nd United Nations General Assembly, those Caribbean leaders and others appealed to the body to rethink humanitarian aid.
They asserted that because climate change is fueling more intense storms, vulnerable countries must have a better way to recover than to beg for money with each new devastation.
Newsletter Sign Up Continue reading the main story Interested in Climate Change?
Sign up to receive our in-depth journalism about climate change around the world.
Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
Climate change, they said, is no longer a distant threat.
Islands are already suffering millions of dollars in losses that they can barely afford because of planet-warming greenhouse gas emissions baked into the atmosphere, the leaders said.
Climate change and its consequences should not be a subject of speculation or debate, Mr. Medina said.
Its a truth which hits us and which causes great uncertainty.
Leaders did not make explicit demands at the formal United Nations session.
Behind the scenes, though, several said it was past time for the creation of a special funding mechanism to help countries deal with the unavoidable consequences of climate change.
No amount of planning in Barbuda, for example, could have protected the island from the utter collapse of its infrastructure, Walton Alfonso Webson, Antigua and Barbudas ambassador to the United Nations, said in an interview.
The small islands have been saying for so many years in the climate change discussions that this is possible, Mr. Webson said.
Its no longer possible.
Its happened.
The issue of whether countries should be assured of some aid to rebuild from storms or droughts, or to relocate citizens if need be, is known in United Nations parlance as loss and damage.
The question of wealthy nations responsibility for providing this compensation has never been fully resolved.
Industrialized nations have consistently rejected being held legally liable for their decades of carbon pollution.
After a protracted debate, the Obama administration allowed the Paris agreement in 2015 to acknowledge the special needs of vulnerable countries, but American negotiators supported a provision saying that doing so does not involve or provide a basis for any liability or compensation.
Island leaders said this week that it was time to forget the issue of compensation and focus on ways rich and poor countries could work together.
Some have called for large-scale insurance programs that pay out after a disaster, while others have proposed a special international fund.
There really has to be some sort of mechanism for insurance so we can have quick restoration after events such as this, Diann Black-Layne, Antiguas ambassador for climate change, said in an interview.
If that doesnt happen, we will have no choice but then to look for a compensation system.
Thats not what we want, to spend years in court.
She and other diplomats said they would press for a funding mechanism at a United Nations session in Germany in November.
The State Department did not respond to questions about the Trump administrations position on loss and damage.
Michele J. Sison, the deputy United States ambassador to the United Nations, told leaders on Monday that the United States Agency for International Development had committed $1.2 million to help Caribbean islands hit by Hurricane Irma.
American assistance has gone toward purchasing hygiene kits, helping to deliver relief supplies, restoring water access and assessing damage.
"It is a core American value to help those in need, Ms. Sison said.
The following year he created a nonprofit consulting firm that became a line of first defense for companies facing health and safety challenges from the E.P.A.
Mr. Dourson has a popular sideline as a writer of books that combine Bible stories with his views on science.
His series, Evidence of Faith, is an examination of the intersection of evolution and bible history.
At a time when the E.P.A.
is in the early stages of putting in place Congresss 2016 overhaul of the law governing toxic chemicals, Mr. Doursons nomination to become the agencys assistant administrator for chemical safety has alarmed Democrats and some former E.P.A.
officials.
Dr. Doursons consistent endorsement of chemical safety standards that not only match industrys views, but are also significantly less protective than E.P.A.
and other regulators have recommended, raises serious doubts about his ability to lead those efforts, said Senator Tom Carper, Democrat of Delaware, the ranking minority member on the panel that will assess Dr. Doursons qualifications.
This is the first time anyone with such clear and extensive ties to the chemical industry has been picked to regulate that industry.
Neither Mr. Dourson nor the E.P.A.
would comment on the criticisms of his industry ties.
A notice on the E.P.A.s website praises Mr. Doursons achievements in toxicology and the quality of his research.
Trade groups for the $800 billion chemical industry are supportive of the nominee.
CropLife America, which lobbies for purveyors of pesticides, fungicides and rodenticides, called Mr. Dourson a perfect fit.
We welcome Dr. Doursons nomination, CropLife America notes on its website.
His extensive experience in risk assessment and science, both in government and private sector make him a valuable addition to the office.
The confirmation hearing for Mr. Dourson and others had been scheduled for Wednesday, but it has been postponed and a new date has not been set.
Advertisement Continue reading the main story

Senator John Barrasso, Republican of Wyoming and chairman of the Senate committee that will hold the hearing, defended Mr. Doursons nomination.
Dr. Dourson is an experienced toxicologist who deserves full and fair committee consideration, followed by a Senate vote, Mr. Barrasso said.
That should be the case for all of the nominees for leadership roles at the E.P.A.
The nonprofit consulting firm that Mr. Dourson founded and ran, TERA, became part of the University of Cincinnati in July 2015.
The department changed its name from the TERA Center to the Risk Science Center in January 2017.
The center disclosed that it collected about 30 percent of its funding from for-profit sources in the 2015-16 fiscal year.
Mr. Doursons ethics agreement says that he will not, once confirmed, participate for one year in any particular matter involving specific parties related to University of Cincinnati work he has done.
But Mr. Doursons financial disclosure report  filed after he was nominated  shows no direct payments to him from any chemical company, meaning any company-funded research Mr. Dourson did in the last year would likely have been paid for through the University of Cincinnati or another organization.
As a result, it is unlikely ethics rules would bar him from overseeing issues related to chemicals manufactured by companies he has conducted research for.
Grants given by companies to universities, but not to the scholars themselves, generally do not create conflicts that require individuals to recuse themselves from matters involving the companies, said Walter Shaub, the former head of the federal Office of Government Ethics.
Mr. Doursons firms clients have included the American Chemistry Council, the industrys top lobbying group.
The firm also advised individual companies, makers of flame retardants, compounds that are called chemicals of concern, and pesticides.
In some cases, his firm provided results that suggested the health risk of a certain chemical or product was less than the assessment by the E.P.A.
and other researchers.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
PPG Industries, for example, a paint and coatings manufacturer, uses a chemical called 1,4-dioxane, which the E.P.A.
designated a likely carcinogen, in its products.
The substance is also created incidentally in some shampoos, cosmetics and personal care products through chemical reactions.
Advertisement Continue reading the main story

PPG hired Mr. Doursons group, which proposed establishing a safe level for 1,4-dioxane that would allow 1,000 times more risk than the E.P.As recommended level.
Other clients have included Albemarle, which makes flame retardants; Dow AgroSciences, which makes the pesticide chlorpyrifos; Waste Management; and Monsanto.
He has also helped DuPont defend a chemical called PFOA, used to make nonstick substances, from states, including West Virginia, that sued the company to clean up contaminated water.
Each of the four chemicals has been associated with severe health issues, like cancer, birth defects and developmental problems in children.
Mr. Doursons studies frequently concluded that the risk associated with these substances is much lower or more dubious than what E.P.A.
scientists and independent researchers have found.
The most striking discrepancy between findings by the agency and his firm is likely Mr. Doursons research funded by Dow AgroSciences on the pesticide chlorpyrifos, in which the authors recommended a safe level that was actually 33 times higher than the agencys standard, according to an analysis by Richard Denison, lead senior scientist at the Environmental Defense Fund.
The agency subsequently lowered its standard even more, to a level nearly 6,000 times less than Mr. Doursons, according to Mr. Denisons analysis.
E.P.A.
scientists then recommended that the product be banned for commercial use as a pesticide.
But E.P.A.
Administrator Scott Pruitt overruled a staff recommendation for a ban, after objections were raised by Dow and other industry players.
More recently, Mr. Dourson published a report titled A case study of potential human health impacts from petroleum coke transfer facilities, that was funded by Koch Industries, which has a subsidiary that handles petroleum coke and coal.
The report concluded that human exposures, if any, are well below levels that could be anticipated to produce adverse health effects in the general population.
Adam Finkel, executive director of the Penn Program on Regulation at the University of Pennsylvania Law School, who worked as a partner on a project with Mr. Dourson, said he observed a disturbing pattern.
Advertisement Continue reading the main story

Most of what he has done over time is to rush headlong to exonerate chemicals, Mr. Finkel said, adding that he stopped working with Mr. Dourson based on these concerns.
Pretty much every piece of work hes ever done, it just so happens that when they are finished with it, the risk is smaller than when they started, the doubt is larger, the concern is less.
But Oliver Kroner, now a Cincinnati city environmental official, praised Mr. Dourson, with whom he worked at TERA for nearly 10 years.
I think Mike is widely misunderstood, Mr. Kroner said.
Here in chemical regulation, were faced with a decision of whether we accept all the health science available to us, or if we exclude some science depending on the source.
Mike has worked hard to help strengthen the regulatory environment by improving the science coming out of industry and bringing a collaborative peer review approach to help assess the quality of industry-derived science, Mr. Kroner said.
Three other E.P.A.
nominees will be vetted at the confirmation hearing, one of whom also has spent much of his career defending businesses against the E.P.A.
: William L. Wehrum, named to head the agencys Office of Air and Radiation.
Mr. Wehrum, who was acting assistant administrator for air and radiation from 2005 to 2007, is now a partner in Hunton & Williams, which has a large energy and environmental law practice.
In the past few years, Mr. Wehrum has represented the Rubber Manufacturers Association, the American Petroleum Institute, the American Forest & Paper Association, and electric utilities, among others, against the E.P.A., legal records show.
Mr. Wehrum did not return messages seeking comment.
Liz Bowman, an E.P.A.
spokeswoman, pointed to Mr. Wehrums decades of working for the government and private sector as evidence of his qualifications for the new job.
Advertisement Continue reading the main story

Mr. Wehrums career includes over 31 years working in the environmental field through engineering, legal practice, and administrative duties, said Ms.
Bowman, who used to work for the American Chemistry Council, in a statement.
This addresses that criticism directly.
Of the 13 named storms so far in 2017, seven have been hurricanes, a number matched or exceeded at this point in the season only four times since 1995.
Four of the seven  Harvey, Irma, Jose and Maria  have reached Category 3 or higher, the threshold for a major hurricane on the Saffir-Simpson scale.
Only five other seasons since 1995 have had that many by Sept. 18.
More named storms have developed in the first three and a half months of the six-month hurricane season than developed in the entirety of the 1997, 1999, 2002, 2006, 2009, 2014 or 2015 seasons, according to National Hurricane Center and Weather Underground data.
Were running at about twice the pace of a typical season, Mr. Henson said.
Newsletter Sign Up Continue reading the main story Get the Morning Briefing by Email What you need to know to start your day, delivered to your inbox Monday through Friday.
Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
A few caveats are in order.
August, September and October are almost always the peak of the season, and it isnt uncommon for several storms to develop on each others heels, as Harvey, Irma, Jose, Katia, Lee and Maria did from Aug. 17 to Sept. 16.
And the phrase above average loses some of its significance when 10 of the 15 most active hurricane seasons since antebellum America have occurred in the past two decades.
What stands out is the combination of frequency and intensity.
It may not be unheard-of for six storms to develop in a month, but it is very unusual for two Category 4 and two Category 5 hurricanes to do so.
It is also extremely unusual for three major hurricanes to pass through the same region in three weeks, as Irma, Jose and Maria have in the northeastern Caribbean.
The last time the northern Leeward Islands experienced two major hurricanes in the same season was 1899, and now it is looking at three in the same month.
Residents of some islands barely had time to assess the wreckage of a Category 5 hurricane before another bore down on them.
Others fled their homes to escape Irma, only to find themselves in the cross hairs of Maria.
A full reckoning of 2017s place in hurricane history will not be possible until the season ends on Nov. 30, but there are a few things we can say with reasonable confidence:

It will almost certainly be the most expensive season on record in the United States.
That distinction, like most others, currently belongs to 2005, when Katrina and three other major hurricanes caused more than $143.5 billion of damage in the country.
But this year, AccuWeather estimated that Hurricanes Harvey and Irma might cost a combined $290 billion: two storms producing double the economic damage of four in 2005.
Advertisement Continue reading the main story

It probably wont be the most active season on record.
That dubious distinction belongs, by a large margin, to 2005 and its 28 named storms, which exhausted the World Meteorological Organizations yearly list of 21 names and forced officials to dip into the Greek alphabet for the first time.
Fifteen of the storms (another record) were hurricanes, including seven (second place, behind 1950) Category 3 or higher.
Five names (yet another record) were retired: Dennis, Rita, Stan, Wilma and, of course, Katrina.
The 2017 season is unlikely to match that.
But it will most likely be near the top.
Currently, the 1933 season, with 20 named storms, sits in second place after 2005.
Behind that are five seasons that produced 19 named storms apiece, one that produced 18, three that had 16 and four that had 15, according to Weather Underground, which maintains a list of the top 10 busiest Atlantic hurricane seasons.
By the end of November, that list will almost certainly include 2017.
Mr. Henson said he would not be surprised if 2017 were the second year to run through the alphabet of names, which would mean at least 21 named storms.
(Hurricane names do not begin with Q, U, X, Y or Z.)
And even if it doesnt get quite that far, he said, the intensity of the activity this year will put it in the pantheon of our most active years regardless of what happens from here outward.
On Monday, she was bracing for Hurricane Maria, which was heading straight for the island that she and hundreds of others had escaped to for sanctuary, Guadeloupe.
This year we are cursed, Ms. Guyard, 28, said after a morning of last-minute grocery shopping as the hurricane approached.
When will we be able to breathe again?
When will all of the hurricanes stop?
Some islands still reeling from the impact of Hurricane Irma were bracing late Monday for Round 2, closing schools, stores and just about everything else before the storm made landfall.
More than two dozen people were killed by Irma, and on Monday emergency shelters were beginning to fill up on Guadeloupe, Dominica and Montserrat, as well as on the islands of St. Kitts and Nevis.
Those who chose to stay home were busy boarding up their houses, trimming trees or gathering stockpiles of food and water.
Karine Fleury, 47, a psychologist in Martinique, which was also expected to be hit by Maria, said she found out about the storm only on Sunday while shopping for groceries.
After that, it was a race to prepare herself  both physically and mentally  for the storms landing.
I know its going to be impressive during the storm, she said.
And when we go out for the first time afterward, seeing the fallen trees and the damage, its always scary.
Though Maria is expected to trace a similar path to Hurricane Irma, some of the islands hit hardest by that storm may be spared.
Instead, having escaped the wrath of Irma, Guadeloupe and Dominica were expected to bear the initial brunt of Maria.
Advertisement Continue reading the main story

But the already storm-battered islands could be affected in other ways.
In addition to being the main sanctuary for those evacuating St. Martin, Guadeloupe has also become the staging ground for the relief effort.
If the storm hits hard, it could delay or upend the desperately needed aid going to its neighbor.
Though the number of hurricanes passing through the Caribbean feels exceptional this year, experts say it is not unheard-of.
Ten years ago, Hurricanes Karl, Igor and Julia were all active at the same time.
In 1998, four hurricanes  Georges, Ivan, Jeanne and Karl  passed through the Atlantic at once, according to the hurricane research division of the National Oceanic and Atmospheric Administration.
Still, the number of serious storms this year is higher than average.
None of this is unusual in terms of the number; we are in the peak week of the peak month in what was forecast to be an active season, said Dennis Feltgen, an agency spokesman.
What is horrific is the succession of Category 4 Harvey, Category 4 and 5 Irma and now Maria.
Newsletter Sign Up Continue reading the main story Sign Up for the Race/Related Newsletter Join a deep and provocative exploration of race with a diverse group of New York Times journalists.
Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
A typical season has 12 named storms, six of which become hurricanes, with three of those becoming major hurricanes.
So far this season, which is more than halfway through, there have been 13 named storms, including seven hurricanes, four which have been major.
The constant threat of storms has created a state of agitation among some residents  and some resistance to making the necessary preparations.
Were tired of this, lamented Stan Musquer, 44, an artist in Guadeloupe who says he has been evacuated three times in his life, forced to move all of his belongings ahead of storms that did not strike as badly as anticipated.
Were tired of this.
Its stressful.
Local authorities across the region have implored residents to take the warnings seriously.
Having suffered season after season of hurricanes, they are fearful residents will shrug off yet another storm.
Mr. Skerrit, Dominicas prime minister, addressed the tiny nation Monday morning, asking residents to remain calm but be prepared.
I want to say to Dominicans that this is not a time for heroism, he said.
This much water in Dominica is dangerous given our terrain, and therefore persons should not wait for something to happen in order to take action.
Advertisement Continue reading the main story

Preparation for many Caribbean islanders has, by now, become second nature.
I stocked up as much as possible with fresh water and dried foods, said Michele Henderson, a musician on Dominica.
I secured my dogs, rabbits and chickens.
We boarded up the windows and we are hunkered down in our basement apartment.
Others said that while they were not worried, they were still taking the proper precautions.
Ive seen lots of hurricanes and know what to expect, said Melissa Roberts, 36, on Dominica.
You stay home, buckle down and wait for it to clear.
Impending storms are often likened to past storms, especially in the minds of survivors.
For those on St. Martin, Hurricane Luis in 1995 was the big one until Irma blasted apart their island.
On the island of Montserrat, meanwhile, Hurricane Hugo looms large.
Photo

After Hugo, my house was full of glass and coconuts, said Susan Edgecombe, who runs Tradewinds Real Estate on the island and recalled the attitude that existed before that hurricane in 1989.
Everyone said: Theres not been a hurricane in over 60 years.
Dont stress.
Yeah right, she snapped.
We didnt get power for over three months, so now I am the prep queen.
On the island of Antigua, which Hurricane Irma skirted while destroying nearby Barbuda, Dr. Jillia Bird, an optometrist, said she had once again gone through her familiar pre-hurricane motions, closing storm shutters, moving items off the floor in case of flooding, covering beds with shower curtains and towels to prevent soaking, packing up valuables and moving from her wooden house to her mothers storm-tested 60-year-old concrete house.
Dr. Bird also chilled her merlot  in case of power loss, she said  and found time for an act of generosity toward friends of hers in the British Virgin Islands, which was slammed by Hurricane Irma: She bought credit for their cellphones, as a kindness gesture as they face another difficult night, she said.
In St. Kitts and Nevis, the storm has particularly cruel timing, landing on the eve of the 34th anniversary of the islands independence.
The good news for residents there, however, was that most of them had already prepared for Irma, and so had less to do to prepare for Maria.
Advertisement Continue reading the main story

Well, following the recent passage of Hurricane Irma, I still have most things in place, like candles, flashlight with batteries and important items in plastic bags, said Precious Mills, a resident.
I would say that I have basic measures in place to weather the storm.
While the island escaped largely unscathed from Irma, some homes sustained damage.
For John Webster, who lives in the affected area of Newton Ground, that means the patchwork fixes he made to his roof after Irma will have to do for now.
I had planned to properly fix it, but I am going to wait until this storm has passed, he said.
He rejected the idea that the governors represent a shadow diplomatic corps.
I dont think its a shadow, he said.
Were in the sunlight.
Were shining the bright light of success.
Advertisement Continue reading the main story

The General Assembly brings together leaders and senior officials from nearly 200 countries for a week of speeches and high-level talks.
Climate change  though not the central issue of the meeting  will have a high profile, in part because of confusion over whether the United States can be persuaded to remain in the Paris agreement.
The White House has asserted that it will stay in the pact if suitable terms are met.
It has not laid out what those terms might be.
Mr. Cohns breakfast meeting on Monday with a handful of ministers to discuss climate change was the only event the White House has scheduled on the topic.
Thats why we have governors here.
Because we dont have someone from Washington D.C., Governor Brown said.
The states are picking up the baton.
Governor Brown, along with Governor Inslee and Gov.
David Y. Ige of Hawaii, is also participating in Climate Week, a series of high-level panels on climate change.
The Trump administration did not send a representative to the meetings, which are not affiliated with the United Nations.
Governor Inslee will also meet with Frank Bainimarama, the prime minister of Fiji, which holds the presidency of United Nations climate change negotiations this year.
Another governor, Roy Cooper of North Carolina, was expected to announce this week that his state would join California, 13 other states and Puerto Rico in the United States Climate Alliance, a group of states and territories that has pledged to uphold the Paris agreement.
Photo

The addition of North Carolina to the group would be notable because it is among the highest-emitting states.
Without big producers of greenhouse gasses, experts say, states will struggle to make significant gains in reaching global climate goals.
As part of the Paris accord, the Obama administration vowed that United States greenhouse gas emissions would fall at least 26 percent below 2005 levels by 2025.
Each governor in the alliance is promoting individual policies aimed at meeting that goal.
Advertisement Continue reading the main story

Oregon and New York, for instance, plan to shutter their last coal plants by 2020.
In Virginia, Gov.
Terry McAuliffe has ordered new carbon regulations for local power plants.
Californias legislature has authorized one of the most sweeping climate programs in the world, aimed at decarbonizing every corner of the states economy, from transportation to agriculture.
We certainly believe if the federal government wont lead in this area, we want the world to understand there are states across the country that are committed, said Governor Ige, whose state enacted the first 100-percent-renewable energy standard.
Despite the push by the climate alliance, though, those states make up just one-third of the nations population, and the United States as a whole is still expected to fall short of Mr. Obamas pledge.
The big question, then, is whether the alliance can persuade other states to join its climate efforts.
Unless their leadership is met with followership, the impact will be pretty limited, said David G. Victor, a professor of international relations at the University of California, San Diego.
The idea has always been that if these states can demonstrate the technologies needed to cut emissions, that will help shift the politics in other states.
But we have yet to see that play out.
Perhaps the governors biggest challenge will be the deep partisan divide over climate change in the country.
Nicolas Loris, a research fellow in energy and environmental policy at the Heritage Foundation, a conservative research groups, said he believed the governors were speaking for a minority of Americans.
He said that more than half of American states opposed Mr. Obamas signature effort to cut emissions, known as the Clean Power Plan.
Its one thing if these governors are communicating their respective state climate plans, as ill-advised as they may be, Mr. Loris said.
No matter how expensive or ineffective these climate policies may be, its their right.
They can deliver that message to anyone they please.
But they shouldnt pretend their actions are the will of the federal government or the entire country.
For now the climate alliance is focused on the economic argument that it is possible to cut emissions without harming the economy.
According to a recent report from the Brookings Institution, every state in the alliance has managed to cut emissions since 2000 even while expanding their overall economic output.
The coalition states say they have created 1.3 million clean energy jobs while cutting emissions.
We have blown up the argument that acting on climate change is bad for your economy, Governor Inslee said.
Thoroughly Americanized by that time, it was a huge culture shock, he recalled.
Leaning on the family loom while his father worked, he listened to stories about what life had been like in the village and how it had changed.
Eventually he rediscovered a passion for weaving.
And he realized that, just as he had forgotten the richness of his culture, the village, too, was slowly losing its age-old traditions.
Advertisement Continue reading the main story

There was not much soul anymore, he said.
These natural dyes were absolutely on the brink of extinction.
Mr. Gutirrez and his family decided to create their own weaving studio to create pieces using only natural dyes and to teach others how to do it.
His sister, Juana Gutirrez Contreras, serves as dye master, combining seven or eight natural elements to produce more than 40 colors.
Ms. Contrerass husband, Antonio Lazo Hernandez, is also a master weaver and helps develop the textile designs.
Photo

Potassium alum, or potash, a mineral found in the mountains around Oaxaca, is used as a mordant, holding the dye to the yarn.
In addition to plants gathered in the mountains, flora common in local gardens  zapote negro, marush and pomegranate, for example  are also used as sources for dye.
The indigo and cochineal pigments, however, are purchased from elsewhere.
The ail plant grows primarily in the southern part of the state of Oaxaca.
As for cochineal  a dye that colored the red coats of British soldiers  tens of thousands of dried insects are needed to produce just one pound of dye.
So the studio buys the pigment from families who farm the prickly pear cactuses that host the parasitic insects.
Only females produce the carminic acid that is responsible for the intense red coloring.
The dye is so harmless that the family uses it to water the garden, while the remaining plant material serves as mulch.
Advertisement Continue reading the main story

In Teotitln, Mr. Gutirrez is not the only artisan concerned with preserving Zapotec weaving traditions.
Perhaps a dozen others in the village use natural dyes exclusively, and some train tourists in the techniques.
But Mr. Gutirrezs fluency in English and familiarity with the United States  he still lives much of the time in Ventura  have given him an opportunity to reach a wider audience.
Im able to see my culture from an outsiders perspective and also from an insiders, as part of the community, he said.
Photo

The family is compiling a book of dye recipes, formulas passed down for centuries by word of mouth.
And Mr. Gutirrez has worked to expand traditional designs used by Zapotec weavers into new territory, for example, by combining wool with agave fiber, palm leaves  used for thousands of years to make mats for sleeping  or other natural materials.
He has contributed samples of natural dyeing materials to the Harvard Art Museums Forbes Pigment Collection.
Last year, with a grant from the Smithsonian Museum of the American Indians Artists Leadership Program, he held a four-day workshop in Teotitln for 15 young weavers, teaching them the science and practice of natural dyes.
Their reaction was almost a, Wow, let me try it, let me do it, said Keevin Lewis, who ran the program and recently retired as the museums outreach coordinator.
They got their hands on the wool, they got their hands in the dye, they were crushing up the cochineal bug.
They were in it.
In July, Mr. Gutirrezs work was featured in the innovation section of the annual International Folk Art Market in Santa Fe.
What my family and I are doing is continuing an art form and honoring the work that our ancestors started, he said.
I think once people learn more about these processes, then theyll support this market, and thats how it will continue.
The surprise discovery came at the conclusion of a project that was spread over three years and a hectic two weeks  necessitated by limited financing and availability.
Atlantic Ocean 100 Miles SCOTLAND North Sea N. IRELAND ENGLAND Irish Sea IRELAND BRITAIN WALES London Celtic Sea Boxford English Channel FRANCE

Among the first to spot it was Joy Appleton, who leads the Boxford History Project and who was a driving force behind the excavation.
I was stunned into silence, Ms. Appleton recalled of her first sight of the small red tiles, each the size of her fingernail.
Which is unusual.
The expert on site, Matt Nichol, was equally surprised.
I will never forget that moment, said Mr. Nichol, a professional archaeologist who was supervising the dig.
It was down to the volunteers, it really was.
I get quite emotional about it; it was something to see their drive, added Mr. Nichol, project officer for Cotswold Archaeology, a company whose normal work includes helping real estate developers preserve archaeological finds.
Experts say the mosaic at what is now called Boxford villa depicts Bellerophon, a hero of Greek mythology who was sent to kill the chimera, a fire-breathing monster with the head of a lion, the torso of a goat and the tail of a serpent.
Hercules is also thought to be featured, fighting a centaur, and so is Cupid.
According to Anthony Beeson, a specialist in classical art and member of the board of the Association for Roman Archaeology, the discovery is important for several reasons.
Advertisement Continue reading the main story

It is so unusual because it has all sorts of quirks which you dont expect, and it has subjects on it that are completely alien to mosaics in this country, he said.
Photo

Some figures breach geometric borders and there seems to be a trompe loeil effect.
Mr. Beeson added that he could not think of another Roman mosaic in this country that is as creative as this one.
There are inscriptions, too, though only about one-third of the mosaic was excavated and the full text was not uncovered.
The execution is uneven, Mr. Beeson said, suggesting that the mosaicist has had ideas above his technical ability, producing what he called a very sophisticated design done in a slightly nave manner.
Boxford villa had been marked  inaccurately, as it turned out  on an old map.
(It later turned out that the site was disturbed in the 19th century, when the installation of a land drainage pipe damaged part of the mosaic.)
With much to be revealed, there is still a lot to learn about life at Boxford villa, though its owner must have been affluent and cultured, and clearly wanted to show off a broad knowledge of mythology to guests.
Mr. Beeson says he believes that it is really vital that we at least see what the other part of the mosaic is like; its too important not to investigate.
For Ms. Appleton, the discovery has filled in part of a missing link in the history of Boxford, a village of around 300 inhabitants.
Evidence of Stone, Bronze and Iron Age life had been discovered, and there is a Saxon window in the local church that dates to the period before the Norman invasion of 1066.
Given the geographical location, and the quality of the agricultural land, Ms. Appleton was confident that this was also the site of a Roman settlement, a conviction reinforced by the discovery of several artifacts from that period.
Photo

So Ms. Appleton and her group pressed ahead.
Survey work began in 2012, and there were discoveries at two nearby sites during digs in 2015 and 2016.
Advertisement Continue reading the main story

Without expert archaeological knowledge, the Boxford History Project secured help from the Heritage Lottery Fund, a national charity funded by lottery receipts, to pay for professional supervision during a project made up of short excavations conducted on three sites in three consecutive years.
By chance, Mr. Nichol, who supervised the dig, does not live far away, an irony that is not lost on an archaeologist who has traveled to the Western Sahara, Macedonia and Serbia in search of antiquities, only to discover something so spectacular so close to home.
Newsletter Sign Up Continue reading the main story Please verify you're not a robot by clicking the box.
Invalid email address.
Please re-enter.
You must select a newsletter to subscribe to.
Sign Up You agree to receive occasional updates and special offers for The New York Times's products and services.
Thank you for subscribing.
An error has occurred.
Please try again later.
View all New York Times newsletters.
I never believed it could have been in Boxford, 30 minutes drive from home, he said.
What happens to the site into the future remains unclear because, once exposed to the atmosphere, a mosaic deteriorates quickly unless it is preserved.
Ms. Appleton and Mr. Nichol hope to uncover the rest of it next year, though that will depend on whether funding can be raised.
But even if they do, making it available for public view would be costly.
Just lifting it and removing it from the site would cost hundreds of thousands of pounds.
In fact, there was little choice about an immediate solution  which was to bury it in the earth that had protected it for so long because the farmer needed his land back to plant his wheat crop.
More Reporting on Archaeology

Even that step proved nerve-racking because there was no money to pay for security to prevent treasure hunters from damaging or destroying the mosaic.
The risks increased when, the day before the mosaic was covered over, the site was opened to friends and families of the volunteers who had worked there, increasing the number of people who knew the location.
Advertisement Continue reading the main story

So for the organizers, it was a relief, rather than a disappointment, when the earth was pushed back to conceal their discovery.
The night before that was done, Mr. Nichol decided to keep watch over the site from his S.U.V.
with a supply of food, a sleeping bag and a bottle of red wine, all donated by volunteers.
The Roman owner of the villa would have invited guests to eat and drink on this spot, using the mosaic as a talking point, so a mildly bacchanalian vigil did not seem out of place.
I was on my own in the field; it was incredible, Mr. Nichol said.
He described how, in the solitude, he felt drawn back across the centuries to experience a unique connection to the more-than-1,600-year-old archaeological site, and to the mythological images of its extraordinary, colorful mosaic.
The wine did help, he added.
